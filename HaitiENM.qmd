---
title: "Ecological Niche Modeling for Aedes aegypti in Haiti"
subtitle: "Climate-driven species distribution analysis"
author: "Ian Pshea-Smith"
date: today
format:
  html:
    theme: journal
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    smooth-scroll: true
    fig-width: 10
    fig-height: 6
    embed-resources: true
    citations-hover: true
    footnotes-hover: true
output-file: Aeae_SDMs_About.html
execute:
  warning: false
  message: false
  cache: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  out.width = "100%"
)
```

## Introduction {#sec-intro}

This analysis focuses on ecological niche modeling for *Aedes aegypti* mosquitoes in Haiti, utilizing bioclimatic variables and presence data to understand species distribution patterns under current and future climate scenarios, and providing an assessment of the degree to which the population at risk might change under three scenarios of climate change.

## Libraries and Setup {#sec-setup}

```{r, echo = FALSE}
#| label: libraries
#| code-summary: "Load required packages"

# Core data manipulation and analysis
  library(readr)
  library(dplyr)
  library(sf)

# Spatial data and modeling
  library(raster)
  library(dismo)
  library(gbm)
  library(exactextractr)
  library(geodata)

# Visualization
  library(ggplot2)
  library(ggfortify)
  library(RColorBrewer)
  library(viridis)
  library(gridExtra)
  library(leaflet)

# Misc. packages
  library(lctools)
  library(pROC)
  library(jsonlite)
  library(parallel)
  library(doParallel)
  library(purrr)
```

## Visualization Setup {#sec-viz-setup}

```{r, echo = FALSE}
#| label: plotting-setup
#| code-summary: "Custom plotting functions and color schemes"

# Custom color palette
  plasma_colors <- c("#5805dfff", "#8d01efff", "#bf09eeff", "#ec2cdbff", 
                   "#ff51bdff", "#ff769fff", "#ff9d82ff", "#ffc666ff", "#fff44aff")

# Custom plotting function
  plot_minimal <- function(raster, title) {
    plot(raster, 
         col = plasma_colors, 
         main = title, 
         legend = TRUE, 
         axes = FALSE, 
         box = FALSE, 
         legend.args = list(
           text = "Value", 
           side = 4, 
           font = 2, 
           line = 2.5, 
           cex = 0.8
         ))
  }
```

## Data Import and Processing {#sec-data-import}

```{r, echo = FALSE}
#| label: data-import-simple
#| code-summary: "Import data directly from GitHub repository"

# Create data directory if it doesn't exist
  if (!dir.exists("data")) {
    dir.create("data", recursive = TRUE)
  }

# GitHub base URL for raw files
  github_base <- "https://raw.githubusercontent.com/IanPsheaSmith/HaitiAeaeENM/main/HaitiShapefiles/"

# Download trap site data
  if (!file.exists("data/HaitiTrapsBin.csv")) {
    download.file(paste0(github_base, "HaitiTrapsBin.csv"), 
                  "data/HaitiTrapsBin.csv", mode = "wb")
  }
  HaitiTrapsBin <- read_csv("data/HaitiTrapsBin.csv")

# Convert to spatial objects
  HaitiTrapsBin_sf <- st_as_sf(HaitiTrapsBin, 
                              coords = c("Longitude", "Latitude"), 
                              crs = 4326)
  HaitiTrapsBin_utm <- st_transform(HaitiTrapsBin_sf, crs = 32618)
```

```{r, echo = FALSE}
  #| label: shapefiles-import
  #| code-summary: "Download and import shapefiles"
  
  # Haiti boundary shapefile components
    haiti_files <- c("HaitiPolygon.shp", "HaitiPolygon.shx", "HaitiPolygon.dbf", 
                     "HaitiPolygon.prj", "HaitiPolygon.cpg")
  
  # Download missing files using vectorized operations
    haiti_local_paths <- paste0("data/", haiti_files)
    haiti_missing <- haiti_files[!file.exists(haiti_local_paths)]
    haiti_urls <- paste0(github_base, haiti_missing)
    haiti_destinations <- paste0("data/", haiti_missing)
  
    if(length(haiti_missing) > 0) {
      mapply(download.file, haiti_urls, haiti_destinations, 
             MoreArgs = list(mode = "wb", quiet = TRUE))
    }
  
  # Load Haiti boundary
    haitiPolygon <- st_read("data/HaitiPolygon.shp")
    haitiPolygon_geo <- st_transform(haitiPolygon, crs = 4326)
  
  # Administrative boundaries shapefile components  
    adm3_files <- c("hti_admbnda_adm3_cnigs_20181129.shp", 
                    "hti_admbnda_adm3_cnigs_20181129.shx",
                    "hti_admbnda_adm3_cnigs_20181129.dbf", 
                    "hti_admbnda_adm3_cnigs_20181129.prj",
                    "hti_admbnda_adm3_cnigs_20181129.CPG")
  
  # Download missing administrative boundary files
    adm3_local_paths <- paste0("data/", adm3_files)
    adm3_missing <- adm3_files[!file.exists(adm3_local_paths)]
    adm3_urls <- paste0(github_base, adm3_missing)
    adm3_destinations <- paste0("data/", adm3_missing)
  
    if(length(adm3_missing) > 0) {
      mapply(download.file, adm3_urls, adm3_destinations, 
             MoreArgs = list(mode = "wb", quiet = TRUE))
    }
  
  # Load administrative boundaries
    Haiti_adm3 <- st_read("data/hti_admbnda_adm3_cnigs_20181129.shp")
    Haiti_adm3$ID <- as.numeric(as.factor(Haiti_adm3$ADM3_EN))
  
  # Haiti without waterways shapefile components (same approach as others)
    haiti_no_water_files <- c("Haiti_no_Wtrwy.shp", "Haiti_no_Wtrwy.shx", "Haiti_no_Wtrwy.dbf", 
                              "Haiti_no_Wtrwy.prj", "Haiti_no_Wtrwy.cpg")
  
  # Download missing waterway files (same pattern as above)
    water_local_paths <- paste0("data/", haiti_no_water_files)
    water_missing <- haiti_no_water_files[!file.exists(water_local_paths)]
    water_urls <- paste0(github_base, water_missing)
    water_destinations <- paste0("data/", water_missing)
  
    if(length(water_missing) > 0) {
      mapply(download.file, water_urls, water_destinations, 
             MoreArgs = list(mode = "wb", quiet = TRUE))
    }
  
  print("All shapefiles processed")
```

## Study Site Maps {#sec-visuals}

```{r}
  #| label: species-visualization
  #| code-summary: "Process Aedes aegypti data and create visualizations"
  #| fig-cap: "Haiti administrative boundaries and Aedes aegypti presence locations"
  
  # Create Aedes aegypti presence dataset first
    AeaePoints <- HaitiTrapsBin[HaitiTrapsBin$Aeae == 1, ]
    AeaePoints <- AeaePoints[, !(names(AeaePoints) %in% c("Cxq", "Aealb"))]
  
  # Convert to spatial objects
    AeaePoints_sf <- st_as_sf(AeaePoints, coords = c("Longitude", "Latitude"), crs = 4326)
    AeaePoints_utm <- st_transform(AeaePoints_sf, crs = 32618)
  
  # Static visualization using first chunk's style with Aedes aegypti points
    static_plot <- ggplot() +
      geom_sf(data = haitiPolygon_geo, fill = "white", color = "grey", size = 0.25) +
      geom_sf(data = Haiti_adm3, fill = NA, color = NA, size = 0.3) +
      geom_sf(data = AeaePoints_sf, color = "#702963", size = 1.5, alpha = 0.7) +
      theme_minimal() +
      theme(
        axis.ticks = element_blank(),
        panel.grid = element_line(color = "grey90", size = 0.3)
      )
    
    print(static_plot)
    
    
    output_folder <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Figures"
    svg(filename = file.path(output_folder, "Fig1.svg"), width = 10, height = 12)
    print(static_plot)
    dev.off()
```

```{r}
  #| label: Interactive visualization with mapgl
  #| code-summary: "Create interactive map of study sites in Haiti"
  #| fig-cap: "Haiti administrative boundaries and Aedes aegypti presence locations"
   
  # Convert sf objects to coordinates for mapgl
    aeae_coords <- st_coordinates(AeaePoints_sf)
    aeae_data <- data.frame(
      longitude = aeae_coords[,1],
      latitude = aeae_coords[,2],
      species = "Aedes aegypti"
    )
  
  # Interactive visualization with leaflet
    interactive_map <- leaflet() %>%
      addTiles() %>%  # Add default OpenStreetMap tiles
      setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
      
      # Add Haiti boundary
      addPolygons(
        data = haitiPolygon_geo,
        fillColor = "white",
        fillOpacity = 0.1,
        color = "grey",
        weight = 1,
        popup = "Haiti"
      ) %>%
      
      # Add Aedes aegypti points
      addCircleMarkers(
        data = AeaePoints_sf,
        radius = 5,
        fillColor = "#702963",
        color = "white",
        weight = 1,
        opacity = 1,
        fillOpacity = 0.8,
        popup = ~paste("Aedes aegypti presence<br>",
                       "Site ID:", row.names(AeaePoints))
      ) %>%
      
      # Add legend
      addLegend(
        position = "bottomright",
        colors = "#702963",
        labels = "Aedes aegypti presence",
        title = "Species Distribution"
      )
  
    print(interactive_map)
  
  # Print summary information
    cat("Dataset Summary:\n")
    cat("- Total trap sites:", nrow(HaitiTrapsBin), "\n")
    cat("- Aedes aegypti presence sites:", nrow(AeaePoints), "\n")
    cat("- Presence rate:", round(nrow(AeaePoints)/nrow(HaitiTrapsBin)*100, 1), "%\n")
```

## Raster Data Download {#sec-rasters}

```{r, echo = FALSE}
#| label: bioclim-download
#| code-summary: "Download bioclimatic variables for Haiti"

# Define where to download the rasters (using relative path)
  haiti_bioclim_path <- "data/bioclim"

# Create directory if it doesn't exist
  if (!dir.exists(haiti_bioclim_path)) {
    dir.create(haiti_bioclim_path, recursive = TRUE)
  }

# Download 30-second (0.5-minute) resolution bioclimatic variables for Haiti
  Haiti_bioclim_data <- worldclim_country(
    country = "HTI",       # ISO code for Haiti
    var = "bio",           # Download bioclimatic variables
    res = 0.5,             # 0.5-minute resolution (30 seconds)
    path = haiti_bioclim_path
  )

# Rename & change to "raster" format using your working approach
  Haiti_bioclim_rast <- raster() # Create an empty raster, then switch formatting and add to a raster stack
  Haiti_bioclim_rast$AnnMeanTemp <- raster(Haiti_bioclim_data$wc2.1_30s_bio_1)
  Haiti_bioclim_rast$MeanDiurnRange <- raster(Haiti_bioclim_data$wc2.1_30s_bio_2)
  Haiti_bioclim_rast$Isothermality <- raster(Haiti_bioclim_data$wc2.1_30s_bio_3)
  Haiti_bioclim_rast$TempSeasonality <- raster(Haiti_bioclim_data$wc2.1_30s_bio_4)    
  Haiti_bioclim_rast$MaxTempWarmMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_5)
  Haiti_bioclim_rast$MinTempColdMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_6)
  Haiti_bioclim_rast$TempAnnRnge <- raster(Haiti_bioclim_data$wc2.1_30s_bio_7)
  Haiti_bioclim_rast$MTempWetQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_8)
  Haiti_bioclim_rast$MTempDryQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_9)
  Haiti_bioclim_rast$MTempWarmQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_10)
  Haiti_bioclim_rast$MTempCldQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_11)
  Haiti_bioclim_rast$AnnPrcp <- raster(Haiti_bioclim_data$wc2.1_30s_bio_12)
  Haiti_bioclim_rast$PrcpWetMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_13)
  Haiti_bioclim_rast$PrcpDryMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_14)
  Haiti_bioclim_rast$PrcpSeasonality <- raster(Haiti_bioclim_data$wc2.1_30s_bio_15)
  Haiti_bioclim_rast$PrcpWetQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_16)
  Haiti_bioclim_rast$PrcpDryQrt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_17)
  Haiti_bioclim_rast$PrcpWrmQrt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_18)
  Haiti_bioclim_rast$PrcpCldQrt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_19)

# Check the result
  print(paste("Created raster stack with", nlayers(Haiti_bioclim_rast), "bioclimatic layers"))
  print("Layer names:")
  print(names(Haiti_bioclim_rast))
```

```{r, echo = FALSE}
#| label: bioclim-processing
#| code-summary: "Process and crop bioclimatic rasters using Haiti boundary without waterways"

# Load Haiti without waterways shapefile 
  haiti_wo_lakes <- st_read("data/Haiti_no_Wtrwy.shp")

# Reproject the shapefile to match the CRS of the bioclim rasters
  haiti_wo_lakes_reproj <- st_transform(haiti_wo_lakes, crs = st_crs(Haiti_bioclim_rast))

# Convert to Spatial object for raster operations (required for mask/crop)
  haiti_wo_lakes_sp <- as(haiti_wo_lakes_reproj, "Spatial")

# Crop and mask Haiti_bioclim_rast using the reprojected shapefile
  Haiti_bioclim_shaped <- mask(crop(Haiti_bioclim_rast, haiti_wo_lakes_sp), haiti_wo_lakes_sp)

# Create final covariate stack for modeling
  BRT_All_Covars <- Haiti_bioclim_shaped

# Display results
  print(paste("Final covariate stack contains", nlayers(BRT_All_Covars), "layers"))
  print("Available covariates:")
  print(names(BRT_All_Covars))

# Show extent and resolution
  print(paste("Extent:", paste(as.vector(extent(BRT_All_Covars)), collapse = ", ")))
  print(paste("Resolution:", paste(res(BRT_All_Covars), collapse = " x ")))
  
# Plot rasters
  plot_minimal(BRT_All_Covars)
```

# Future Climate Scenarios

```{r }
#| label: setup-python-environment
#| echo: false
#| eval: false

# Only install if not already present
if (!requireNamespace("reticulate", quietly = TRUE)) {
  options(repos = c(CRAN = "https://cloud.r-project.org/"))
  install.packages("reticulate")
}

library(reticulate)

# For conda/python setup, you typically only want to run this once
# Set eval: false to prevent running during render

```

```{r, echo = FALSE}
# First, fix Python setup
library(reticulate)

# Check your Python installation
py_config()

# Install packages one by one to catch errors
install_packages <- function() {
    # Basic packages first
    py_install("requests", pip = TRUE)
    py_install("pandas", pip = TRUE)
    py_install("numpy", pip = TRUE)
    
    # Then spatial packages (these can be tricky on Windows)
    tryCatch({
        py_install("rasterio", pip = TRUE)
    }, error = function(e) {
        message("rasterio failed, trying conda-forge")
        conda_install("r-reticulate", "rasterio", channel = "conda-forge")
    })
    
    # Verify what's installed
    message("Checking installations:")
    message("requests: ", py_module_available("requests"))
    message("pandas: ", py_module_available("pandas"))
    message("numpy: ", py_module_available("numpy"))
}

install_packages()
```

```{python, echo = FALSE, eval = FALSE}

import xarray as xr
import numpy as np
import pandas as pd
import geopandas as gpd
import rasterio
from rasterio.mask import mask
import requests
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# Configuration
# ============================================================================

class CMIP6Downloader:
    def __init__(self, output_dir="cmip6_haiti_python", cache_dir="cache"):
        self.output_dir = Path(output_dir)
        self.cache_dir = Path(cache_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        
        # Alternative CMIP6 data sources
        self.sources = {
            'worldclim': 'https://geodata.ucdavis.edu/cmip6/30s/',
            'pangeo': 'https://storage.googleapis.com/cmip6/',
            'esgf': 'https://esgf-node.llnl.gov/search/cmip6/'
        }
        
        # Haiti bounding box
        self.haiti_bounds = {
            'min_lon': -74.5,
            'max_lon': -71.5,
            'min_lat': 18.0,
            'max_lat': 20.1
        }
    
    def smart_download(self, url, local_path, chunk_size=8192*1024, max_retries=3):
        """Download with resume capability and chunking"""
        
        local_path = Path(local_path)
        temp_path = local_path.with_suffix('.tmp')
        
        # Check if already downloaded
        if local_path.exists():
            print(f"  ✓ Already exists: {local_path.name}")
            return True
        
        headers = {}
        mode = 'wb'
        resume_pos = 0
        
        # Check for partial download
        if temp_path.exists():
            resume_pos = temp_path.stat().st_size
            headers['Range'] = f'bytes={resume_pos}-'
            mode = 'ab'
            print(f"  ↻ Resuming from {resume_pos/1024**2:.1f} MB")
        
        for attempt in range(max_retries):
            try:
                with requests.get(url, headers=headers, stream=True, timeout=30) as r:
                    r.raise_for_status()
                    
                    # Get total size
                    total_size = int(r.headers.get('content-length', 0)) + resume_pos
                    
                    with open(temp_path, mode) as f:
                        downloaded = resume_pos
                        for chunk in r.iter_content(chunk_size=chunk_size):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                                
                                # Progress
                                if total_size > 0:
                                    pct = (downloaded / total_size) * 100
                                    print(f"\r  ⬇ {pct:.1f}% ({downloaded/1024**3:.2f} GB / "
                                          f"{total_size/1024**3:.2f} GB)", end='', flush=True)
                
                print()  # New line after progress
                # Move temp to final
                temp_path.rename(local_path)
                print(f"  ✓ Downloaded: {local_path.name}")
                return True
                
            except Exception as e:
                print(f"\n  ✗ Attempt {attempt+1} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))  # Exponential backoff
                    
        return False
    
    def download_with_crop(self, url, model, scenario, period, variable='bioc'):
        """Download and immediately crop to Haiti"""
        
        filename = f"wc2.1_30s_{variable}_{model}_{scenario}_{period}.tif"
        global_path = self.cache_dir / filename
        haiti_path = self.output_dir / f"haiti_{filename}"
        
        # Check if Haiti version exists
        if haiti_path.exists():
            print(f"  ✓ Haiti subset exists: {haiti_path.name}")
            return str(haiti_path)
        
        # Download global file if needed
        if not global_path.exists():
            full_url = f"{self.sources['worldclim']}{model}/{scenario}/{filename}"
            success = self.smart_download(full_url, global_path)
            if not success:
                return None
        
        # Crop to Haiti
        print(f"  ✂ Cropping to Haiti...")
        try:
            with rasterio.open(global_path) as src:
                # Create bounding box
                from rasterio.mask import mask
                from shapely.geometry import box
                
                haiti_box = box(self.haiti_bounds['min_lon'], 
                              self.haiti_bounds['min_lat'],
                              self.haiti_bounds['max_lon'], 
                              self.haiti_bounds['max_lat'])
                
                # Crop
                out_image, out_transform = mask(src, [haiti_box], crop=True)
                out_meta = src.meta.copy()
                
                # Update metadata
                out_meta.update({
                    "driver": "GTiff",
                    "height": out_image.shape[1],
                    "width": out_image.shape[2],
                    "transform": out_transform,
                    "compress": "deflate"
                })
                
                # Write Haiti subset
                with rasterio.open(haiti_path, "w", **out_meta) as dest:
                    dest.write(out_image)
            
            # Size comparison
            global_size = global_path.stat().st_size / 1024**3
            haiti_size = haiti_path.stat().st_size / 1024**2
            print(f"  ✓ Size: {global_size:.2f} GB → {haiti_size:.2f} MB "
                  f"({100*(1-haiti_size/(global_size*1024)):.1f}% reduction)")
            
            # Delete global file to save space
            global_path.unlink()
            print(f"  ✓ Deleted global file")
            
            return str(haiti_path)
            
        except Exception as e:
            print(f"  ✗ Crop failed: {e}")
            return None

# Alternative: Use intake-esm for direct CMIP6 access
def get_cmip6_from_pangeo(model, scenario, variable='tas'):
    """Access CMIP6 data from Pangeo cloud catalog"""
    import intake
    
    # Open CMIP6 catalog
    col_url = "https://storage.googleapis.com/cmip6/pangeo-cmip6.json"
    col = intake.open_esm_datastore(col_url)
    
    # Search for specific data
    query = col.search(
        experiment_id=scenario,
        table_id='Amon',
        variable_id=variable,
        source_id=model
    )
    
    # Load as xarray dataset
    ds_dict = query.to_dataset_dict(aggregate=True)
    
    return ds_dict

# Alternative: Use OpenDAP for streaming access
def stream_cmip6_opendap(opendap_url, haiti_bounds):
    """Stream data via OpenDAP without full download"""
    
    # Open remote dataset
    ds = xr.open_dataset(opendap_url)
    
    # Subset to Haiti bounds immediately
    ds_haiti = ds.sel(
        lon=slice(haiti_bounds['min_lon'], haiti_bounds['max_lon']),
        lat=slice(haiti_bounds['min_lat'], haiti_bounds['max_lat'])
    )
    
    # Load only Haiti subset into memory
    ds_haiti = ds_haiti.load()
    
    return ds_haiti

```

```{python, echo = FALSE, eval = FALSE}
# ============================================================================
# Main Processing Pipeline
# ============================================================================

def run_haiti_climate_pipeline(models, scenarios, periods, use_opendap=False):
    """Main pipeline using multiple download strategies"""
    
    downloader = CMIP6Downloader()
    results = []
    
    for model in models:
        for scenario in scenarios:
            for period in periods:
                print(f"\n{'='*60}")
                print(f"Processing: {model} - {scenario} - {period}")
                print('='*60)
                
                # Strategy 1: Try WorldClim with smart download
                haiti_file = downloader.download_with_crop(
                    url=None,  # Built in method
                    model=model,
                    scenario=scenario,
                    period=period
                )
                
                if haiti_file:
                    results.append({
                        'model': model,
                        'scenario': scenario,
                        'period': period,
                        'file': haiti_file,
                        'source': 'worldclim'
                    })
                    continue
                
                # Strategy 2: Try Pangeo cloud data
                if use_opendap:
                    print("  Trying Pangeo cloud catalog...")
                    try:
                        ds = get_cmip6_from_pangeo(model, scenario.replace('ssp', 'ssp'))
                        if ds:
                            # Process and save
                            haiti_file = process_pangeo_data(ds, model, scenario, period)
                            results.append({
                                'model': model,
                                'scenario': scenario,
                                'period': period,
                                'file': haiti_file,
                                'source': 'pangeo'
                            })
                            continue
                    except Exception as e:
                        print(f"  Pangeo failed: {e}")
                
                print(f"  ✗ Could not obtain data for this combination")
    
    return pd.DataFrame(results)

# Parallel download with threading
def parallel_download_batch(download_tasks, max_workers=3):
    """Download multiple files in parallel with controlled concurrency"""
    
    downloader = CMIP6Downloader()
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_task = {
            executor.submit(
                downloader.download_with_crop,
                None,  # URL built internally
                task['model'],
                task['scenario'],
                task['period']
            ): task for task in download_tasks
        }
        
        # Process completed tasks
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                result = future.result()
                if result:
                    print(f"✓ Completed: {task['model']} {task['scenario']} {task['period']}")
                    results.append(result)
            except Exception as e:
                print(f"✗ Failed: {task['model']} {task['scenario']} {task['period']}: {e}")
    
    return results

# ============================================================================
# Execute - FULL DATASET
# ============================================================================
if __name__ == "__main__":
    # Complete list of available models
    models = [
        'ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 
        'CanESM5', 'CanESM5-CanOE', 'CMCC-ESM2', 'CNRM-CM6-1', 
        'CNRM-ESM2-1', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FIO-ESM-2-0', 
        'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 
        'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'MIROC-ES2L', 
        'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'UKESM1-0-LL'
    ]
    
    # All SSP scenarios
    scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']
    
    # All available time periods
    periods = [
        '2021-2040', '2041-2060', '2061-2080', '2081-2100'
    ]
    
    print(f"Starting full download pipeline:")
    print(f"  Models: {len(models)} ({', '.join(models[:3])}...)")
    print(f"  Scenarios: {len(scenarios)} ({', '.join(scenarios)})")
    print(f"  Periods: {len(periods)} ({', '.join(periods)})")
    print(f"  Total combinations: {len(models) * len(scenarios) * len(periods)}")
    print(f"  Estimated time: {(len(models) * len(scenarios) * len(periods)) * 2} minutes")
    
    # Add progress tracking
    import time
    start_time = time.time()
    
    # Run the full pipeline
    results_df = run_haiti_climate_pipeline(models, scenarios, periods, use_opendap=False)
    
    # Calculate runtime
    runtime = time.time() - start_time
    hours = runtime // 3600
    minutes = (runtime % 3600) // 60
    
    # Save comprehensive results
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results_file = f'haiti_climate_data_complete_{timestamp}.csv'
    results_df.to_csv(results_file, index=False)
    
    # Print final summary
    print(f"\n{'='*80}")
    print(f"PIPELINE COMPLETE!")
    print(f"{'='*80}")
    print(f"Runtime: {int(hours)}h {int(minutes)}m")
    print(f"Successfully downloaded: {len(results_df)}/{len(models) * len(scenarios) * len(periods)} datasets")
    print(f"Success rate: {len(results_df)/(len(models) * len(scenarios) * len(periods))*100:.1f}%")
    print(f"Results saved to: {results_file}")
    
    # Show breakdown by model
    if len(results_df) > 0:
        print(f"\nSuccess by model:")
        model_success = results_df.groupby('model').size().sort_values(ascending=False)
        for model, count in model_success.head(10).items():
            print(f"  {model}: {count}/{len(scenarios)*len(periods)} datasets")
```

```{python, echo = FALSE, eval = FALSE}

import os
import time
import requests
from pathlib import Path

# Configuration
OUTPUT_DIR = Path(r"C:\Users\ianpsheasmith\OneDrive - University of Florida\Documents - Haiti Vector\General\cmip6_haiti_python")
TEMP_DIR = OUTPUT_DIR / "temp"
TEMP_DIR.mkdir(exist_ok=True)

# All expected models and scenarios
ALL_MODELS = [
    'ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 
    'CanESM5', 'CanESM5-CanOE', 'CMCC-ESM2', 'CNRM-CM6-1', 
    'CNRM-ESM2-1', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FIO-ESM-2-0', 
    'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 
    'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'MIROC-ES2L', 
    'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'UKESM1-0-LL'
]

SCENARIOS = ['ssp126', 'ssp245', 'ssp370', 'ssp585']
PERIODS = ['2021-2040', '2041-2060', '2061-2080', '2081-2100']

def check_existing_files():
    """Check which files already exist"""
    existing = []
    for file in OUTPUT_DIR.glob("haiti_*.tif"):
        existing.append(file.name)
    return existing

def identify_missing_files():
    """Identify which files need to be downloaded"""
    existing = check_existing_files()
    missing = []
    
    for model in ALL_MODELS:
        for scenario in SCENARIOS:
            for period in PERIODS:
                filename = f"haiti_wc2.1_30s_bioc_{model}_{scenario}_{period}.tif"
                if filename not in existing:
                    missing.append({
                        'model': model,
                        'scenario': scenario,
                        'period': period,
                        'filename': filename
                    })
    
    return missing

def download_file(url, output_path, max_retries=3):
    """Download with retry logic and resume capability"""
    
    for attempt in range(max_retries):
        try:
            # Check if we can resume
            headers = {}
            if output_path.exists():
                resume_size = output_path.stat().st_size
                headers['Range'] = f'bytes={resume_size}-'
                print(f"  Resuming from {resume_size/1024**2:.1f} MB")
            
            response = requests.get(url, headers=headers, stream=True, timeout=1800)
            
            # Write file
            mode = 'ab' if output_path.exists() else 'wb'
            with open(output_path, mode) as f:
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                for chunk in response.iter_content(chunk_size=8192*1024):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            pct = (downloaded / total_size) * 100
                            print(f"\r  Progress: {pct:.1f}%", end='', flush=True)
                
                print()  # New line
                return True
                
        except Exception as e:
            print(f"\n  Attempt {attempt+1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(10)
    
    return False

def download_missing_files(max_files=None):
    """Main function to download missing files"""
    
    print("="*60)
    print("CMIP6 Missing Files Downloader")
    print("="*60)
    
    # Check what's missing
    missing = identify_missing_files()
    existing = check_existing_files()
    
    print(f"\nInventory:")
    print(f"  Existing files: {len(existing)}")
    print(f"  Missing files: {len(missing)}")
    print(f"  Total expected: 400")
    print(f"  Completion: {len(existing)/400*100:.1f}%")
    
    if len(missing) == 0:
        print("\nAll files downloaded!")
        return
    
    # Show missing by model
    missing_by_model = {}
    for m in missing:
        model = m['model']
        if model not in missing_by_model:
            missing_by_model[model] = 0
        missing_by_model[model] += 1
    
    print("\nMissing files by model:")
    for model, count in sorted(missing_by_model.items()):
        print(f"  {model}: {count} files")
    
    # Limit downloads if requested
    if max_files:
        missing = missing[:max_files]
        print(f"\nLimiting to {max_files} downloads")
    
    # Download missing files
    print(f"\nStarting downloads for {len(missing)} files...")
    print("="*60)
    
    success_count = 0
    fail_count = 0
    
    for i, file_info in enumerate(missing, 1):
        print(f"\n[{i}/{len(missing)}] {file_info['filename']}")
        
        # Build URL
        model = file_info['model']
        scenario = file_info['scenario']
        period = file_info['period']
        
        base_filename = f"wc2.1_30s_bioc_{model}_{scenario}_{period}.tif"
        url = f"https://geodata.ucdavis.edu/cmip6/30s/{model}/{scenario}/{base_filename}"
        
        # Download to temp first
        temp_path = TEMP_DIR / base_filename
        
        print(f"  URL: {url}")
        print(f"  Downloading...")
        
        if download_file(url, temp_path):
            # If successful, move to final location
            # Here you would normally crop to Haiti, but for now just rename
            final_path = OUTPUT_DIR / file_info['filename']
            
            # For now, just copy the file (you'll need to add cropping)
            temp_path.rename(final_path)
            
            print(f"  ✓ Success! Saved as {file_info['filename']}")
            success_count += 1
        else:
            print(f"  ✗ Failed to download")
            fail_count += 1
            if temp_path.exists():
                temp_path.unlink()  # Clean up partial download
        
        # Pause between downloads
        if i < len(missing):
            print(f"  Pausing 10 seconds...")
            time.sleep(10)
    
    # Final summary
    print("\n" + "="*60)
    print("Download Summary:")
    print(f"  Successful: {success_count}")
    print(f"  Failed: {fail_count}")
    print(f"  New total: {len(existing) + success_count}/400 files")
    print(f"  New completion: {(len(existing) + success_count)/400*100:.1f}%")
    print("="*60)

if __name__ == "__main__":
    download_missing_files()
    
import os
import glob
from pathlib import Path
import rasterio
from rasterio.windows import from_bounds
import pandas as pd
import time
import shutil

def cleanup_and_validate_rasters(data_dir, size_threshold_small=1000, size_threshold_large=2000):
    """
    Clean up downloaded rasters with better file handling
    """
    
    print(f"Looking for .tif files in: {os.path.abspath(data_dir)}")
    
    # Get all .tif files
    tif_files = glob.glob(os.path.join(data_dir, "*.tif"))
    print(f"Found {len(tif_files)} .tif files to validate")
    
    removed_files = []
    recropped_files = []
    valid_files = []
    failed_files = []
    
    # Haiti bounding box (left, bottom, right, top)
    haiti_bbox = (-74.5, 18.0, -71.6, 20.1)
    
    for tif_file in tif_files:
        file_size_kb = os.path.getsize(tif_file) / 1024
        file_name = os.path.basename(tif_file)
        
        print(f"\nProcessing: {file_name} ({file_size_kb:.1f} KB)")
        
        # Remove files that are too small (failed downloads)
        if file_size_kb < size_threshold_small:
            print(f"  ✗ Removing (too small): {file_size_kb:.1f} KB")
            try:
                os.remove(tif_file)
                removed_files.append(file_name)
            except PermissionError:
                print(f"    ⚠ Could not remove (file in use), marking for manual deletion")
                failed_files.append(file_name)
            continue
            
        # Check if file needs re-cropping (too large, likely global)
        if file_size_kb > size_threshold_large:
            print(f"  ✂ Re-cropping (too large): {file_size_kb:.1f} KB")
            
            # Try multiple times with delays for file locking issues
            success = False
            for attempt in range(3):
                try:
                    # Wait a bit for file locks to clear
                    if attempt > 0:
                        time.sleep(2)
                        print(f"    Retry attempt {attempt + 1}/3...")
                    
                    # Read the raster
                    with rasterio.open(tif_file) as src:
                        bounds = src.bounds
                        width = bounds.right - bounds.left
                        height = bounds.top - bounds.bottom
                        
                        if width > 10 or height > 10:  # Likely too big for Haiti
                            print(f"    Large extent detected: {width:.1f}° × {height:.1f}°")
                            
                            # Create window for Haiti cropping
                            window = from_bounds(*haiti_bbox, src.transform)
                            
                            # Read the windowed data
                            out_image = src.read(window=window)
                            out_transform = src.window_transform(window)
                            
                            # Update metadata
                            out_meta = src.meta.copy()
                            out_meta.update({
                                "driver": "GTiff",
                                "height": out_image.shape[1],
                                "width": out_image.shape[2],
                                "transform": out_transform,
                                "compress": "lzw"
                            })
                    
                    # Create temp file with different name to avoid conflicts
                    temp_file = os.path.join(data_dir, f"temp_cropped_{int(time.time())}_{file_name}")
                    
                    # Write cropped version
                    with rasterio.open(temp_file, "w", **out_meta) as dest:
                        dest.write(out_image)
                    
                    # Close all file handles and wait
                    time.sleep(1)
                    
                    # Replace original with cropped version
                    if os.path.exists(temp_file):
                        # Try to remove original
                        try:
                            os.remove(tif_file)
                            time.sleep(0.5)
                            os.rename(temp_file, tif_file)
                            
                            new_size_kb = os.path.getsize(tif_file) / 1024
                            print(f"    ✓ Cropped: {file_size_kb:.1f} KB → {new_size_kb:.1f} KB")
                            recropped_files.append(file_name)
                            valid_files.append(file_name)
                            success = True
                            break
                            
                        except (PermissionError, OSError) as e:
                            print(f"    ⚠ Could not replace original file: {e}")
                            # Keep the cropped version with a new name
                            cropped_name = file_name.replace('.tif', '_cropped.tif')
                            final_path = os.path.join(data_dir, cropped_name)
                            if os.path.exists(final_path):
                                os.remove(final_path)
                            os.rename(temp_file, final_path)
                            print(f"    ✓ Saved as: {cropped_name}")
                            recropped_files.append(cropped_name)
                            valid_files.append(cropped_name)
                            failed_files.append(f"{file_name} (original too large, see {cropped_name})")
                            success = True
                            break
                    
                except Exception as e:
                    print(f"    ✗ Attempt {attempt + 1} failed: {e}")
                    if os.path.exists(temp_file):
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                    continue
            
            if not success:
                print(f"    ✗ Failed to process after 3 attempts")
                failed_files.append(file_name)
                continue
        else:
            print(f"  ✓ Valid size: {file_size_kb:.1f} KB")
            valid_files.append(file_name)
    
    # Summary report
    print(f"\n{'='*80}")
    print(f"CLEANUP COMPLETE")
    print(f"{'='*80}")
    print(f"Files removed (too small): {len(removed_files)}")
    print(f"Files re-cropped (too large): {len(recropped_files)}")
    print(f"Valid files remaining: {len(valid_files)}")
    print(f"Failed to process: {len(failed_files)}")
    
    if failed_files:
        print(f"\nFiles that need manual attention:")
        for f in failed_files:
            print(f"  - {f}")
    
    return {
        'removed': removed_files,
        'recropped': recropped_files,
        'valid': valid_files,
        'failed': failed_files
    }

# ============================================================================
# Execute cleanup
# ============================================================================
if __name__ == "__main__":
    data_directory = r"C:\Users\ianpsheasmith\OneDrive - University of Florida\Documents - Haiti Vector\General\cmip6_haiti_python"
    
    print("Starting raster cleanup and validation...")
    print("Note: Large files will be processed with retry logic for file locking issues")
    
    results = cleanup_and_validate_rasters(data_directory)
    
    if results:
        print(f"\nCleanup completed!")
        if results['failed']:
            print(f"Some files couldn't be processed automatically - check the failed files list above")
    else:
        print("No .tif files found - check the directory path")
```

```{r, echo = FALSE}

  #| label: parallel-processing-setup
  #| code-summary: "Configure parallel processing for ensemble modeling"
  
  # Configure parallel processing - use half of available cores
    n_cores <- max(1, floor(detectCores() / 2))
    cl <- makeCluster(n_cores)
    registerDoParallel(cl)
    cat("Configured parallel processing with", n_cores, "cores (half of available)\n")
  
  # Export necessary objects and functions to cluster
    clusterEvalQ(cl, {
        library(dismo)
        library(gbm)
        library(raster)
        library(sp)
        library(dplyr)
    })

```

```{r, echo = FALSE}

  #| label: pseudo-absence-functions
  #| code-summary: "Define functions for pseudo-absence generation and data preparation"
  
  # Define function to generate spatially-stratified pseudo-absence points
    generate_pseudo_absences_aeae <- function(presence_points, raster_stack,
                                            buffer_radius = 717, n_pseudo = 60, seed_val = 123) {
        set.seed(seed_val)
        presence_sp <- SpatialPoints(presence_points, proj4string = crs(raster_stack))
        presence_buffers <- buffer(presence_sp, width = buffer_radius)
        buffer_mask <- rasterize(presence_buffers, raster_stack, field = 1, background = NA)
        valid_area <- mask(raster_stack[[1]], buffer_mask, inverse = TRUE)
        pseudo_points <- randomPoints(valid_area, n = n_pseudo)
        pseudo_df <- as.data.frame(pseudo_points)
        colnames(pseudo_df) <- c("x", "y")
        return(pseudo_df)
    }
  
  # Define function to prepare modeling dataset with scaling
    prepare_modeling_data <- function(presence_data, pseudo_absence_data) {
        presence_labeled <- cbind(Presence = 1, presence_data)
        pseudo_labeled <- cbind(Presence = 0, pseudo_absence_data)
        combined_data <- rbind(presence_labeled, pseudo_labeled)
        combined_data <- as.data.frame(combined_data)
        combined_data$Presence <- as.numeric(combined_data$Presence)
        clean_data <- na.omit(combined_data)
        
        numeric_cols <- sapply(clean_data, is.numeric)
        if (any(numeric_cols)) {
            for (col in names(clean_data)[numeric_cols]) {
                if (col != "Presence") {
                    clean_data[[col]][is.infinite(clean_data[[col]])] <- NA
                }
            }
            clean_data <- na.omit(clean_data)
        }
        
        # Scale all predictor variables (excluding Presence)
          predictor_cols <- names(clean_data)[names(clean_data) != "Presence"]
          scaled_predictors <- scale(clean_data[, predictor_cols])
          
        # Combine scaled predictors with Presence variable
          scaled_data <- data.frame(
              Presence = clean_data$Presence,
              scaled_predictors
          )
        
        # Store scaling parameters
          scaling_params <- list(
              center = attr(scaled_predictors, "scaled:center"),
              scale = attr(scaled_predictors, "scaled:scale")
          )
          attr(scaled_data, "scaling_params") <- scaling_params
          return(scaled_data)
    }
  
    cat("Pseudo-absence generation functions defined\n")

```

```{r, echo = FALSE}

#| label: generate-pseudo-absence-datasets
#| code-summary: "Generate pseudo-absence datasets with NA handling and presence point relocation"

# Function to relocate presence points that fall on NA values
relocate_na_presence_points <- function(presence_points, raster_stack, search_radius = 100) {
    # Extract environmental values
    env_values <- extract(raster_stack, presence_points)
    na_rows <- which(apply(env_values, 1, function(x) any(is.na(x))))
    
    if (length(na_rows) == 0) {
        return(presence_points)
    }
    
    cat("Found", length(na_rows), "presence points with NA environmental values\n")
    cat("Relocating to nearest valid cells...\n")
    
    # Create a copy of presence points
    adjusted_points <- presence_points
    
    # Process each problematic point
    relocated_info <- purrr::map_dfr(na_rows, function(idx) {
        original_point <- presence_points[idx, ]
        
        # Create a small buffer around the point to search for valid cells
        point_sp <- SpatialPoints(matrix(c(original_point$x, original_point$y), ncol = 2),
                                 proj4string = crs(raster_stack))
        buffer_zone <- buffer(point_sp, width = search_radius)
        
        # Sample candidate points within the buffer
        candidate_points <- spsample(buffer_zone, n = 100, type = "regular")
        candidate_coords <- coordinates(candidate_points)
        
        # Check which candidates have valid environmental data
        candidate_env <- extract(raster_stack, candidate_coords)
        valid_candidates <- !apply(candidate_env, 1, function(x) any(is.na(x)))
        
        if (any(valid_candidates)) {
            # Find the closest valid point
            valid_coords <- candidate_coords[valid_candidates, , drop = FALSE]
            distances <- sqrt((valid_coords[, 1] - original_point$x)^2 + 
                            (valid_coords[, 2] - original_point$y)^2)
            closest_idx <- which.min(distances)
            new_coords <- valid_coords[closest_idx, ]
            
            adjusted_points[idx, ] <- data.frame(x = new_coords[1], y = new_coords[2])
            
            return(data.frame(
                point_id = idx,
                original_x = original_point$x,
                original_y = original_point$y,
                new_x = new_coords[1],
                new_y = new_coords[2],
                distance_moved = distances[closest_idx],
                relocated = TRUE
            ))
        } else {
            return(data.frame(
                point_id = idx,
                original_x = original_point$x,
                original_y = original_point$y,
                new_x = NA,
                new_y = NA,
                distance_moved = NA,
                relocated = FALSE
            ))
        }
    })
    
    # Report relocation results
    successfully_relocated <- sum(relocated_info$relocated, na.rm = TRUE)
    cat("Successfully relocated", successfully_relocated, "of", length(na_rows), "points\n")
    
    if (successfully_relocated < length(na_rows)) {
        failed_points <- relocated_info[!relocated_info$relocated, ]
        cat("Warning: Could not relocate", nrow(failed_points), "points\n")
        # Remove points that couldn't be relocated
        adjusted_points <- adjusted_points[-failed_points$point_id, ]
    }
    
    return(list(
        adjusted_points = adjusted_points,
        relocation_info = relocated_info
    ))
}

# Enhanced function that ensures we get exactly n_pseudo valid points
generate_pseudo_absences_aeae_fixed <- function(presence_points, raster_stack,
                                                buffer_radius = 717, n_pseudo = 60, 
                                                seed_val = 123, max_attempts = 10) {
    set.seed(seed_val)
    presence_sp <- SpatialPoints(presence_points, proj4string = crs(raster_stack))
    presence_buffers <- buffer(presence_sp, width = buffer_radius)
    buffer_mask <- rasterize(presence_buffers, raster_stack, field = 1, background = NA)
    valid_area <- mask(raster_stack[[1]], buffer_mask, inverse = TRUE)
    
    # Generate extra points to account for NAs
    generate_batch <- function(n_needed, valid_area_raster, raster_stack_full) {
        n_to_generate <- ceiling(n_needed * 1.5)
        candidate_points <- randomPoints(valid_area_raster, n = n_to_generate)
        candidate_df <- as.data.frame(candidate_points)
        colnames(candidate_df) <- c("x", "y")
        
        # Check which points have valid environmental data
        env_values <- extract(raster_stack_full, candidate_df)
        valid_rows <- !apply(env_values, 1, function(x) any(is.na(x)))
        
        candidate_df[valid_rows, ]
    }
    
    # Iteratively generate points until we have enough
    all_valid_points <- data.frame()
    attempt <- 0
    
    while (nrow(all_valid_points) < n_pseudo && attempt < max_attempts) {
        attempt <- attempt + 1
        n_needed <- n_pseudo - nrow(all_valid_points)
        new_points <- generate_batch(n_needed, valid_area, raster_stack)
        all_valid_points <- rbind(all_valid_points, new_points)
    }
    
    # Return exactly n_pseudo points
    if (nrow(all_valid_points) >= n_pseudo) {
        return(all_valid_points[1:n_pseudo, ])
    } else {
        warning(paste("Could only generate", nrow(all_valid_points), "valid pseudo-absence points"))
        return(all_valid_points)
    }
}

# Set parameters
n_replicates <- 30
n_pseudo_per_replicate <- 60
buffer_radius <- 717  # meters

cat("Generating", n_replicates, "pseudo-absence datasets for Aedes aegypti\n")
cat("Configuration:\n")
cat("  Pseudo-absence points per replicate:", n_pseudo_per_replicate, "\n")
cat("  Buffer radius:", buffer_radius, "meters\n")
cat("  Search radius for point relocation: 100 meters\n\n")

# Prepare presence points coordinates
AeaePoints_df <- data.frame(
    x = AeaePoints$Longitude,
    y = AeaePoints$Latitude
)

# First, investigate the NA issue
cat("Investigating presence points with NA environmental values:\n")
presence_env_test <- extract(BRT_All_Covars, AeaePoints_df)
na_rows <- which(apply(presence_env_test, 1, function(x) any(is.na(x))))

if (length(na_rows) > 0) {
    cat("Points with NA values (indices):", na_rows, "\n")
    cat("Coordinates of problematic points:\n")
    print(AeaePoints_df[na_rows, ])
    
    # Check which variables have NAs
    na_vars <- purrr::map_chr(na_rows, function(idx) {
        na_cols <- which(is.na(presence_env_test[idx, ]))
        paste(names(BRT_All_Covars)[na_cols], collapse = ", ")
    })
    cat("Variables with NA values for each point:\n")
    purrr::walk2(na_rows, na_vars, ~ cat("  Point", .x, ":", .y, "\n"))
}

# Relocate presence points that fall on NA values
cat("\nRelocating presence points with NA values...\n")
relocation_result <- relocate_na_presence_points(AeaePoints_df, BRT_All_Covars)
AeaePoints_df_adjusted <- relocation_result$adjusted_points

cat("Adjusted presence points:", nrow(AeaePoints_df_adjusted), 
    "(originally", nrow(AeaePoints_df), ")\n\n")

# Function to process a single replicate
process_replicate <- function(i, presence_points_adjusted, raster_stack, 
                            buffer_radius, n_pseudo_per_rep) {
    seed_val <- 1999 + i
    cat("Processing replicate", i, "of 30... ")
    
    # Generate pseudo-absence points with NA handling
    pseudo_points <- generate_pseudo_absences_aeae_fixed(
        presence_points = presence_points_adjusted,
        raster_stack = raster_stack,
        buffer_radius = buffer_radius,
        n_pseudo = n_pseudo_per_rep,
        seed_val = seed_val
    )
    
    # Extract environmental data for presence points
    presence_env <- extract(raster_stack, presence_points_adjusted)
    presence_data <- cbind(presence_points_adjusted, presence_env)
    
    # Extract environmental data for pseudo-absence points
    pseudo_env <- extract(raster_stack, pseudo_points)
    pseudo_data <- cbind(pseudo_points, pseudo_env)
    
    # Prepare the combined modeling dataset with scaling
    modeling_data <- prepare_modeling_data(
        presence_data = presence_data,
        pseudo_absence_data = pseudo_data
    )
    
    # Store metadata about the replicate
    replicate_info <- list(
        pseudo_points = pseudo_points,
        seed = seed_val,
        n_pseudo = nrow(pseudo_points),
        n_presence = sum(modeling_data$Presence == 1),
        n_absence = sum(modeling_data$Presence == 0),
        total_points = nrow(modeling_data)
    )
    
    cat("completed (", nrow(modeling_data), "points )\n")
    
    return(list(
        dataset = modeling_data,
        info = replicate_info
    ))
}

# Generate all replicates using purrr::map
replicate_results <- purrr::map(
    1:n_replicates,
    ~ process_replicate(.x, 
                       AeaePoints_df_adjusted, 
                       BRT_All_Covars,
                       buffer_radius,
                       n_pseudo_per_replicate)
)

# Extract datasets and assign to global environment
purrr::walk2(
    replicate_results,
    1:n_replicates,
    ~ {
        dataset_name <- paste0("AeaeData_PseudoAbs_", .y)
        assign(dataset_name, .x$dataset, envir = .GlobalEnv)
    }
)

# Store replicate information
pseudo_absence_replicates <- purrr::map(replicate_results, ~ .x$info)

# Summary statistics
point_counts <- purrr::map_dbl(replicate_results, ~ .x$info$total_points)
presence_counts <- purrr::map_dbl(replicate_results, ~ .x$info$n_presence)
absence_counts <- purrr::map_dbl(replicate_results, ~ .x$info$n_absence)

cat("\n===============================================\n")
cat("PSEUDO-ABSENCE GENERATION COMPLETE\n")
cat("===============================================\n")
cat("Successfully created", n_replicates, "datasets\n\n")

cat("Dataset statistics:\n")
cat("  Total points per dataset:", unique(point_counts), "\n")
cat("  Presence points per dataset:", unique(presence_counts), "\n")
cat("  Pseudo-absence points per dataset:", unique(absence_counts), "\n")

# Get one example dataset for variable names
example_data <- replicate_results[[1]]$dataset
cat("\nEnvironmental variables:\n")
variable_names <- names(example_data)[!(names(example_data) %in% c("Presence", "x", "y"))]
cat("  ", paste(variable_names, collapse = "\n  "), "\n")

# Verification
dataset_names <- paste0("AeaeData_PseudoAbs_", 1:n_replicates)
datasets_exist <- purrr::map_lgl(dataset_names, exists)
cat("\nDataset verification:", sum(datasets_exist), "/", n_replicates, 
    "datasets successfully created\n")

# Store relocation info if points were relocated
if (exists("relocation_result") && !is.null(relocation_result$relocation_info)) {
    assign("AeaePoints_relocation_info", relocation_result$relocation_info, envir = .GlobalEnv)
    cat("\nRelocation information saved to 'AeaePoints_relocation_info'\n")
}
```

```{r}

#| label: visualize-pseudo-absence-distribution
#| code-summary: "Visualize pseudo-absence point distribution"
#| fig-cap: "Spatial distribution of presence points and sample pseudo-absence replicates"

# Create visualization of presence points and first few pseudo-absence sets
plot_list <- list()

# Base map with presence points
base_plot <- ggplot() +
  geom_sf(data = haiti_wo_lakes_reproj, fill = "white", color = "gray50", size = 0.3) +
  geom_point(data = as.data.frame(AeaePoints_df), 
             aes(x = x, y = y), 
             color = "#702963", size = 2, alpha = 0.8) +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  labs(title = "Aedes aegypti Presence Points",
       subtitle = paste(nrow(AeaePoints_df), "presence locations"))

plot_list[[1]] <- base_plot

# Add plots for first 3 pseudo-absence replicates
for (i in 1:3) {
  rep_data <- pseudo_absence_replicates[[i]]
  pseudo_df <- rep_data$pseudo_points
  
  p <- ggplot() +
    geom_sf(data = haiti_wo_lakes_reproj, fill = "white", color = "gray50", size = 0.3) +
    geom_point(data = as.data.frame(AeaePoints_df), 
               aes(x = x, y = y), 
               color = "#702963", size = 1.5, alpha = 0.7) +
    geom_point(data = pseudo_df, 
               aes(x = x, y = y), 
               color = "#2E86AB", size = 1, alpha = 0.6) +
    theme_minimal() +
    theme(
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.grid = element_blank()
    ) +
    labs(title = paste("Replicate", i),
         subtitle = paste(nrow(pseudo_df), "pseudo-absence points"))
  
  plot_list[[i + 1]] <- p
}

# Arrange plots
grid_plot <- do.call(grid.arrange, c(plot_list, ncol = 2))
print(grid_plot)

```

```{r}

  #| label: visualize-pseudo-absence-distribution-interactive
  #| code-summary: "Interactive leaflet map with toggleable pseudo-absence replicates"
  #| fig-cap: "Interactive map showing presence points and toggleable pseudo-absence replicates"
  
  # Create color palette for different replicates
    n_replicates_to_show <- min(30, length(pseudo_absence_replicates))
    replicate_colors <- brewer.pal(min(11, n_replicates_to_show), "Spectral")
    if (n_replicates_to_show > 11) {
      # If more than 11 replicates, create additional colors
      additional_colors <- rainbow(n_replicates_to_show - 11, start = 0.7, end = 1)
      replicate_colors <- c(replicate_colors, additional_colors)
    }
  
  # Initialize the leaflet map
    interactive_pseudo_map <- leaflet() %>%
      addTiles() %>%
      setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
      
      # Add Haiti boundary
        addPolygons(
          data = haiti_wo_lakes_reproj,
          fillColor = "lightgray",
          fillOpacity = 0.1,
          color = "black",
          weight = 2,
          popup = "Haiti (excluding water bodies)"
        ) %>%
      
      # Add presence points (always visible)
        addCircleMarkers(
          data = AeaePoints_sf,
          radius = 6,
          fillColor = "#702963",
          color = "white",
          weight = 2,
          opacity = 1,
          fillOpacity = 0.9,
          popup = ~paste("<b>Aedes aegypti presence</b><br>",
                         "Site ID:", row.names(AeaePoints)),
          group = "Presence Points"
        ) %>%
      
      # Add legend for presence points
      addLegend(
        position = "topright",
        colors = "#702963",
        labels = "Aedes aegypti presence",
        title = "Species Data",
        opacity = 1
      )
  
  # Add each pseudo-absence replicate as a separate layer group
    for (i in 1:n_replicates_to_show) {
      rep_data <- pseudo_absence_replicates[[i]]
      pseudo_df <- rep_data$pseudo_points
      
      # Convert to sf object for leaflet
      pseudo_sf <- st_as_sf(pseudo_df, coords = c("x", "y"), crs = 4326)
      
      # Color for this replicate
      rep_color <- replicate_colors[i]
      
      interactive_pseudo_map <- interactive_pseudo_map %>%
        addCircleMarkers(
          data = pseudo_sf,
          radius = 4,
          fillColor = rep_color,
          color = "white",
          weight = 1,
          opacity = 0.8,
          fillOpacity = 0.7,
          popup = paste0("<b>Pseudo-absence Replicate ", i, "</b><br>",
                         "Seed: ", rep_data$seed, "<br>",
                         "Total pseudo-absence points: ", nrow(pseudo_df)),
          group = paste("Replicate", i)
        )
    }
  
  # Add layers control to toggle between replicates
    layer_groups <- c("Presence Points", paste("Replicate", 1:n_replicates_to_show))
    
    interactive_pseudo_map <- interactive_pseudo_map %>%
      addLayersControl(
        overlayGroups = layer_groups,
        options = layersControlOptions(collapsed = FALSE),
        position = "bottomleft"
      ) %>%
      
      # Hide all pseudo-absence layers initially except the first one
      hideGroup(paste("Replicate", 2:n_replicates_to_show)) %>%
      
      # Add a secondary legend for pseudo-absence replicates
      addLegend(
        position = "bottomright",
        colors = replicate_colors[1:min(5, n_replicates_to_show)],
        labels = paste("Replicate", 1:min(5, n_replicates_to_show)),
        title = "Pseudo-absence Sets<br>(Use layer control to toggle)",
        opacity = 0.7
      )
  
  # Print the interactive map
    print(interactive_pseudo_map)

```

```{r BRT ENSEMBLE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#             HAITI AEDES AEGYPTI ENSEMBLE BRT MODELING            #
#             WITH COORDINATE FIXES AND PROGRESS TRACKING           #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# Load required libraries
suppressPackageStartupMessages({
    library(gbm)
    library(dplyr)
    library(purrr)
    library(raster)
    library(parallel)
    library(doParallel)
    library(sf)
    library(sp)
})

# Helper function for null coalescing
`%||%` <- function(x, y) if (is.null(x) || length(x) == 0 || any(is.na(x))) y else x

# Progress tracking function
update_progress <- function(current, total, prefix = "Progress") {
    percent <- round((current / total) * 100, 1)
    bar_length <- 40
    filled_length <- round(bar_length * current / total)
    bar <- paste0(c(rep("█", filled_length), rep("░", bar_length - filled_length)), collapse = "")
    cat("\r", prefix, ": [", bar, "] ", percent, "% (", current, "/", total, ")", sep = "")
    if (current == total) cat("\n")
    flush.console()
}

# Configure parallel processing
cat("=== HAITI AEDES AEGYPTI ENSEMBLE MODELING ===\n")
cat("Setting up parallel processing...\n")
cl <- makeCluster(15)
registerDoParallel(cl)

# Prepare Haiti boundary for masking
cat("Preparing Haiti boundary for masking...\n")
if (exists("Haiti_adm3")) {
    Haiti_boundary_union <- st_union(Haiti_adm3)
    Haiti_boundary_sp <- as(Haiti_boundary_union, "Spatial")
} else {
    stop("Haiti_adm3 shapefile not found. Please load it first.")
}

# Enhanced BRT fitting function with variable selection and coordinate removal
fit_brt_with_selection <- function(data, weights, predictors = NULL) {
    tryCatch({
        # CRITICAL: Remove coordinate columns before any processing
        coordinate_cols <- c("Longitude", "Latitude", "x", "y", "X", "Y", "lon", "lat")
        coord_present <- names(data)[names(data) %in% coordinate_cols]
        
        if (length(coord_present) > 0) {
            data <- data[, !names(data) %in% coordinate_cols, drop = FALSE]
            cat("    Removed coordinate columns:", paste(coord_present, collapse = ", "), "\n")
        }
        
        # Verify we still have predictors after coordinate removal
        non_response_cols <- names(data)[names(data) != "Presence"]
        if (length(non_response_cols) == 0) {
            return(NULL)
        }
        
        # Select specific predictors if provided
        if (!is.null(predictors)) {
            available_predictors <- predictors[predictors %in% names(data)]
            if (length(available_predictors) == 0) {
                return(NULL)
            }
            model_data <- data[, c("Presence", available_predictors)]
        } else {
            model_data <- data
        }
        
        # Initial model fit
        initial_model <- gbm(
            Presence ~ ., 
            data = model_data, 
            distribution = "bernoulli",
            n.trees = 2500, 
            interaction.depth = 5, 
            shrinkage = 0.005, 
            bag.fraction = 0.5,
            cv.folds = 10, 
            weights = weights, 
            n.minobsinnode = 10, 
            verbose = FALSE
        )
        
        if (is.null(initial_model)) return(NULL)
        
        # Variable selection with iterative removal
        current_model <- initial_model
        current_predictors <- names(model_data)[names(model_data) != "Presence"]
        iteration <- 0
        max_iterations <- 20
        
        while (iteration < max_iterations) {
            iteration <- iteration + 1
            
            rel_inf <- summary(current_model, plot = FALSE)
            low_importance_vars <- rel_inf[rel_inf$rel.inf < 5.0, "var"]
            
            if (length(low_importance_vars) == 0 || length(current_predictors) <= 3) {
                break
            }
            
            # Remove variable with lowest importance
            var_to_remove <- low_importance_vars[which.min(
                rel_inf[rel_inf$var %in% low_importance_vars, "rel.inf"])]
            current_predictors <- setdiff(current_predictors, var_to_remove)
            
            # Refit model with reduced variables
            reduced_data <- data[, c("Presence", current_predictors), drop = FALSE]
            
            updated_model <- tryCatch({
                gbm(
                    Presence ~ ., 
                    data = reduced_data, 
                    distribution = "bernoulli",
                    n.trees = 2500, 
                    interaction.depth = 5, 
                    shrinkage = 0.005, 
                    bag.fraction = 0.5,
                    cv.folds = 10, 
                    weights = weights, 
                    n.minobsinnode = 10, 
                    verbose = FALSE
                )
            }, error = function(e) {
                current_predictors <<- c(current_predictors, var_to_remove)
                return(current_model)
            })
            
            if (!is.null(updated_model)) {
                current_model <- updated_model
            }
        }
        
        return(current_model)
        
    }, error = function(e) {
        return(NULL)
    })
}

# AUC calculation function
calculate_auc <- function(observed, predicted) {
    tryCatch({
        if (requireNamespace("pROC", quietly = TRUE)) {
            as.numeric(pROC::auc(pROC::roc(observed, predicted, quiet = TRUE)))
        } else {
            # Manual AUC calculation
            pred_order <- order(predicted, decreasing = TRUE)
            pos_ranks <- which(observed[pred_order] == 1)
            n_pos <- sum(observed == 1)
            n_neg <- sum(observed == 0)
            if (n_pos == 0 || n_neg == 0) return(NA_real_)
            auc_val <- (sum(pos_ranks) - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)
            as.numeric(auc_val)
        }
    }, error = function(e) NA_real_)
}

# Enhanced model fitting function for a single replicate with detailed diagnostics
fit_single_replicate <- function(replicate_num, dataset_names, total_replicates) {
    dataset_name <- dataset_names[replicate_num]
    
    # Update progress
    update_progress(replicate_num - 1, total_replicates, "Model Fitting")
    
    if (!exists(dataset_name)) {
        update_progress(replicate_num, total_replicates, "Model Fitting")
        return(list(
            replicate = replicate_num,
            dataset_name = dataset_name,
            success = FALSE,
            error = "Dataset not found"
        ))
    }
    
    tryCatch({
        # Load and prepare data
        raw_data <- get(dataset_name)
        clean_data <- raw_data %>%
            mutate(Presence = as.numeric(Presence)) %>%
            na.omit()
        
        # Comprehensive coordinate column removal with diagnostic reporting
        coordinate_cols <- c("Longitude", "Latitude", "x", "y", "X", "Y", "lon", "lat")
        found_coords <- names(clean_data)[names(clean_data) %in% coordinate_cols]
        
        if (length(found_coords) > 0) {
            cat("  Replicate", replicate_num, "- Removing coordinates:", paste(found_coords, collapse = ", "), "\n")
            clean_data <- clean_data[, !names(clean_data) %in% coordinate_cols, drop = FALSE]
        }
        
        # Verify no coordinate columns remain
        remaining_coords <- names(clean_data)[names(clean_data) %in% coordinate_cols]
        if (length(remaining_coords) > 0) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = paste("Failed to remove coordinates:", paste(remaining_coords, collapse = ", "))
            ))
        }
        
        # Data quality checks
        predictor_cols <- names(clean_data)[names(clean_data) != "Presence"]
        n_presences <- sum(clean_data$Presence)
        n_absences <- sum(clean_data$Presence == 0)
        
        cat("  Replicate", replicate_num, "- Predictors:", length(predictor_cols), 
            "| Presence:", n_presences, "| Absence:", n_absences, "\n")
        
        if (n_presences <= 15) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "Insufficient presence records"
            ))
        }
        
        if (length(predictor_cols) == 0) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "No predictor variables remaining"
            ))
        }
        
        # Train/test split
        set.seed(1999 + replicate_num)
        train_indices <- sample(seq_len(nrow(clean_data)),
                                size = floor(0.75 * nrow(clean_data)))
        train_data <- clean_data[train_indices, ]
        test_data <- clean_data[-train_indices, ]
        
        # Check training data adequacy
        train_presence <- sum(train_data$Presence == 1)
        train_absence <- sum(train_data$Presence == 0)
        
        if (train_presence < 5 || train_absence < 5) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "Insufficient training data"
            ))
        }
        
        # Apply differential weighting (presence = 1.0, absence = 0.5)
        train_weights <- ifelse(train_data$Presence == 1, 1.0, 0.5)
        
        # Fit BRT model with variable selection
        final_model <- fit_brt_with_selection(train_data, train_weights)
        
        if (is.null(final_model)) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "Model fitting failed"
            ))
        }
        
        # Calculate optimal number of trees
        optimal_trees <- gbm.perf(final_model, method = "cv", plot.it = FALSE)
        
        # Extract variable importance
        importance_summary <- summary(final_model, plot = FALSE)
        final_variables <- importance_summary$var
        
        # Calculate performance metrics
        train_preds <- predict(final_model, train_data, 
                               n.trees = optimal_trees, type = "response")
        test_preds <- predict(final_model, test_data, 
                              n.trees = optimal_trees, type = "response")
        
        train_rmse <- sqrt(mean((train_data$Presence - train_preds)^2))
        test_rmse <- sqrt(mean((test_data$Presence - test_preds)^2))
        train_auc <- calculate_auc(train_data$Presence, train_preds)
        test_auc <- calculate_auc(test_data$Presence, test_preds)
        
        # Cross-validation AUC from model
        cv_auc <- if (!is.null(final_model$cv.statistics)) {
            as.numeric(final_model$cv.statistics$discrimination.mean)
        } else {
            NA_real_
        }
        
        # Extract scaling parameters
        scaling_params <- attr(raw_data, "scaling_params")
        
        update_progress(replicate_num, total_replicates, "Model Fitting")
        
        # Return successful result (without spatial predictions)
        return(list(
            replicate = replicate_num,
            dataset_name = dataset_name,
            success = TRUE,
            model = final_model,
            optimal_trees = optimal_trees,
            variable_importance = importance_summary,
            final_variables = final_variables,
            performance = list(
                train_rmse = train_rmse,
                test_rmse = test_rmse,
                train_auc = train_auc,
                test_auc = test_auc,
                cv_auc = cv_auc,
                cv_error = min(final_model$cv.error)
            ),
            n_variables_final = length(final_variables),
            n_presences = n_presences,
            scaling_params = scaling_params
        ))
        
    }, error = function(e) {
        update_progress(replicate_num, total_replicates, "Model Fitting")
        return(list(
            replicate = replicate_num,
            dataset_name = dataset_name,
            success = FALSE,
            error = as.character(e$message)
        ))
    })
}

# Enhanced spatial prediction function with detailed error reporting
make_spatial_prediction <- function(model_result, replicate_index, total_replicates) {
    # Update progress for spatial predictions
    update_progress(replicate_index - 1, total_replicates, "Spatial Predictions")
    
    result <- list(
        replicate = model_result$replicate,
        success = FALSE,
        raster = NULL,
        error = NULL
    )
    
    tryCatch({
        model <- model_result$model
        optimal_trees <- model_result$optimal_trees
        scaling_params <- model_result$scaling_params
        final_variables <- model_result$final_variables
        
        if (!exists("BRT_All_Covars")) {
            result$error <- "BRT_All_Covars not found"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Check variable matching (coordinates should already be excluded from final_variables)
        available_predictors <- final_variables[final_variables %in% names(BRT_All_Covars)]
        
        if (length(available_predictors) == 0) {
            result$error <- paste("No matching variables. Model vars:", 
                                  paste(final_variables, collapse = ", "))
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        if (length(available_predictors) != length(final_variables)) {
            missing_vars <- setdiff(final_variables, available_predictors)
            cat("  Replicate", model_result$replicate, "- Note: Missing raster variables:", 
                paste(missing_vars, collapse = ", "), "\n")
        }
        
        # Select relevant covariates
        selected_covars <- BRT_All_Covars[[available_predictors]]
        
        # Extract raster values
        raster_values <- getValues(selected_covars)
        na_mask <- apply(is.na(raster_values), 1, any)
        valid_rows <- !na_mask
        
        if (sum(valid_rows) == 0) {
            result$error <- "No valid raster cells"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Verify scaling parameters exist and match
        if (is.null(scaling_params$center) || is.null(scaling_params$scale)) {
            result$error <- "Scaling parameters missing"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Check scaling parameter alignment
        scaling_vars_center <- names(scaling_params$center)
        scaling_vars_scale <- names(scaling_params$scale)
        
        if (!all(available_predictors %in% scaling_vars_center) || 
            !all(available_predictors %in% scaling_vars_scale)) {
            result$error <- "Scaling parameter names don't match variables"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Scale values using stored parameters
        valid_values <- raster_values[valid_rows, , drop = FALSE]
        scaled_values <- scale(valid_values,
                               center = scaling_params$center[available_predictors],
                               scale = scaling_params$scale[available_predictors])
        
        # Prepare prediction data
        pred_data <- as.data.frame(scaled_values)
        names(pred_data) <- available_predictors
        
        # Verify column alignment with model
        model_vars <- all.vars(model$Terms)[-1]  # Exclude response variable
        if (!all(model_vars %in% names(pred_data))) {
            result$error <- "Prediction data columns don't match model variables"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Make predictions
        predictions_valid <- predict(model, pred_data, 
                                     n.trees = optimal_trees, type = "response")
        
        # Create full prediction vector
        predictions_full <- rep(NA, nrow(raster_values))
        predictions_full[valid_rows] <- predictions_valid
        
        # Create prediction raster
        pred_raster <- selected_covars[[1]]
        values(pred_raster) <- predictions_full
        
        # Apply Haiti boundary mask
        haiti_sp_transformed <- spTransform(Haiti_boundary_sp, 
                                            CRSobj = crs(pred_raster))
        pred_raster_masked <- tryCatch({
            mask(crop(pred_raster, haiti_sp_transformed), haiti_sp_transformed)
        }, error = function(e) {
            pred_raster
        })
        
        result$success <- TRUE
        result$raster <- pred_raster_masked
        result$error <- NULL
        
        update_progress(replicate_index, total_replicates, "Spatial Predictions")
        return(result)
        
    }, error = function(e) {
        result$error <- as.character(e$message)
        update_progress(replicate_index, total_replicates, "Spatial Predictions")
        return(result)
    })
}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#                    EXECUTE COMPLETE MODELING PIPELINE            #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# Check dataset availability and run initial diagnostics
dataset_names <- paste0("AeaeData_PseudoAbs_", 1:n_replicates)
existing_datasets <- dataset_names[map_lgl(dataset_names, exists)]

cat("Initial Dataset Diagnostics:\n")
cat("Found", length(existing_datasets), "datasets out of", n_replicates, "expected\n")

if (length(existing_datasets) == 0) {
    stop("No pseudo-absence datasets found")
}

# Quick diagnostic on first dataset to verify coordinate removal will work
if (length(existing_datasets) > 0) {
    first_data <- get(existing_datasets[1])
    coordinate_cols <- c("Longitude", "Latitude", "x", "y", "X", "Y", "lon", "lat")
    coords_found <- names(first_data)[names(first_data) %in% coordinate_cols]
    cat("Coordinates found in datasets:", paste(coords_found, collapse = ", "), "\n")
    cat("Environmental predictors available:", sum(!names(first_data) %in% c("Presence", coordinate_cols)), "\n")
}

# STEP 1: FIT ALL MODELS WITH PROGRESS TRACKING
cat("\nStep 1: Fitting BRT models for all replicates...\n")
model_results <- map(seq_along(existing_datasets), 
                     ~fit_single_replicate(.x, existing_datasets, length(existing_datasets)))

# Analyze model fitting results
successful_models <- model_results[map_lgl(model_results, ~.x$success)]
failed_models <- model_results[!map_lgl(model_results, ~.x$success)]

n_successful <- length(successful_models)
n_failed <- length(failed_models)

cat("\nModel fitting results:\n")
cat("  Successful models:", n_successful, "/", length(existing_datasets), "\n")
cat("  Failed models:", n_failed, "/", length(existing_datasets), "\n")

if (n_failed > 0) {
    cat("\nFailure analysis:\n")
    failure_reasons <- map_chr(failed_models, ~.x$error %||% "Unknown error")
    failure_table <- table(failure_reasons)
    print(failure_table)
}

# Display variable importance summary from successful models
if (n_successful > 0) {
    cat("\nVariable importance summary (top 5 variables across successful models):\n")
    all_importance <- map_dfr(successful_models, function(result) {
        if (!is.null(result$variable_importance)) {
            result$variable_importance %>%
                mutate(replicate = result$replicate) %>%
                head(5)
        }
    })
    
    if (nrow(all_importance) > 0) {
        importance_summary <- all_importance %>%
            group_by(var) %>%
            summarise(
                mean_importance = mean(rel.inf),
                frequency = n(),
                .groups = "drop"
            ) %>%
            arrange(desc(mean_importance)) %>%
            head(10)
        
        print(importance_summary)
    }
}

# STEP 2: SPATIAL PREDICTIONS FOR SUCCESSFUL MODELS
if (n_successful > 0) {
    cat("\nStep 2: Creating spatial predictions for successful models...\n")
    
    spatial_results <- map2(successful_models, seq_along(successful_models), 
                            ~make_spatial_prediction(.x, .y, length(successful_models)))
    
    # Analyze spatial prediction results
    valid_predictions <- map(spatial_results[map_lgl(spatial_results, ~.x$success)], 
                             ~.x$raster)
    failed_spatial <- spatial_results[!map_lgl(spatial_results, ~.x$success)]
    
    n_spatial_success <- length(valid_predictions)
    
    cat("\nSpatial prediction results:\n")
    cat("  Successful spatial predictions:", n_spatial_success, "/", n_successful, "\n")
    
    # Analyze spatial failures
    if (length(failed_spatial) > 0) {
        cat("\nSpatial prediction failure analysis:\n")
        error_types <- map_chr(failed_spatial, ~.x$error %||% "Unknown error")
        error_table <- table(error_types)
        print(error_table)
    }
    
    # Update model results with spatial prediction status
    successful_models_with_spatial <- map2(successful_models, spatial_results,
                                           function(model_result, spatial_result) {
                                               model_result$predictions_raster <- spatial_result$raster
                                               model_result$has_spatial <- spatial_result$success
                                               model_result$spatial_error <- spatial_result$error
                                               return(model_result)
                                           })
    
    # STEP 3: CREATE ENSEMBLE FROM VALID PREDICTIONS
    if (n_spatial_success > 0) {
        cat("\nStep 3: Creating ensemble from valid spatial predictions...\n")
        
        # Validate spatial consistency
        first_pred <- valid_predictions[[1]]
        pred_dims <- dim(first_pred)
        pred_extent <- extent(first_pred)
        pred_crs <- crs(first_pred)
        
        cat("Ensemble spatial properties:\n")
        cat("  Dimensions:", pred_dims[1], "x", pred_dims[2], "\n")
        cat("  Extent:", as.character(pred_extent), "\n")
        cat("  Valid predictions for ensemble:", n_spatial_success, "\n")
        
        # Check dimensional consistency
        dimension_check <- map_lgl(valid_predictions, function(pred) {
            identical(dim(pred), pred_dims) && 
                identical(extent(pred), pred_extent) &&
                compareCRS(crs(pred), pred_crs)
        })
        
        if (!all(dimension_check)) {
            cat("Warning: Inconsistent prediction dimensions detected\n")
            valid_predictions <- valid_predictions[dimension_check]
            n_spatial_success <- length(valid_predictions)
            cat("Proceeding with", n_spatial_success, "dimensionally consistent predictions\n")
        }
        
        # Create ensemble statistics based on available predictions
        if (n_spatial_success == 0) {
            cat("Error: No predictions with consistent dimensions\n")
        } else if (n_spatial_success == 1) {
            cat("Note: Only 1 valid prediction - using as ensemble\n")
            ensemble_mean <- valid_predictions[[1]]
            ensemble_sd <- ensemble_mean * 0
            ensemble_median <- ensemble_mean
            ensemble_cv <- ensemble_mean * 0
            ensemble_ci_lower <- ensemble_mean
            ensemble_ci_upper <- ensemble_mean
        } else {
            cat("Creating ensemble statistics from", n_spatial_success, "predictions...\n")
            
            ensemble_stack <- tryCatch({
                stack(valid_predictions)
            }, error = function(e) {
                cat("Error creating stack, attempting with explicit naming...\n")
                pred_list <- valid_predictions
                names(pred_list) <- paste0("pred_", seq_along(pred_list))
                stack(pred_list)
            })
            
            if (!is.null(ensemble_stack)) {
                ensemble_mean <- calc(ensemble_stack, fun = mean, na.rm = TRUE)
                ensemble_sd <- calc(ensemble_stack, fun = sd, na.rm = TRUE)
                ensemble_median <- calc(ensemble_stack, fun = median, na.rm = TRUE)
                ensemble_cv <- ensemble_sd / ensemble_mean
                ensemble_ci_lower <- ensemble_mean - 1.96 * ensemble_sd
                ensemble_ci_upper <- ensemble_mean + 1.96 * ensemble_sd
            }
        }
        
        # Store all products in global environment
        if (exists("ensemble_mean") && !is.null(ensemble_mean)) {
            assign("Aeae_ensemble_mean", ensemble_mean, envir = .GlobalEnv)
            assign("Aeae_ensemble_sd", ensemble_sd, envir = .GlobalEnv)
            assign("Aeae_ensemble_median", ensemble_median, envir = .GlobalEnv)
            assign("Aeae_ensemble_cv", ensemble_cv, envir = .GlobalEnv)
            assign("Aeae_ensemble_ci_lower", ensemble_ci_lower, envir = .GlobalEnv)
            assign("Aeae_ensemble_ci_upper", ensemble_ci_upper, envir = .GlobalEnv)
            assign("Aeae_successful_results", successful_models_with_spatial, envir = .GlobalEnv)
            assign("Aeae_individual_predictions", valid_predictions, envir = .GlobalEnv)
            
            # Create comprehensive performance summary
            performance_summary <- map_dfr(successful_models_with_spatial, function(result) {
                data.frame(
                    replicate = result$replicate,
                    dataset_name = result$dataset_name,
                    test_rmse = result$performance$test_rmse %||% NA,
                    test_auc = result$performance$test_auc %||% NA,
                    cv_auc = result$performance$cv_auc %||% NA,
                    n_variables_final = result$n_variables_final %||% NA,
                    has_spatial = result$has_spatial %||% FALSE,
                    spatial_error = if(is.null(result$spatial_error)) NA else result$spatial_error,
                    stringsAsFactors = FALSE
                )
            })
            
            assign("Aeae_performance_summary", performance_summary, envir = .GlobalEnv)
            
            # Calculate and display summary statistics
            mean_values <- values(ensemble_mean)
            mean_values_clean <- mean_values[!is.na(mean_values)]
            
            cat("\n", paste(rep("=", 60), collapse = ""), "\n")
            cat("HAITI AEDES AEGYPTI ENSEMBLE MODELING COMPLETE\n")
            cat(paste(rep("=", 60), collapse = ""), "\n")
            cat("Total datasets processed:", length(existing_datasets), "\n")
            cat("Successful model fits:", n_successful, "/", length(existing_datasets), 
                " (", round(n_successful/length(existing_datasets)*100, 1), "%)\n")
            cat("Models with spatial predictions:", n_spatial_success, "/", n_successful,
                " (", round(n_spatial_success/n_successful*100, 1), "%)\n")
            cat("Mean habitat suitability:", round(mean(mean_values_clean), 4), "\n")
            cat("Suitability range:", round(min(mean_values_clean), 4), "to", 
                round(max(mean_values_clean), 4), "\n")
            
            if (nrow(performance_summary) > 0) {
                spatial_success <- performance_summary[performance_summary$has_spatial == TRUE, ]
                if (nrow(spatial_success) > 0) {
                    cat("Mean test AUC (spatial models):", 
                        round(mean(spatial_success$test_auc, na.rm = TRUE), 3), "\n")
                    cat("Mean final variables per model:", 
                        round(mean(spatial_success$n_variables_final, na.rm = TRUE), 1), "\n")
                }
            }
            
            cat("\nGlobal Environment Products Created:\n")
            cat("- Aeae_ensemble_mean: Mean habitat suitability\n")
            cat("- Aeae_ensemble_sd: Prediction uncertainty\n") 
            cat("- Aeae_ensemble_median: Median suitability\n")
            cat("- Aeae_ensemble_cv: Coefficient of variation\n")
            cat("- Aeae_ensemble_ci_lower/upper: 95% confidence intervals\n")
            cat("- Aeae_individual_predictions: List of", length(valid_predictions), "individual model rasters\n")
            cat("- Aeae_successful_results: Detailed results for all successful models\n")
            cat("- Aeae_performance_summary: Model performance metrics\n")
            cat(paste(rep("=", 60), collapse = ""), "\n")
            
        } else {
            cat("Failed to create ensemble statistics\n")
        }
        
    } else {
        cat("No models produced valid spatial predictions\n")
    }
} else {
    cat("No successful models to create ensemble\n")
}

# Clean up parallel processing
stopCluster(cl)

cat("=== ENSEMBLE MODELING COMPLETE ===\n")

```

```{r}

#| label: visualize-ensemble-static
#| code-summary: "Static visualization of Aedes aegypti habitat suitability ensemble"
#| fig-cap: "Aedes aegypti habitat suitability ensemble across Haiti"

# Load required libraries
library(rasterVis)
library(RColorBrewer)
library(viridis)
library(gridExtra)

# Convert ensemble rasters to data frames for ggplot
ensemble_mean_df <- as.data.frame(Aeae_ensemble_mean, xy = TRUE) %>%
    filter(!is.na(layer)) %>%
    rename(suitability = layer)

ensemble_cv_df <- as.data.frame(Aeae_ensemble_cv, xy = TRUE) %>%
    filter(!is.na(layer)) %>%
    rename(cv = layer)

# Main ensemble mean plot
ensemble_plot <- ggplot() +
    geom_raster(data = ensemble_mean_df, aes(x = x, y = y, fill = suitability)) +
    geom_sf(data = haiti_wo_lakes_reproj, fill = NA, color = "black", size = 0.5, alpha = 0.8) +
    geom_point(data = as.data.frame(AeaePoints_df), 
               aes(x = x, y = y), 
               color = "white", size = 2.5, alpha = 0.9) +
    geom_point(data = as.data.frame(AeaePoints_df), 
               aes(x = x, y = y), 
               color = "#702963", size = 1.8, alpha = 0.9) +
    scale_fill_viridis_c(
        name = "Habitat\nSuitability",
        option = "plasma",
        direction = 1,
        labels = scales::number_format(accuracy = 0.01)
    ) +
    theme_minimal() +
    theme(
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.background = element_rect(fill = "lightblue", color = NA),
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        legend.position = "right"
    ) +
    coord_sf(expand = FALSE) +
    labs(
        title = "Aedes aegypti Habitat Suitability - Ensemble Mean",
        subtitle = paste("Based on", length(Aeae_individual_predictions), "BRT models"),
        x = NULL, y = NULL
    )

# Coefficient of variation plot
cv_plot <- ggplot() +
    geom_raster(data = ensemble_cv_df, aes(x = x, y = y, fill = cv)) +
    geom_sf(data = haiti_wo_lakes_reproj, fill = NA, color = "black", size = 0.5, alpha = 0.8) +
    scale_fill_viridis_c(
        name = "Coefficient\nof Variation",
        option = "inferno",
        direction = 1,
        labels = scales::number_format(accuracy = 0.01),
        trans = "sqrt"
    ) +
    theme_minimal() +
    theme(
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.background = element_rect(fill = "lightblue", color = NA),
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        legend.position = "right"
    ) +
    coord_sf(expand = FALSE) +
    labs(
        title = "Model Uncertainty (Coefficient of Variation)",
        subtitle = "Higher values indicate greater disagreement between models",
        x = NULL, y = NULL
    )

# Summary statistics panel
stats_df <- data.frame(
    Metric = c("Mean Suitability", "Min Suitability", "Max Suitability", 
               "Mean CV", "Models Used", "Presence Points"),
    Value = c(
        round(mean(ensemble_mean_df$suitability, na.rm = TRUE), 3),
        round(min(ensemble_mean_df$suitability, na.rm = TRUE), 3),
        round(max(ensemble_mean_df$suitability, na.rm = TRUE), 3),
        round(mean(ensemble_cv_df$cv, na.rm = TRUE), 3),
        length(Aeae_individual_predictions),
        nrow(AeaePoints_df)
    )
)

stats_table <- tableGrob(stats_df, rows = NULL, theme = ttheme_minimal(
    core = list(fg_params = list(cex = 0.9)),
    colhead = list(fg_params = list(cex = 1.0, fontface = "bold"))
))

# Alternative approach without textGrob
combined_plot <- grid.arrange(
    ensemble_plot, 
    cv_plot,
    stats_table,
    layout_matrix = rbind(c(1, 2), c(3, 3)),
    heights = c(3, 1)
)

# Add title manually
title_plot <- ggplot() + 
    ggtitle("Haiti Aedes aegypti Habitat Suitability Ensemble Results") +
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

# Final combined plot with title
final_plot <- grid.arrange(
    title_plot,
    combined_plot,
    heights = c(0.1, 1)
)

print(final_plot)

```

```{r}

#| label: visualize-ensemble-suitability-interactive
#| code-summary: "Interactive leaflet map with habitat suitability predictions and individual models"
#| fig-cap: "Interactive map showing habitat suitability ensemble and individual BRT model predictions"

# Load required libraries
library(leaflet)
library(RColorBrewer)
library(htmlwidgets)

# Create color palette for suitability data
suitability_palette <- colorNumeric(
    palette = viridis::plasma(100),
    domain = c(0, 1),
    na.color = "transparent"
)

# Initialize the leaflet map
interactive_suitability_map <- leaflet() %>%
    addTiles() %>%
    setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
    
    # Add Haiti boundary
    addPolygons(
        data = haiti_wo_lakes_reproj,
        fillColor = "lightgray",
        fillOpacity = 0.1,
        color = "black",
        weight = 2,
        popup = "Haiti (excluding water bodies)",
        group = "Haiti Boundary"
    )

# Add ensemble suitability layers
interactive_suitability_map <- interactive_suitability_map %>%
    
    # Ensemble Mean
    addRasterImage(
        Aeae_ensemble_mean,
        colors = suitability_palette,
        opacity = 0.8,
        group = "Ensemble Mean"
    ) %>%
    
    # Ensemble Median
    addRasterImage(
        Aeae_ensemble_median,
        colors = suitability_palette,
        opacity = 0.8,
        group = "Ensemble Median"
    ) %>%
    
    # Confidence Intervals
    addRasterImage(
        Aeae_ensemble_ci_lower,
        colors = suitability_palette,
        opacity = 0.8,
        group = "95% CI Lower Bound"
    ) %>%
    
    addRasterImage(
        Aeae_ensemble_ci_upper,
        colors = suitability_palette,
        opacity = 0.8,
        group = "95% CI Upper Bound"
    )

# Add individual model predictions
cat("Adding individual model layers to suitability map...\n")
for (i in 1:length(Aeae_individual_predictions)) {
    # Update progress
    if (i %% 5 == 0) {
        cat("  Added", i, "of", length(Aeae_individual_predictions), "individual models\n")
    }
    
    interactive_suitability_map <- interactive_suitability_map %>%
        addRasterImage(
            Aeae_individual_predictions[[i]],
            colors = suitability_palette,
            opacity = 0.8,
            group = paste("Individual Model", i)
        )
}

# Add presence points (always visible)
interactive_suitability_map <- interactive_suitability_map %>%
    addCircleMarkers(
        data = AeaePoints_sf,
        radius = 6,
        fillColor = "#702963",
        color = "white",
        weight = 2,
        opacity = 1,
        fillOpacity = 0.9,
        popup = ~paste("<b>Aedes aegypti presence</b><br>",
                       "Site ID:", row.names(AeaePoints)),
        group = "Presence Points"
    )

# Create layer group names for layer control
ensemble_suitability_layers <- c(
    "Ensemble Mean",
    "Ensemble Median", 
    "95% CI Lower Bound",
    "95% CI Upper Bound"
)

individual_layers <- paste("Individual Model", 1:length(Aeae_individual_predictions))

all_suitability_layers <- c("Haiti Boundary", "Presence Points", ensemble_suitability_layers, individual_layers)

# Add layers control with organized groups
interactive_suitability_map <- interactive_suitability_map %>%
    addLayersControl(
        baseGroups = c("OpenStreetMap"),
        overlayGroups = all_suitability_layers,
        options = layersControlOptions(collapsed = TRUE),
        position = "topleft"
    ) %>%
    
    # Show only ensemble mean and presence points initially
    hideGroup(c(ensemble_suitability_layers[-1], individual_layers)) %>%
    
    # Add single legend for habitat suitability (compact format)
    addLegend(
        position = "bottomright",
        pal = suitability_palette,
        values = values(Aeae_ensemble_mean),
        title = "Suitability",
        opacity = 0.8,
        labFormat = labelFormat(digits = 2),
        className = "info legend-small"
    )

# Add custom control instructions
interactive_suitability_map <- interactive_suitability_map %>%
    addControl(
        html = '<div style="background: white; padding: 10px; border-radius: 5px; font-size: 12px; max-width: 200px;">
                    <strong>Habitat Suitability Explorer</strong><br>
                    • <strong>Layer Control:</strong> Top-left to toggle layers<br>
                    • <strong>Ensemble layers:</strong> Statistical summaries across all models<br>
                    • <strong>Individual models:</strong> Single BRT predictions (1-30)<br>
                    • <span style="color: #702963;">●</span> <strong>Purple dots:</strong> Observed presence points<br>
                </div>',
        position = "topright"
    )

# Print summary of what was created
cat("Available layers:\n")
cat("  Ensemble suitability layers:", length(ensemble_suitability_layers), "\n")
cat("  Individual model layers:", length(individual_layers), "\n")
cat("  Reference layers: Haiti boundary, presence points\n")
cat("  Total interactive layers:", length(all_suitability_layers), "\n\n")

# Display the interactive map
print(interactive_suitability_map)

```

```{r}

#| label: visualize-ensemble-uncertainty-interactive
#| code-summary: "Interactive leaflet map with model uncertainty metrics"
#| fig-cap: "Interactive map showing model uncertainty and prediction variability across Haiti"

# Load required libraries
library(leaflet)
library(RColorBrewer)
library(htmlwidgets)

# Create color palette for uncertainty data
uncertainty_palette <- colorNumeric(
    palette = viridis::inferno(100),
    domain = values(Aeae_ensemble_cv),
    na.color = "transparent"
)

# Create secondary palette for standard deviation (if different scaling needed)
sd_palette <- colorNumeric(
    palette = viridis::magma(100),
    domain = values(Aeae_ensemble_sd),
    na.color = "transparent"
)

# Initialize the leaflet map with multiple base layers
interactive_uncertainty_map <- leaflet() %>%
    addTiles(group = "OpenStreetMap") %>%
    addProviderTiles(providers$Esri.WorldTopoMap, group = "Topographic") %>%
    addProviderTiles(providers$OpenTopoMap, group = "OpenTopoMap") %>%
    setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
    
    # Add Haiti boundary
    addPolygons(
        data = haiti_wo_lakes_reproj,
        fillColor = "lightgray",
        fillOpacity = 0.1,
        color = "black",
        weight = 2,
        popup = "Haiti (excluding water bodies)",
        group = "Haiti Boundary"
    )

# Add ensemble uncertainty layers
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    
    # Ensemble Coefficient of Variation
    addRasterImage(
        Aeae_ensemble_cv,
        colors = uncertainty_palette,
        opacity = 0.8,
        group = "Coefficient of Variation (CV)"
    ) %>%
    
    # Ensemble Standard Deviation
    addRasterImage(
        Aeae_ensemble_sd,
        colors = sd_palette,
        opacity = 0.8,
        group = "Standard Deviation"
    )

# Add presence points (always visible)
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    addCircleMarkers(
        data = AeaePoints_sf,
        radius = 6,
        fillColor = "#702963",
        color = "white",
        weight = 2,
        opacity = 1,
        fillOpacity = 0.9,
        popup = ~paste("<b>Aedes aegypti presence</b><br>",
                       "Site ID:", row.names(AeaePoints),
                       "<br>Click to see local uncertainty"),
        group = "Presence Points"
    )

# Create layer group names for layer control
uncertainty_layers <- c(
    "Coefficient of Variation (CV)",
    "Standard Deviation"
)

all_uncertainty_layers <- c("Haiti Boundary", "Presence Points", uncertainty_layers)

# Add layers control with base maps
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    addLayersControl(
        baseGroups = c("OpenStreetMap", "Topographic", "OpenTopoMap"),
        overlayGroups = all_uncertainty_layers,
        options = layersControlOptions(collapsed = TRUE),
        position = "topleft"
    ) %>%
    
    # Show CV by default
    hideGroup("Standard Deviation") %>%
    
    # Add legend for coefficient of variation (primary uncertainty metric) - smaller format
    addLegend(
        position = "bottomright",
        pal = uncertainty_palette,
        values = values(Aeae_ensemble_cv),
        title = "CV",
        opacity = 0.8,
        labFormat = labelFormat(digits = 2),
        className = "info legend-small"
    ) %>%
    
    # Add secondary legend for standard deviation - smaller format
    addLegend(
        position = "bottomleft",
        pal = sd_palette,
        values = values(Aeae_ensemble_sd),
        title = "Std Dev",
        opacity = 0.8,
        labFormat = labelFormat(digits = 2),
        className = "info legend-small"
    )

# Calculate uncertainty statistics for the information panel
cv_values <- values(Aeae_ensemble_cv)
cv_clean <- cv_values[!is.na(cv_values)]
sd_values <- values(Aeae_ensemble_sd)
sd_clean <- sd_values[!is.na(sd_values)]

mean_cv <- round(mean(cv_clean), 3)
max_cv <- round(max(cv_clean), 3)
mean_sd <- round(mean(sd_clean), 3)
max_sd <- round(max(sd_clean), 3)

# Add custom control instructions (compact version)
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    addControl(
        html = paste0('<div style="background: rgba(255,255,255,0.9); padding: 6px; border-radius: 3px; font-size: 9px; max-width: 140px; font-family: Arial, sans-serif;">
                    <strong>Uncertainty</strong><br>
                    Dark = Models agree<br>
                    Bright = Models disagree<br>
                    CV: ', mean_cv, ' avg, ', max_cv, ' max<br>
                    SD: ', mean_sd, ' avg, ', max_sd, ' max
                </div>'),
        position = "topright"
    )

# Print summary of what was created
cat("Uncertainty metrics available:\n")
cat("  Coefficient of Variation - relative uncertainty measure\n")
cat("  Standard Deviation - absolute model disagreement\n")
cat("Summary statistics:\n")
cat("  Mean CV:", mean_cv, "| Max CV:", max_cv, "\n")
cat("  Mean SD:", mean_sd, "| Max SD:", max_sd, "\n")
cat("Total interactive layers:", length(all_uncertainty_layers), "\n\n")



```

```{r}
#| label: interactive-uncertainty-map
#| code-summary: "Identify acceptable CMIP6 models and inventory downloaded data"

# Display the interactive map
print(interactive_uncertainty_map)
```

```{r, echo=FALSE}
#| label: acceptable-models-inventory-windows-path
#| code-summary: "Identify acceptable CMIP6 models and inventory downloaded data"

library(tidyverse)
library(stringr)

# Define the acceptable models (moderate sensitivity, not too hot/cold)
acceptable_models <- c(
    'ACCESS-CM2',
    'ACCESS-ESM1-5',
    'AWI-CM-1-1-MR',
    'BCC-CSM2-MR',
    'BCC-ESM1',
    'CAMS-CSM1-0',
    'CAS-ESM2-0',
    'CESM2',
    'CESM2-FV2',
    'CESM2-WACCM',
    'CESM2-WACCM-FV2',
    'CMCC-CM2-SR5',
    'CNRM-CM6-1',
    'CNRM-ESM2-1',
    'FGOALS-f3-L',
    'FGOALS-g3',
    'GFDL-CM4',
    'GFDL-ESM4',
    'GISS-E2-1-G',
    'GISS-E2-1-H',
    'GISS-E2-2-G',
    'IITM-ESM',
    'KACE-1-0-G',
    'MCM-UA-1-0',
    'MIROC-ES2L',
    'MIROC6',
    'MPI-ESM-1-2-HAM',
    'MPI-ESM1-2-HR',
    'MPI-ESM1-2-LR',
    'MRI-ESM2-0',
    'NorCPM1',
    'NorESM2-LM'
)

# Inventory your downloaded Haiti CMIP6 data (Windows path)
cmip6_data_dir <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/cmip6_haiti_python"

# Check if directory exists
if (!dir.exists(cmip6_data_dir)) {
    stop("CMIP6 data directory not found: ", cmip6_data_dir)
}

# Get all downloaded files
downloaded_files <- list.files(
    cmip6_data_dir, 
    pattern = "\\.tif$",  # More permissive pattern initially
    full.names = TRUE
)

cat("Found", length(downloaded_files), "total .tif files\n")

# Parse filenames to extract model, SSP, and period information
file_inventory <- purrr::map_dfr(downloaded_files, function(filepath) {
    filename <- basename(filepath)
    
    # Try multiple potential filename patterns
    # Pattern 1: haiti_wc2.1_30s_bioc_{MODEL}_{SSP}_{PERIOD}.tif
    parts <- stringr::str_match(
        filename, 
        "haiti_wc2\\.1_30s_bioc_(.+?)_(ssp\\d+)_(\\d{4}-\\d{4})\\.tif"
    )
    
    # Pattern 2: wc2.1_30s_bioc_{MODEL}_{SSP}_{PERIOD}.tif (without "haiti_" prefix)
    if (is.na(parts[1])) {
        parts <- stringr::str_match(
            filename,
            "wc2\\.1_30s_bioc_(.+?)_(ssp\\d+)_(\\d{4}-\\d{4})\\.tif"
        )
    }
    
    # Pattern 3: More flexible pattern for model names with hyphens
    if (is.na(parts[1])) {
        parts <- stringr::str_match(
            filename,
            "(?:haiti_)?wc2\\.1_30s_bioc_([A-Za-z0-9-]+)_(ssp\\d+)_(\\d{4}-\\d{4})\\.tif"
        )
    }
    
    # Return tibble only if we successfully parsed the filename
    if (!is.na(parts[1]) && length(parts) >= 4) {
        # Extract period start and end years
        period_years <- stringr::str_split(parts[4], "-")[[1]]
        
        tibble::tibble(
            filepath = filepath,
            filename = filename,
            model = parts[2],
            ssp = parts[3],
            time_period = parts[4],
            period_start = as.integer(period_years[1]),
            period_end = as.integer(period_years[2]),
            file_exists = file.exists(filepath),
            file_size_mb = file.info(filepath)$size / 1024^2
        )
    } else {
        # Return NULL for failed matches - will be removed by map_dfr
        NULL
    }
})

# Check parsing results
cat("\nFilename parsing results:\n")
cat("  Successfully parsed:", nrow(file_inventory), "files\n")
cat("  Failed to parse:", length(downloaded_files) - nrow(file_inventory), "files\n")

# Show examples of unparsed files if any
if (nrow(file_inventory) < length(downloaded_files)) {
    unparsed_files <- setdiff(
        basename(downloaded_files), 
        file_inventory$filename
    )
    cat("\nExamples of unparsed filenames:\n")
    cat("  ", head(unparsed_files, 5), sep = "\n  ")
}

# Filter to only acceptable models
acceptable_inventory <- file_inventory %>%
    dplyr::filter(model %in% acceptable_models) %>%
    dplyr::arrange(model, ssp, period_start)

# Summary statistics
cat("\n=== CMIP6 Data Inventory ===\n")
cat("Total files found:", length(downloaded_files), "\n")
cat("Files successfully parsed:", nrow(file_inventory), "\n")
cat("Files from acceptable models:", nrow(acceptable_inventory), "\n")
cat("Unique models in parsed files:", 
    length(unique(file_inventory$model)), "\n")
cat("Unique acceptable models found:", 
    length(unique(acceptable_inventory$model)), "\n\n")

# Show which acceptable models we found
found_acceptable_models <- sort(unique(acceptable_inventory$model))
cat("Acceptable models found:\n")
cat("  ", found_acceptable_models, sep = "\n  ")
cat("\n")

# Check for missing models
missing_models <- setdiff(acceptable_models, found_acceptable_models)
if (length(missing_models) > 0) {
    cat("\nAcceptable models NOT found in data:\n")
    cat("  ", missing_models, sep = "\n  ")
    cat("\n")
}

# Show SSPs and time periods
if (nrow(acceptable_inventory) > 0) {
    cat("SSPs represented:", 
        paste(sort(unique(acceptable_inventory$ssp)), collapse = ", "), "\n")
    cat("Time periods:", 
        paste(sort(unique(acceptable_inventory$time_period)), collapse = ", "), "\n\n")
    
    # Summary by model
    model_summary <- acceptable_inventory %>%
        dplyr::group_by(model) %>%
        dplyr::summarise(
            n_files = n(),
            n_ssps = n_distinct(ssp),
            n_periods = n_distinct(time_period),
            total_size_gb = sum(file_size_mb) / 1024,
            .groups = "drop"
        ) %>%
        dplyr::arrange(desc(n_files))
    
    cat("Files per acceptable model:\n")
    print(model_summary, n = Inf)
    
    # Check completeness - should have 4 SSPs × 4 periods = 16 files per model
    complete_models <- model_summary %>%
        dplyr::filter(n_files == 16)
    
    cat("\nComplete models (16 files each):", nrow(complete_models), "of", nrow(model_summary), "\n")
    
    if (nrow(model_summary) > nrow(complete_models)) {
        incomplete_models <- model_summary %>%
            dplyr::filter(n_files < 16)
        
        cat("\nIncomplete models:\n")
        print(incomplete_models, n = Inf)
    }
}

# Store for later use
assign("acceptable_cmip6_inventory", acceptable_inventory, envir = .GlobalEnv)
assign("all_cmip6_inventory", file_inventory, envir = .GlobalEnv)

# Diagnostic: show all unique models found (even non-acceptable ones)
cat("\n=== All Models Found (for diagnostic purposes) ===\n")
all_models_found <- sort(unique(file_inventory$model))
cat("  ", all_models_found, sep = "\n  ")
cat("\nTotal unique models:", length(all_models_found), "\n")

# Additional diagnostic: check for any files with unexpected patterns
cat("\n=== File Pattern Analysis ===\n")
if (nrow(file_inventory) > 0) {
    cat("Example filenames (first 5):\n")
    cat("  ", head(file_inventory$filename, 5), sep = "\n  ")
}
```

```{r, echo=FALSE}
#| label: load-warming-levels-from-github
#| code-summary: "Load CMIP6 warming levels data from custom GitHub repository"

library(readr)
library(dplyr)
library(tidyr)

# Load warming levels data from your GitHub repository
warming_levels_url <- paste0(
    "https://raw.githubusercontent.com/IanPsheaSmith/",
    "HaitiAeaeENM/main/WarmingLevelsList/",
    "cmip6_warming_levels_Hauser2022.csv"
)

cat("Loading warming levels data from GitHub...\n")
cat("URL:", warming_levels_url, "\n\n")

warming_levels_raw <- tryCatch({
    readr::read_csv(warming_levels_url, show_col_types = FALSE)
}, error = function(e) {
    cat("Failed to download from GitHub:\n")
    cat("  Error:", e$message, "\n")
    return(NULL)
})

if (!is.null(warming_levels_raw)) {
    
    cat("✓ Successfully loaded warming levels data\n")
    cat("Dimensions:", nrow(warming_levels_raw), "rows ×", 
        ncol(warming_levels_raw), "columns\n\n")
    
    # Display structure
    cat("Column names:\n")
    cat("  ", names(warming_levels_raw), sep = "\n  ")
    cat("\n")
    
    cat("First few rows:\n")
    print(head(warming_levels_raw, 5))
    cat("\n")
    
    # Process for our needs
    warming_levels_processed <- warming_levels_raw %>%
        # Filter to only acceptable models and SSPs we have data for
        dplyr::filter(
            model %in% acceptable_models,
            exp %in% c("ssp126", "ssp245", "ssp370", "ssp585")
        ) %>%
        # Calculate period midpoint for matching with our data
        dplyr::mutate(
            period_midpoint = (start_year + end_year) / 2,
            period_range = paste0(start_year, "-", end_year)
        ) %>%
        dplyr::select(
            model, 
            ensemble,
            exp, 
            warming_level, 
            start_year, 
            end_year,
            period_midpoint,
            period_range
        )
    
    cat("=== Warming Levels Data Summary ===\n")
    cat("Total rows after filtering:", nrow(warming_levels_processed), "\n")
    cat("Acceptable models found:", 
        length(unique(warming_levels_processed$model)), "of 32\n")
    
    # Check which acceptable models we have
    found_models <- unique(warming_levels_processed$model)
    cat("\nAcceptable models with warming level data:\n")
    cat("  ", sort(found_models), sep = "\n  ")
    
    missing_models <- setdiff(acceptable_models, found_models)
    if (length(missing_models) > 0) {
        cat("\n\nAcceptable models WITHOUT warming level data:\n")
        cat("  ", missing_models, sep = "\n  ")
    }
    
    cat("\n\nSSPs available:", 
        paste(sort(unique(warming_levels_processed$exp)), collapse = ", "), "\n")
    cat("Warming levels available:", 
        paste(sort(unique(warming_levels_processed$warming_level)), 
              collapse = "°C, "), "°C\n\n")
    
    # Focus on key warming levels: 1.5, 2.0, 3.0, 4.0
    key_warming_levels <- c(1.5, 2.0, 3.0, 4.0)
    
    warming_levels_key <- warming_levels_processed %>%
        dplyr::filter(warming_level %in% key_warming_levels)
    
    cat("Focusing on key warming levels (1.5, 2.0, 3.0, 4.0°C):\n")
    cat("Total rows:", nrow(warming_levels_key), "\n\n")
    
    # Summary: when each warming level is reached
    wl_timing_summary <- warming_levels_key %>%
        dplyr::group_by(warming_level, exp) %>%
        dplyr::summarise(
            n_models = n_distinct(model),
            earliest_midpoint = min(period_midpoint),
            latest_midpoint = max(period_midpoint),
            median_midpoint = median(period_midpoint),
            .groups = "drop"
        ) %>%
        dplyr::arrange(warming_level, exp)
    
    cat("Timing of warming levels by SSP:\n")
    print(wl_timing_summary, n = Inf)
    
    # Check coverage for each acceptable model
    cat("\n\nCoverage check (models × warming levels × SSPs):\n")
    coverage_summary <- warming_levels_key %>%
        dplyr::group_by(model) %>%
        dplyr::summarise(
            n_wl_ssp_combinations = n(),
            n_warming_levels = n_distinct(warming_level),
            n_ssps = n_distinct(exp),
            warming_levels = paste(sort(unique(warming_level)), collapse = ", "),
            .groups = "drop"
        ) %>%
        dplyr::arrange(desc(n_wl_ssp_combinations))
    
    print(coverage_summary, n = Inf)
    
    # Expected: 4 warming levels × 4 SSPs = 16 combinations per model
    cat("\n")
    complete_models <- coverage_summary %>%
        dplyr::filter(n_wl_ssp_combinations == 16)
    
    cat("Models with complete coverage (16 combinations):", 
        nrow(complete_models), "of", nrow(coverage_summary), "\n")
    
    # Store in global environment
    assign("cmip6_warming_levels", warming_levels_key, envir = .GlobalEnv)
    assign("cmip6_warming_levels_all", warming_levels_processed, envir = .GlobalEnv)
    assign("cmip6_wl_timing_summary", wl_timing_summary, envir = .GlobalEnv)
    assign("cmip6_wl_coverage", coverage_summary, envir = .GlobalEnv)
    
    cat("\n✓ Warming levels data successfully processed and stored\n")
    cat("  Key warming levels (1.5-4.0°C): cmip6_warming_levels\n")
    cat("  All warming levels: cmip6_warming_levels_all\n")
    cat("  Timing summary: cmip6_wl_timing_summary\n")
    cat("  Coverage summary: cmip6_wl_coverage\n")
    
} else {
    cat("\n✗ Failed to load warming levels data\n")
    cat("Please check the GitHub URL and try again\n")
}
```

```{r, echo=FALSE}
#| label: map-files-to-warming-levels
#| code-summary: "Match downloaded CMIP6 files to appropriate warming levels"

library(dplyr)
library(tidyr)

cat("=== Mapping Downloaded Files to Warming Levels ===\n\n")

# Our downloaded files have these time periods:
# 2021-2040, 2041-2060, 2061-2080, 2081-2100

# Calculate midpoints for our downloaded periods
file_periods <- acceptable_cmip6_inventory %>%
    dplyr::mutate(
        period_midpoint = (period_start + period_end) / 2
    )

cat("Downloaded file periods:\n")
period_summary <- file_periods %>%
    dplyr::distinct(time_period, period_start, period_end, period_midpoint)
print(period_summary)
cat("\n")

# Function to find the best warming level match for each file
find_best_warming_level <- function(model, ssp, file_midpoint, wl_data, 
                                     max_year_difference = 15) {
    
    # Get all warming levels for this model/SSP combination
    model_wl <- wl_data %>%
        dplyr::filter(
            model == !!model,
            exp == !!ssp
        )
    
    if (nrow(model_wl) == 0) {
        return(tibble::tibble(
            warming_level = NA_real_,
            wl_period_midpoint = NA_real_,
            wl_start_year = NA_integer_,
            wl_end_year = NA_integer_,
            year_difference = NA_real_,
            match_quality = "no_data"
        ))
    }
    
    # Calculate distance between file midpoint and each warming level midpoint
    model_wl <- model_wl %>%
        dplyr::mutate(
            year_difference = abs(period_midpoint - !!file_midpoint)
        ) %>%
        dplyr::arrange(year_difference)
    
    # Get the closest match
    best_match <- model_wl %>%
        dplyr::slice(1) %>%
        dplyr::select(
            warming_level,
            wl_period_midpoint = period_midpoint,
            wl_start_year = start_year,
            wl_end_year = end_year,
            year_difference
        ) %>%
        dplyr::mutate(
            match_quality = dplyr::case_when(
                year_difference <= 5 ~ "excellent",
                year_difference <= 10 ~ "good",
                year_difference <= 15 ~ "fair",
                TRUE ~ "poor"
            )
        )
    
    return(best_match)
}

# Apply matching for all files
cat("Matching files to warming levels...\n")

files_with_wl <- file_periods %>%
    dplyr::rowwise() %>%
    dplyr::mutate(
        wl_match = list(find_best_warming_level(
            model, ssp, period_midpoint, cmip6_warming_levels
        ))
    ) %>%
    dplyr::ungroup() %>%
    tidyr::unnest(wl_match)

cat("✓ Matching complete\n\n")

# Summary of matches
cat("=== Matching Summary ===\n")

match_quality_summary <- files_with_wl %>%
    dplyr::group_by(match_quality) %>%
    dplyr::summarise(
        n_files = n(),
        pct_files = round(n() / nrow(files_with_wl) * 100, 1),
        .groups = "drop"
    ) %>%
    dplyr::arrange(desc(n_files))

print(match_quality_summary)
cat("\n")

# Files that couldn't be matched
unmatched_files <- files_with_wl %>%
    dplyr::filter(is.na(warming_level))

if (nrow(unmatched_files) > 0) {
    cat("Files without warming level matches:", nrow(unmatched_files), "\n")
    cat("Breakdown by model/SSP:\n")
    unmatched_summary <- unmatched_files %>%
        dplyr::group_by(model, ssp) %>%
        dplyr::summarise(
            n_files = n(),
            periods = paste(sort(unique(time_period)), collapse = ", "),
            .groups = "drop"
        )
    print(unmatched_summary, n = Inf)
    cat("\n")
}

# Successfully matched files
matched_files <- files_with_wl %>%
    dplyr::filter(!is.na(warming_level))

cat("Successfully matched files:", nrow(matched_files), "of", nrow(files_with_wl), "\n\n")

# Summary by warming level
wl_file_summary <- matched_files %>%
    dplyr::group_by(warming_level) %>%
    dplyr::summarise(
        n_files = n(),
        n_models = n_distinct(model),
        n_ssps = n_distinct(ssp),
        mean_year_diff = round(mean(year_difference, na.rm = TRUE), 1),
        median_year_diff = round(median(year_difference, na.rm = TRUE), 1),
        .groups = "drop"
    ) %>%
    dplyr::arrange(warming_level)

cat("Files available for each warming level:\n")
print(wl_file_summary, n = Inf)
cat("\n")

# Detailed breakdown by warming level and SSP
wl_ssp_breakdown <- matched_files %>%
    dplyr::group_by(warming_level, ssp) %>%
    dplyr::summarise(
        n_models = n_distinct(model),
        n_files = n(),
        models = paste(sort(unique(model)), collapse = ", "),
        .groups = "drop"
    ) %>%
    dplyr::arrange(warming_level, ssp)

cat("Breakdown by warming level and SSP:\n")
print(wl_ssp_breakdown, n = Inf)
cat("\n")

# Check if we have sufficient coverage for ensemble creation
# Aim for at least 5 models per warming level for robust ensembles
sufficient_coverage <- wl_file_summary %>%
    dplyr::filter(n_models >= 5)

cat("Warming levels with sufficient coverage (≥5 models):\n")
print(sufficient_coverage)
cat("\n")

# Store results
assign("cmip6_files_with_wl", files_with_wl, envir = .GlobalEnv)
assign("cmip6_matched_files", matched_files, envir = .GlobalEnv)
assign("cmip6_wl_file_summary", wl_file_summary, envir = .GlobalEnv)

cat("✓ File-to-warming-level mapping complete\n")
cat("  All files with matches: cmip6_files_with_wl\n")
cat("  Successfully matched files: cmip6_matched_files\n")
cat("  Summary by warming level: cmip6_wl_file_summary\n\n")

cat("=== Recommendation ===\n")
if (nrow(sufficient_coverage) >= 3) {
    cat("✓ You have sufficient data for", nrow(sufficient_coverage), 
        "warming levels\n")
    cat("  Warming levels ready for ensemble creation:\n")
    cat("    ", paste(sufficient_coverage$warming_level, "°C", sep = ""), 
        sep = "\n    ")
    cat("\nProceed to Phase 4: Create warming level ensembles\n")
} else {
    cat("⚠ Limited coverage - only", nrow(sufficient_coverage), 
        "warming levels have ≥5 models\n")
    cat("Consider:\n")
    cat("  1. Lowering minimum model threshold (e.g., 3 models)\n")
    cat("  2. Relaxing year difference tolerance\n")
    cat("  3. Downloading additional time periods\n")
}
```

```{r}
#| label: create-supplementary-table-warming-levels
#| code-summary: "Export warming level assignments to supplementary table"

library(dplyr)
library(readr)

cat("=== Creating Supplementary Table for Publication ===\n\n")

# Define output directory
output_dir <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Figures"

# Verify directory exists
if (!dir.exists(output_dir)) {
    cat("⚠ Output directory does not exist. Creating it...\n")
    dir.create(output_dir, recursive = TRUE)
}

cat("Output directory:", output_dir, "\n\n")

# ============================================================================
# COMPREHENSIVE VERSION (for reviewers/supplementary material)
# ============================================================================

supplementary_table_comprehensive <- cmip6_matched_files %>%
    dplyr::mutate(
        # Construct the WL reference period from start/end years
        WL_Reference_Period = paste0(wl_start_year, "-", wl_end_year)
    ) %>%
    dplyr::select(
        Model = model,
        SSP = ssp,
        Time_Period = time_period,
        Period_Start_Year = period_start,
        Period_End_Year = period_end,
        Period_Midpoint = period_midpoint,
        Assigned_Warming_Level_C = warming_level,
        WL_Reference_Period,
        WL_Reference_Start = wl_start_year,
        WL_Reference_End = wl_end_year,
        WL_Reference_Midpoint = wl_period_midpoint,
        Year_Difference = year_difference,
        Match_Quality = match_quality
    ) %>%
    # Add descriptive columns
    dplyr::mutate(
        SSP_Description = dplyr::case_when(
            SSP == "ssp126" ~ "Low emissions (SSP1-2.6)",
            SSP == "ssp245" ~ "Intermediate emissions (SSP2-4.5)",
            SSP == "ssp370" ~ "High emissions (SSP3-7.0)",
            SSP == "ssp585" ~ "Very high emissions (SSP5-8.5)",
            TRUE ~ SSP
        ),
        Warming_Level_Description = paste0(
            Assigned_Warming_Level_C, "°C above pre-industrial baseline"
        ),
        Match_Quality_Description = dplyr::case_when(
            Match_Quality == "excellent" ~ "Excellent (≤5 years difference)",
            Match_Quality == "good" ~ "Good (6-10 years difference)",
            Match_Quality == "fair" ~ "Fair (11-15 years difference)",
            Match_Quality == "poor" ~ "Poor (>15 years difference)",
            TRUE ~ Match_Quality
        )
    ) %>%
    # Sort by warming level, SSP, then model
    dplyr::arrange(
        Assigned_Warming_Level_C,
        SSP,
        Model,
        Period_Start_Year
    )

# Save comprehensive version
comprehensive_file <- file.path(
    output_dir, 
    "Supplementary_Table_X_CMIP6_Warming_Level_Assignments_Comprehensive.csv"
)

readr::write_csv(supplementary_table_comprehensive, comprehensive_file)

cat("✓ Comprehensive table saved:\n")
cat("  ", comprehensive_file, "\n\n")

# ============================================================================
# SIMPLIFIED VERSION (more readable for main supplement)
# ============================================================================

supplementary_table_simple <- cmip6_matched_files %>%
    dplyr::mutate(
        # Construct the WL reference period from start/end years
        WL_Reference_Period = paste0(wl_start_year, "-", wl_end_year)
    ) %>%
    dplyr::select(
        Model = model,
        `Emission Scenario` = ssp,
        `Data Time Period` = time_period,
        `Warming Level (°C)` = warming_level,
        `WL Reference Period` = WL_Reference_Period,
        `Year Difference` = year_difference,
        `Match Quality` = match_quality
    ) %>%
    dplyr::mutate(
        `Emission Scenario` = dplyr::case_when(
            `Emission Scenario` == "ssp126" ~ "SSP1-2.6",
            `Emission Scenario` == "ssp245" ~ "SSP2-4.5",
            `Emission Scenario` == "ssp370" ~ "SSP3-7.0",
            `Emission Scenario` == "ssp585" ~ "SSP5-8.5",
            TRUE ~ `Emission Scenario`
        )
    ) %>%
    dplyr::arrange(
        `Warming Level (°C)`,
        `Emission Scenario`,
        Model
    )

# Save simplified version
simple_file <- file.path(
    output_dir,
    "Supplementary_Table_X_CMIP6_Warming_Level_Assignments.csv"
)

readr::write_csv(supplementary_table_simple, simple_file)

cat("✓ Simplified table saved:\n")
cat("  ", simple_file, "\n\n")

# ============================================================================
# SUMMARY STATISTICS TABLE (for manuscript text reference)
# ============================================================================

summary_stats_table <- cmip6_matched_files %>%
    dplyr::group_by(
        Warming_Level = warming_level
    ) %>%
    dplyr::summarise(
        N_Model_Scenario_Combinations = n(),
        N_Unique_Models = n_distinct(model),
        N_Scenarios = n_distinct(ssp),
        Models_Included = paste(sort(unique(model)), collapse = "; "),
        Scenarios_Included = paste(sort(unique(ssp)), collapse = ", "),
        Mean_Year_Difference = round(mean(year_difference), 1),
        Median_Year_Difference = round(median(year_difference), 1),
        Match_Quality_Range = paste(
            sort(unique(match_quality)), 
            collapse = ", "
        ),
        .groups = "drop"
    ) %>%
    dplyr::arrange(Warming_Level)

# Save summary stats
summary_file <- file.path(
    output_dir,
    "Supplementary_Table_Summary_Stats_by_Warming_Level.csv"
)

readr::write_csv(summary_stats_table, summary_file)

cat("✓ Summary statistics table saved:\n")
cat("  ", summary_file, "\n\n")

# ============================================================================
# MODEL INVENTORY TABLE (which models are in each warming level)
# ============================================================================

model_inventory <- cmip6_matched_files %>%
    dplyr::group_by(Model = model) %>%
    dplyr::summarise(
        N_Warming_Levels = n_distinct(warming_level),
        Warming_Levels_Included = paste(
            sort(unique(warming_level)), 
            collapse = ", "
        ),
        N_Scenario_Period_Combinations = n(),
        SSPs_Included = paste(
            sort(unique(ssp)), 
            collapse = ", "
        ),
        Time_Periods_Included = paste(
            sort(unique(time_period)), 
            collapse = "; "
        ),
        .groups = "drop"
    ) %>%
    dplyr::arrange(Model)

# Save model inventory
inventory_file <- file.path(
    output_dir,
    "Supplementary_Table_Model_Inventory.csv"
)

readr::write_csv(model_inventory, inventory_file)

cat("✓ Model inventory table saved:\n")
cat("  ", inventory_file, "\n\n")

# ============================================================================
# DISPLAY SUMMARY
# ============================================================================

cat("=== Tables Created ===\n\n")

cat("1. COMPREHENSIVE TABLE (for detailed review):\n")
cat("   Rows:", nrow(supplementary_table_comprehensive), "\n")
cat("   Columns:", ncol(supplementary_table_comprehensive), "\n")
cat("   File:", basename(comprehensive_file), "\n\n")

cat("2. SIMPLIFIED TABLE (recommended for main supplement):\n")
cat("   Rows:", nrow(supplementary_table_simple), "\n")
cat("   Columns:", ncol(supplementary_table_simple), "\n")
cat("   File:", basename(simple_file), "\n\n")

cat("3. SUMMARY STATISTICS (for manuscript reference):\n")
cat("   Warming levels:", nrow(summary_stats_table), "\n")
cat("   File:", basename(summary_file), "\n\n")

cat("4. MODEL INVENTORY (for methods section):\n")
cat("   Models:", nrow(model_inventory), "\n")
cat("   File:", basename(inventory_file), "\n\n")

# Display preview of simplified table
cat("=== Preview of Simplified Table (first 15 rows) ===\n\n")
print(head(supplementary_table_simple, 15))

cat("\n=== Preview of Summary Statistics ===\n\n")
print(summary_stats_table, n = Inf)

cat("\n=== Preview of Model Inventory ===\n\n")
print(model_inventory, n = Inf)

cat("\n✓ All supplementary tables successfully created!\n")
cat("Location:", output_dir, "\n")
```

```{r, echo=FALSE}
#| label: create-warming-level-ensembles
#| code-summary: "Generate ensemble climate rasters for each warming level"

library(raster)
library(dplyr)
library(purrr)

cat("=== Phase 4: Creating Warming Level Ensembles ===\n\n")

# Function to create ensemble for a specific warming level
create_wl_ensemble <- function(target_wl, matched_files_data, 
                                max_year_diff = 15,
                                match_quality_threshold = "fair") {
    
    cat("Processing warming level:", target_wl, "°C\n")
    
    # Quality thresholds mapping
    quality_levels <- c("excellent", "good", "fair", "poor")
    quality_cutoff <- which(quality_levels == match_quality_threshold)
    acceptable_quality <- quality_levels[1:quality_cutoff]
    
    # Filter files for this warming level with quality control
    wl_files <- matched_files_data %>%
        dplyr::filter(
            warming_level == target_wl,
            year_difference <= max_year_diff,
            match_quality %in% acceptable_quality
        )
    
    if (nrow(wl_files) == 0) {
        cat("  ✗ No files found for", target_wl, "°C\n\n")
        return(NULL)
    }
    
    cat("  Models available:", n_distinct(wl_files$model), "\n")
    cat("  Total files:", nrow(wl_files), "\n")
    cat("  SSPs represented:", paste(sort(unique(wl_files$ssp)), collapse = ", "), "\n")
    cat("  Match quality:", paste(sort(unique(wl_files$match_quality)), collapse = ", "), "\n")
    cat("  Mean year difference:", round(mean(wl_files$year_difference), 1), "years\n")
    
    # Load all rasters for this warming level
    cat("  Loading", nrow(wl_files), "raster files...\n")
    
    raster_list <- list()
    reference_raster <- NULL
    
    for (i in seq_len(nrow(wl_files))) {
        file_info <- wl_files[i, ]
        
        tryCatch({
            # Check if file exists and is valid
            if (!file.exists(file_info$filepath)) {
                cat("    ✗ File not found:", basename(file_info$filepath), "\n")
                next
            }
            
            # Load raster brick (19 bioclim variables)
            r <- raster::brick(file_info$filepath)
            
            # Check if raster loaded properly
            if (is.null(r) || nlayers(r) == 0) {
                cat("    ✗ Invalid raster:", basename(file_info$filepath), "\n")
                next
            }
            
            # Set reference raster from first successful load
            if (is.null(reference_raster)) {
                reference_raster <- r[[1]]  # Use first layer as template
                cat("    ✓ Set reference extent from:", basename(file_info$filepath), "\n")
            }
            
            # Resample to match reference if needed
            if (!compareRaster(r, reference_raster, extent = TRUE, rowcol = TRUE, crs = TRUE, stopiffalse = FALSE)) {
                cat("    → Resampling to match reference:", basename(file_info$filepath), "\n")
                r <- raster::resample(r, reference_raster, method = "bilinear")
            }
            
            # Track metadata
            attr(r, "model") <- file_info$model
            attr(r, "ssp") <- file_info$ssp
            attr(r, "period") <- file_info$time_period
            
            raster_list[[length(raster_list) + 1]] <- r
            
        }, error = function(e) {
            cat("    ✗ Failed to load:", basename(file_info$filepath), "-", e$message, "\n")
        })
    }
    
    if (length(raster_list) == 0) {
        cat("  ✗ Failed to load any valid rasters\n\n")
        return(NULL)
    }
    
    cat("  Successfully loaded:", length(raster_list), "rasters\n")
    
    # Create ensemble statistics for each bioclim variable
    cat("  Creating ensemble statistics across 19 bioclim variables...\n")
    
    # Get variable names from first raster
    var_names <- names(raster_list[[1]])
    n_vars <- length(var_names)
    
    # Initialize output lists
    ensemble_means <- list()
    ensemble_sds <- list()
    ensemble_medians <- list()
    ensemble_cvs <- list()
    
    # Process each bioclim variable separately
    for (v in seq_len(n_vars)) {
        var_name <- var_names[v]
        
        # Extract this variable from all models
        var_layers <- purrr::map(raster_list, ~ .x[[v]])
        
        # Stack layers for this variable
        var_stack <- raster::stack(var_layers)
        
        # Calculate statistics
        var_mean <- calc(var_stack, fun = mean, na.rm = TRUE)
        var_sd <- calc(var_stack, fun = sd, na.rm = TRUE)
        var_median <- calc(var_stack, fun = median, na.rm = TRUE)
        
        # Calculate coefficient of variation (handle division by zero)
        var_cv <- overlay(var_sd, var_mean, fun = function(sd, mean) {
            ifelse(abs(mean) > 1e-10, sd / abs(mean), NA)
        })
        
        # Name the layers
        names(var_mean) <- var_name
        names(var_sd) <- paste0(var_name, "_sd")
        names(var_median) <- paste0(var_name, "_median")
        names(var_cv) <- paste0(var_name, "_cv")
        
        # Store in lists
        ensemble_means[[v]] <- var_mean
        ensemble_sds[[v]] <- var_sd
        ensemble_medians[[v]] <- var_median
        ensemble_cvs[[v]] <- var_cv
    }
    
    # Stack all variables into final raster stacks
    ensemble_mean <- raster::stack(ensemble_means)
    ensemble_sd <- raster::stack(ensemble_sds)
    ensemble_median <- raster::stack(ensemble_medians)
    ensemble_cv <- raster::stack(ensemble_cvs)
    
    cat("  ✓ Ensemble creation complete\n\n")
    
    # Return comprehensive result
    return(list(
        warming_level = target_wl,
        mean = ensemble_mean,
        sd = ensemble_sd,
        median = ensemble_median,
        cv = ensemble_cv,
        n_models = n_distinct(wl_files$model),
        n_files = length(raster_list),
        models_used = sort(unique(sapply(raster_list, function(x) attr(x, "model")))),
        ssps_used = sort(unique(sapply(raster_list, function(x) attr(x, "ssp")))),
        file_metadata = wl_files %>%
            dplyr::select(model, ssp, time_period, warming_level, 
                         year_difference, match_quality)
    ))
}

# Generate ensembles for all 4 warming levels
warming_levels_to_process <- c(1.5, 2.0, 3.0, 4.0)

cat("Creating ensembles for warming levels:", 
    paste(warming_levels_to_process, "°C", collapse = ", "), "\n\n")
cat(paste(rep("=", 60), collapse = ""), "\n\n")

wl_ensembles <- purrr::map(
    warming_levels_to_process,
    ~ create_wl_ensemble(
        target_wl = .x,
        matched_files_data = cmip6_matched_files,
        max_year_diff = 15,
        match_quality_threshold = "fair"
    )
)

# Name the list elements
names(wl_ensembles) <- paste0("wl_", warming_levels_to_process, "C")

# Remove any NULL results
wl_ensembles <- purrr::compact(wl_ensembles)

# Store results
assign("cmip6_wl_ensembles", wl_ensembles, envir = .GlobalEnv)

cat("✓ Warming level ensemble creation complete\n")
cat("  Available ensembles:", length(wl_ensembles), "warming levels\n")
cat("  Stored as: cmip6_wl_ensembles\n")

# Summary
if (length(wl_ensembles) > 0) {
    cat("\nEnsemble Summary:\n")
    for (i in seq_along(wl_ensembles)) {
        ens <- wl_ensembles[[i]]
        cat("  ", ens$warming_level, "°C:", ens$n_files, "files from", 
            ens$n_models, "models\n")
    }
}
```

```{r}
#| label: enforce-100pct-coverage-and-predict
#| code-summary: "Require 100% variable coverage and run predictions with corrected names"

library(raster)
library(dplyr)
library(purrr)
library(sf)

# 1) Rename ensemble covariates to exactly match BRT expectations (including WarmQurt/WetQurt)
create_complete_brt_mapping <- function() {
  c(
    "1" = "AnnMeanTemp",
    "2" = "MeanDiurnRange",
    "3" = "Isothermality",
    "4" = "TempSeasonality",
    "5" = "MaxTempWarmMnth",
    "6" = "MinTempColdMnth",
    "7" = "TempAnnRnge",
    "8" = "MTempWetQurt",
    "9" = "MTempDryQurt",
    "10" = "MTempWarmQurt",  # WarmQurt (exactly as in BRTs)
    "11" = "MTempCldQurt",
    "12" = "AnnPrcp",
    "13" = "PrcpWetMnth",
    "14" = "PrcpDryMnth",
    "15" = "PrcpSeasonality",
    "16" = "PrcpWetQurt",    # WetQurt (exactly as in BRTs)
    "17" = "PrcpDryQrt",
    "18" = "PrcpWrmQrt",
    "19" = "PrcpCldQrt"
  )
}

rename_to_brt_expectations <- function(wl_raster) {
  current_names <- names(wl_raster)
  brt_map <- create_complete_brt_mapping()
  bio_numbers <- gsub(".*_([0-9]+)$", "\\1", current_names)
  new_names <- brt_map[bio_numbers]
  new_names[is.na(new_names)] <- current_names[is.na(new_names)]
  names(wl_raster) <- new_names
  wl_raster
}

# 2) Recompute model coverage correctly (avoid the earlier percent bug and name collisions)
compute_coverage <- function(brt_models, wl_vars) {
  purrr::map_dfr(seq_along(brt_models), function(i) {
    model_vars <- brt_models[[i]]$final_variables
    matching <- intersect(model_vars, wl_vars)
    missing_names <- setdiff(model_vars, wl_vars)
    tibble::tibble(
      model_id = i,
      total_vars = length(model_vars),
      matching_vars = length(matching),
      missing_vars_n = length(missing_names),
      pct_available = round((length(matching) / length(model_vars)) * 100, 1),
      missing_var_list = if (length(missing_names) == 0) "" else paste(missing_names, collapse = ", ")
    )
  })
}

# 3) Prediction function requiring 100% coverage, no robust scaling tweaks
predict_warming_level_100pct <- function(wl_ensemble, brt_models, haiti_boundary) {
  wl <- wl_ensemble$warming_level
  cat("Generating predictions for", wl, "°C (100% coverage required)...\n")

  wl_covars <- rename_to_brt_expectations(wl_ensemble$mean)
  wl_vars <- names(wl_covars)

  coverage <- compute_coverage(brt_models, wl_vars)
  # Require 100% coverage
  eligible_ids <- coverage %>% filter(pct_available == 100) %>% pull(model_id)

  cat("  Eligible BRT models with 100% coverage:", length(eligible_ids), "of", nrow(coverage), "\n")
  if (length(eligible_ids) == 0) {
    cat("  ✗ No models meet 100% coverage. Aborting.\n\n")
    return(NULL)
  }

  predictions <- purrr::map(eligible_ids, function(i) {
    mr <- brt_models[[i]]
    final_vars <- mr$final_variables
    # Preserve model variable order
    use_vars <- final_vars[final_vars %in% wl_vars]

    # Select covariates and values
    covars <- wl_covars[[use_vars]]
    vals <- getValues(covars)
    na_mask <- apply(is.na(vals), 1, any)
    valid_rows <- !na_mask
    if (sum(valid_rows) == 0) return(NULL)

    valid_vals <- vals[valid_rows, , drop = FALSE]

    # Scale with original parameters (no modifications)
    centers <- mr$scaling_params$center[use_vars]
    scales  <- mr$scaling_params$scale[use_vars]
    if (any(is.na(centers)) || any(is.na(scales))) return(NULL)

    scaled <- scale(valid_vals, center = centers, scale = scales)
    if (any(!is.finite(scaled))) return(NULL)

    pred_df <- as.data.frame(scaled)
    names(pred_df) <- use_vars

    # Predict
    preds_valid <- tryCatch(
      predict(mr$model, pred_df, n.trees = mr$optimal_trees, type = "response"),
      error = function(e) NULL
    )
    if (is.null(preds_valid) || any(!is.finite(preds_valid))) return(NULL)

    # Rebuild raster
    preds_full <- rep(NA_real_, nrow(vals))
    preds_full[valid_rows] <- preds_valid
    r <- covars[[1]]
    values(r) <- preds_full

    # Mask to Haiti
    haiti_sp <- as(st_union(haiti_boundary), "Spatial")
    if (!identical(crs(r), crs(haiti_sp))) {
      haiti_sp <- spTransform(haiti_sp, CRSobj = crs(r))
    }
    mask(crop(r, haiti_sp), haiti_sp)
  })

  predictions <- purrr::compact(predictions)
  cat("  Successfully predicted:", length(predictions), "BRT models\n")
  if (length(predictions) == 0) {
    cat("  ✗ No successful predictions after filtering.\n\n")
    return(NULL)
  }

  # Ensemble
  stack_pred <- raster::stack(predictions)
  ens_mean   <- calc(stack_pred, mean, na.rm = TRUE)
  ens_sd     <- calc(stack_pred, sd, na.rm = TRUE)
  ens_median <- calc(stack_pred, median, na.rm = TRUE)
  ens_cv     <- overlay(ens_sd, ens_mean, fun = function(sd, mean) ifelse(abs(mean) > 1e-10, sd / abs(mean), NA))

  names(ens_mean)   <- paste0("suitability_wl", wl)
  names(ens_sd)     <- paste0("suitability_sd_wl", wl)
  names(ens_median) <- paste0("suitability_median_wl", wl)
  names(ens_cv)     <- paste0("suitability_cv_wl", wl)

  cat("Prediction ensemble complete (", length(predictions), "models )\n\n")
  list(
    warming_level = wl,
    mean = ens_mean,
    sd = ens_sd,
    median = ens_median,
    cv = ens_cv,
    n_brt_predictions = length(predictions),
    individual_predictions = predictions
  )
}

# 4) Run for all warming levels with 100% coverage
cat("Running predictions for all warming levels (100% coverage)...\n")
wl_predictions_100 <- purrr::map(wl_ensembles, ~ predict_warming_level_100pct(.x, Aeae_successful_results, Haiti_adm3))
wl_predictions_100 <- purrr::compact(wl_predictions_100)

assign("cmip6_wl_predictions_100pct", wl_predictions_100, envir = .GlobalEnv)

# Store per-warming-level rasters
purrr::walk(wl_predictions_100, function(pred) {
  wl_name <- paste0("Aeae_WL", gsub("\\.", "_", pred$warming_level))
  assign(paste0(wl_name, "_suitability_mean"),   pred$mean,   envir = .GlobalEnv)
  assign(paste0(wl_name, "_suitability_sd"),     pred$sd,     envir = .GlobalEnv)
  assign(paste0(wl_name, "_suitability_median"), pred$median, envir = .GlobalEnv)
  assign(paste0(wl_name, "_suitability_cv"),     pred$cv,     envir = .GlobalEnv)
  cat("✓", wl_name, "- 100% coverage predictions from", pred$n_brt_predictions, "BRT models\n")
})

# Summary
if (length(wl_predictions_100) > 0) {
  summary_100 <- purrr::map_dfr(wl_predictions_100, function(pred) {
    mv <- values(pred$mean); mv <- mv[!is.na(mv)]
    tibble::tibble(
      warming_level = pred$warming_level,
      n_brt_models = pred$n_brt_predictions,
      mean_suitability = round(mean(mv), 4),
      sd_suitability   = round(sd(mv), 4),
      min_suitability  = round(min(mv), 4),
      max_suitability  = round(max(mv), 4)
    )
  })
  assign("cmip6_wl_predictions_100pct_summary", summary_100, envir = .GlobalEnv)
  print(summary_100, n = Inf)
} else {
  cat("No warming level predictions produced under 100% coverage.\n")
}
```

```{r, results='asis'}

#| label: enhanced-interactive-warming-level-maps
#| code-summary: "Create enhanced interactive leaflet maps with separate uncertainty maps and improved styling"
#| output: asis

library(leaflet)
library(raster)
library(dplyr)
library(purrr)
library(htmltools)
library(sf)
library(viridis)

cat("=== Creating Enhanced Interactive Warming Level Maps ===\n\n")

# Function to create plasma color palette for suitability predictions
create_suitability_palette <- function() {
    colorNumeric(
        palette = viridis::plasma(256),
        domain = c(0, 1),
        na.color = "transparent"
    )
}

# Function to create inferno color palette for uncertainty measures
create_uncertainty_palette <- function(values) {
    colorNumeric(
        palette = viridis::inferno(256),
        domain = range(values, na.rm = TRUE),
        na.color = "transparent"
    )
}

# Function to create a suitability prediction map for one warming level
create_suitability_map <- function(wl_prediction, wl_ensemble) {
    
    wl <- wl_prediction$warming_level
    wl_name <- paste0("WL", gsub("\\.", "_", wl))
    
    cat("Creating suitability map for", wl, "°C warming level...\n")
    
    # Get individual predictions and ensemble
    individual_preds <- wl_prediction$individual_predictions
    ensemble_mean <- wl_prediction$mean
    ensemble_median <- wl_prediction$median
    
    # Create color palette
    pal <- create_suitability_palette()
    
    # Initialize leaflet map with custom options
    map <- leaflet(options = leafletOptions(
        zoomControl = FALSE  # We'll add it to the right
    )) %>%
        addProviderTiles("OpenStreetMap", group = "OpenStreetMap") %>%
        addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
        addProviderTiles("OpenTopoMap", group = "Topographic") %>%
        setView(lng = -72.3, lat = 19.0, zoom = 8) %>%
        # Add zoom control to the right
        addControl(
            html = '<div class="leaflet-control-zoom leaflet-bar leaflet-control">
                      <a class="leaflet-control-zoom-in" href="#" title="Zoom in" role="button" aria-label="Zoom in">+</a>
                      <a class="leaflet-control-zoom-out" href="#" title="Zoom out" role="button" aria-label="Zoom out">−</a>
                    </div>',
            position = "topright"
        )
    
    # Add individual BRT predictions
    for (i in seq_along(individual_preds)) {
        pred_raster <- individual_preds[[i]]
        layer_name <- paste0("BRT Model ", i)
        
        map <- map %>%
            addRasterImage(
                pred_raster,
                colors = pal,
                opacity = 0.8,
                group = layer_name
            )
    }
    
    # Add ensemble layers
    map <- map %>%
        addRasterImage(
            ensemble_mean,
            colors = pal,
            opacity = 0.8,
            group = "Ensemble Mean"
        ) %>%
        addRasterImage(
            ensemble_median,
            colors = pal,
            opacity = 0.8,
            group = "Ensemble Median"
        )
    
    # Create layer control groups
    individual_groups <- paste0("BRT Model ", 1:length(individual_preds))
    ensemble_groups <- c("Ensemble Mean", "Ensemble Median")
    
    # Add layer controls on the left
    map <- map %>%
        addLayersControl(
            baseGroups = c("OpenStreetMap", "Satellite", "Topographic"),
            overlayGroups = c(ensemble_groups, individual_groups),
            options = layersControlOptions(collapsed = FALSE),
            position = "topleft"
        ) %>%
        # Show only ensemble mean by default
        hideGroup(individual_groups) %>%
        hideGroup("Ensemble Median")
    
    # Add legend for suitability
    map <- map %>%
        addLegend(
            pal = pal,
            values = c(0, 1),
            title = HTML(paste0("<i>Aedes aegypti</i><br>Habitat Suitability<br>(", wl, "°C warming)")),
            position = "bottomright",
            opacity = 1
        )
    
    # Add title
    title_html <- tags$div(
        style = "position: fixed; top: 10px; left: 50%; transform: translateX(-50%); 
                 z-index: 1000; background: rgba(255,255,255,0.9); padding: 10px; 
                 border-radius: 5px; font-weight: bold; text-align: center;
                 box-shadow: 0 2px 4px rgba(0,0,0,0.3);",
        HTML(paste0("<i>Aedes aegypti</i> Habitat Suitability Predictions<br>", wl, "°C Global Warming"))
    )
    
    map <- map %>%
        addControl(title_html, position = "topleft")
    
    return(map)
}

# Function to create an uncertainty map for one warming level
create_uncertainty_map <- function(wl_prediction, wl_ensemble) {
    
    wl <- wl_prediction$warming_level
    wl_name <- paste0("WL", gsub("\\.", "_", wl))
    
    cat("Creating uncertainty map for", wl, "°C warming level...\n")
    
    # Get uncertainty measures
    ensemble_sd <- wl_prediction$sd
    ensemble_cv <- wl_prediction$cv
    
    # Create color palettes
    sd_pal <- create_uncertainty_palette(values(ensemble_sd))
    cv_pal <- create_uncertainty_palette(values(ensemble_cv))
    
    # Initialize leaflet map with custom options
    map <- leaflet(options = leafletOptions(
        zoomControl = FALSE  # We'll add it to the right
    )) %>%
        addProviderTiles("OpenStreetMap", group = "OpenStreetMap") %>%
        addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
        addProviderTiles("OpenTopoMap", group = "Topographic") %>%
        setView(lng = -72.3, lat = 19.0, zoom = 8) %>%
        # Add zoom control to the right
        addControl(
            html = '<div class="leaflet-control-zoom leaflet-bar leaflet-control">
                      <a class="leaflet-control-zoom-in" href="#" title="Zoom in" role="button" aria-label="Zoom in">+</a>
                      <a class="leaflet-control-zoom-out" href="#" title="Zoom out" role="button" aria-label="Zoom out">−</a>
                    </div>',
            position = "topright"
        )
    
    # Add uncertainty layers
    map <- map %>%
        addRasterImage(
            ensemble_sd,
            colors = sd_pal,
            opacity = 0.8,
            group = "Standard Deviation"
        ) %>%
        addRasterImage(
            ensemble_cv,
            colors = cv_pal,
            opacity = 0.8,
            group = "Coefficient of Variation"
        )
    
    # Add layer controls on the left
    map <- map %>%
        addLayersControl(
            baseGroups = c("OpenStreetMap", "Satellite", "Topographic"),
            overlayGroups = c("Standard Deviation", "Coefficient of Variation"),
            options = layersControlOptions(collapsed = FALSE),
            position = "topleft"
        ) %>%
        # Show standard deviation by default
        hideGroup("Coefficient of Variation")
    
    # Add legends (conditional based on which layer is active)
    map <- map %>%
        addLegend(
            pal = sd_pal,
            values = values(ensemble_sd),
            title = HTML("Prediction<br>Standard Deviation"),
            position = "bottomright",
            opacity = 1,
            group = "Standard Deviation"
        ) %>%
        addLegend(
            pal = cv_pal,
            values = values(ensemble_cv),
            title = HTML("Prediction<br>Coefficient of Variation"),
            position = "bottomright",
            opacity = 1,
            group = "Coefficient of Variation"
        )
    
    # Add title
    title_html <- tags$div(
        style = "position: fixed; top: 10px; left: 50%; transform: translateX(-50%); 
                 z-index: 1000; background: rgba(255,255,255,0.9); padding: 10px; 
                 border-radius: 5px; font-weight: bold; text-align: center;
                 box-shadow: 0 2px 4px rgba(0,0,0,0.3);",
        HTML(paste0("<i>Aedes aegypti</i> Prediction Uncertainty<br>", wl, "°C Global Warming"))
    )
    
    map <- map %>%
        addControl(title_html, position = "topleft")
    
    return(map)
}

# Create suitability maps for all warming levels
cat("Creating suitability prediction maps...\n")
suitability_maps <- purrr::map2(
    wl_predictions_100,
    wl_ensembles[1:length(wl_predictions_100)],
    ~ create_suitability_map(.x, .y)
)

# Create uncertainty maps for all warming levels
cat("Creating uncertainty maps...\n")
uncertainty_maps <- purrr::map2(
    wl_predictions_100,
    wl_ensembles[1:length(wl_predictions_100)],
    ~ create_uncertainty_map(.x, .y)
)

# Name the maps
wl_levels <- sapply(wl_predictions_100, function(x) x$warming_level)
suitability_names <- paste0("suitability_WL", gsub("\\.", "_", wl_levels), "C")
uncertainty_names <- paste0("uncertainty_WL", gsub("\\.", "_", wl_levels), "C")

names(suitability_maps) <- suitability_names
names(uncertainty_maps) <- uncertainty_names

# Store in global environment
assign("suitability_interactive_maps", suitability_maps, envir = .GlobalEnv)
assign("uncertainty_interactive_maps", uncertainty_maps, envir = .GlobalEnv)

cat("\n", paste(rep("=", 60), collapse = ""), "\n")
cat("ENHANCED INTERACTIVE MAPS CREATED\n")
cat(paste(rep("=", 60), collapse = ""), "\n\n")

cat("Available suitability prediction maps (plasma colors):\n")
for (i in seq_along(suitability_maps)) {
    wl_val <- wl_predictions_100[[i]]$warming_level
    n_models <- wl_predictions_100[[i]]$n_brt_predictions
    map_name <- names(suitability_maps)[i]
    cat("  ", map_name, "- ", wl_val, "°C warming (", n_models, " BRT models + ensembles)\n")
}

cat("\nAvailable uncertainty maps (inferno colors):\n")
for (i in seq_along(uncertainty_maps)) {
    wl_val <- wl_predictions_100[[i]]$warming_level
    map_name <- names(uncertainty_maps)[i]
    cat("  ", map_name, "- ", wl_val, "°C warming (SD + CV)\n")
}

cat("\nTo display maps, use:\n")
cat("  # Suitability predictions:\n")
cat("  suitability_interactive_maps$suitability_WL1_5C\n")
cat("  suitability_interactive_maps$suitability_WL2C\n")
cat("  suitability_interactive_maps$suitability_WL3C\n")
cat("  suitability_interactive_maps$suitability_WL4C\n")
cat("\n  # Uncertainty measures:\n")
cat("  uncertainty_interactive_maps$uncertainty_WL1_5C\n")
cat("  uncertainty_interactive_maps$uncertainty_WL2C\n")
cat("  uncertainty_interactive_maps$uncertainty_WL3C\n")
cat("  uncertainty_interactive_maps$uncertainty_WL4C\n")

# Display examples
cat("\nDisplaying 1.5°C warming level suitability map:\n")
print(suitability_interactive_maps[[1]])
print(suitability_interactive_maps[[2]])
print(suitability_interactive_maps[[3]])
print(suitability_interactive_maps[[4]])

cat("\nDisplaying 1.5°C warming level uncertainty map:\n")
print(uncertainty_interactive_maps[[1]])
```

```{r, results='asis'}

#| label: load-GPW2020-data
#| code-summary: "Load in and map gridded population of the world data for Haiti"

  # Use geodata::population() to download the data
    GPW2020 <- geodata::population(2020, 0.5, path=tempdir())

  # Crop & Mask the raster to Haiti  
    GPW2020_cropped <- mask(crop(raster(GPW2020), haiti_wo_lakes_sp), haiti_wo_lakes_sp)

  # Plot to verify the succesful download   
    plot(GPW2020_cropped)
```

```{r, results='asis'}

#| label: load-IDP-data
#| code-summary: "Load in & map IDP data"

# Load required libraries
library(readxl)
library(dplyr)
library(sf)
library(ggplot2)
library(viridis)
library(classInt)

# Load the IDP data
idp_data <- read_excel("C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/data/IDP_Data_Cleaned.xlsx")

# Clean column names to remove any invisible characters
names(idp_data) <- trimws(names(idp_data))
names(idp_data) <- iconv(names(idp_data), to = "UTF-8")

# Alternative approach: Use column positions instead of names
idp_cleaned <- idp_data %>%
  # Select columns by position and rename them
  dplyr::select(
    key_informants = 1,           # "Nombre d'informateurs clés interviewés dans la localité"
    evaluation_date = 2,          # "Date de l'évaluation"
    collection_method = 3,        # "Méthode de collecte"
    zone_type = 4,               # "Zone située dans la ZMPP (capital) ou en provinces"
    idp_present = 5,             # "Présence des PDI dans cette communauté ?"
    idp_location_type = 6,       # "Si oui, est-ce en sites ou en familles d'accueil/hors sites ?"
    returnee_present = 7,        # "Présence des retournés anciennes PDI dans cette communauté ?"
    dept_pcode = 8,              # "P-code département"
    dept_name = 9,               # "Nom du département"
    commune_pcode = 10,          # "P-code commune"
    commune_name = 11,           # "Nom commune"
    section_pcode = 12,          # "P-code section communale"
    section_name = 13,           # "Nom section communale"
    quartier_pcode = 14,         # "P-code quartier (uniquement dans la ZMPP)"
    quartier_name = 15,          # "Nom quartier  (uniquement dans la ZMPP)"
    site_pcode = 16,             # "P-code site"
    site_name = 17,              # "Nom du site"
    site_status = 18,            # "Statut de site"
    site_creation_date = 19,     # "Date de création de site"
    site_type = 20,              # "Type de site"
    school_continues = 21,       # School continues question
    sufficient_space = 22,       # Sufficient space question
    site_accessible = 23,        # Site accessible
    management_committee = 24,   # Management committee
    committee_composition = 25,  # Committee composition
    committee_lives_onsite = 26, # Committee lives on site
    closure_threat = 27,         # Closure threat
    idp_households = 28,         # "Ménages déplacés internes"
    idp_total = 29,              # "Personnes déplacées internes (PDI)"
    # Age/gender breakdown for IDPs
    idp_baby_male = 30,          # "B4.a. 1 Bébé (moins d'1 an) - Garçon (PDI)"
    idp_baby_female = 31,        # "B4.a. 2 Bébé (moins d'1 an) - Fille  (PDI)"
    idp_child1_5_male = 32,      # "B4.b. 1 Enfants (1 à 5 ans) - Garçon  (PDI)"
    idp_child1_5_female = 33,    # "B4.b. 2 Enfants (1 à 5 ans) -  Fille  (PDI)"
    idp_child6_11_male = 34,     # "B4.c. 1 Enfants (6 à 11 ans) - Garçon  (PDI)"
    idp_child6_11_female = 35,   # "B4.c. 2 Enfants (6 à 11 ans) - Fille  (PDI)"
    idp_adolescent_male = 36,    # "B4.d. 1 Adolescents (12 à 17 ans) - Garçon  (PDI)"
    idp_adolescent_female = 37,  # "B4.d. 2 Adolescents (12 à 17 ans) - Fille  (PDI)"
    idp_adult_male = 38,         # "B4.e. Adultes (18 à 59 ans) - Homme  (PDI)"
    idp_adult_female = 39,       # "B4.e. Adultes (18 à 59 ans) - Femme  (PDI)"
    idp_elderly_male = 40,       # "B4.f Personnes âgées (+=60ans) - Homme  (PDI)"
    idp_elderly_female = 41,     # "B4.f Personnes âgées (+=60ans)- Femme  (PDI)"
    # Origin information
    origin_dept_pcode = 42,      # Origin department p-code
    origin_dept_name = 43,       # Origin department name
    origin_commune_pcode = 44,   # Origin commune p-code
    origin_commune_name = 45,    # Origin commune name
    displacement_date = 46,      # Displacement date
    displacement_reason = 47,    # Displacement reason
    # Returnee data
    returnee_households = 48,    # "Ménages retournés anciennes PDI"
    returnee_total = 49,         # "Personnes retournées anciennes PDI"
    # Priority needs
    need_food = 74,              # Food need
    need_water = 75,             # Water need
    need_shelter = 76,           # Shelter need
    need_health = 77,            # Health need
    need_nfi = 78,               # NFI need
    need_hygiene = 79,           # Hygiene need
    need_education = 80,         # Education need
    need_work = 81               # Work need
  ) %>%
  # Convert numeric columns
  dplyr::mutate(
    dplyr::across(c(key_informants, idp_households, idp_total, idp_baby_male:idp_elderly_female, 
             returnee_households, returnee_total), as.numeric),
    # Create total children and adult categories - fixed approach
    idp_children_total = (idp_baby_male + idp_baby_female + idp_child1_5_male + 
                         idp_child1_5_female + idp_child6_11_male + idp_child6_11_female +
                         idp_adolescent_male + idp_adolescent_female),
    idp_adults_total = (idp_adult_male + idp_adult_female + idp_elderly_male + idp_elderly_female),
    # Replace NAs with 0 for the totals
    idp_children_total = ifelse(is.na(idp_children_total), 0, idp_children_total),
    idp_adults_total = ifelse(is.na(idp_adults_total), 0, idp_adults_total),
    # Clean the commune p-code for matching
    commune_pcode = trimws(commune_pcode),
    # Convert evaluation_date to Date if needed
    evaluation_date = as.Date(evaluation_date)
  ) %>%
  # Filter for records with IDP presence
  dplyr::filter(idp_present == "oui" | !is.na(idp_total))

# Aggregate data by section (ADM3 level) - this is the correct approach
idp_by_section <- idp_cleaned %>%
  dplyr::group_by(section_pcode, section_name, commune_name, dept_name) %>%
  dplyr::summarise(
    total_idp_sites = n(),
    total_idp_households = sum(idp_households, na.rm = TRUE),
    total_idp_persons = sum(idp_total, na.rm = TRUE),
    total_children = sum(idp_children_total, na.rm = TRUE),
    total_adults = sum(idp_adults_total, na.rm = TRUE),
    total_returnee_households = sum(returnee_households, na.rm = TRUE),
    total_returnee_persons = sum(returnee_total, na.rm = TRUE),
    # Calculate priority needs percentages
    pct_need_food = mean(need_food == "oui", na.rm = TRUE) * 100,
    pct_need_water = mean(need_water == "oui", na.rm = TRUE) * 100,
    pct_need_shelter = mean(need_shelter == "oui", na.rm = TRUE) * 100,
    pct_need_health = mean(need_health == "oui", na.rm = TRUE) * 100,
    .groups = 'drop'
  ) %>%
  # Remove records with no IDPs
  dplyr::filter(total_idp_persons > 0)

# Check the aggregated data
cat("Aggregated IDP data by section:\n")
print(head(idp_by_section))
cat("\nTotal sections with IDPs:", nrow(idp_by_section), "\n")

# Join with spatial data using section_pcode
haiti_idp_map <- Haiti_adm3 %>%
  dplyr::left_join(idp_by_section, by = c("ADM3_PCODE" = "section_pcode")) %>%
  # Replace NA values with 0 for mapping
  dplyr::mutate(
    total_idp_persons = ifelse(is.na(total_idp_persons), 0, total_idp_persons),
    total_idp_households = ifelse(is.na(total_idp_households), 0, total_idp_households),
    total_idp_sites = ifelse(is.na(total_idp_sites), 0, total_idp_sites)
  )

# Create choropleth map
# Calculate natural breaks for non-zero values
idp_values <- haiti_idp_map$total_idp_persons[haiti_idp_map$total_idp_persons > 0]
breaks <- classIntervals(idp_values, n = 6, style = "jenks")
break_points <- breaks$brks

# Create labels for the breaks
break_labels <- c("0", paste0(scales::comma(break_points[-length(break_points)]), " - ", 
                              scales::comma(break_points[-1])))

# Create discrete categories
haiti_idp_map <- haiti_idp_map %>%
  dplyr::mutate(
    idp_category = case_when(
      total_idp_persons == 0 ~ "0",
      total_idp_persons <= break_points[2] ~ break_labels[2],
      total_idp_persons <= break_points[3] ~ break_labels[3],
      total_idp_persons <= break_points[4] ~ break_labels[4],
      total_idp_persons <= break_points[5] ~ break_labels[5],
      total_idp_persons <= break_points[6] ~ break_labels[6],
      TRUE ~ break_labels[7]
    ),
    idp_category = factor(idp_category, levels = break_labels)
  )

# Create choropleth map
idp_map <- ggplot(haiti_idp_map) +
  geom_sf(aes(fill = idp_category), color = "gray80", size = 0.05) +
  scale_fill_viridis_d(
    name = "IDPs\n(Persons)",
    option = "plasma",
    na.value = "grey90",
    direction = 1
  ) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    legend.position = "right"
  ) +
  labs(
    title = "Distribution of Internally Displaced Persons in Haiti",
    subtitle = "By Administrative Level 3 (Section Communale)",
    caption = "Source: IDP Assessment Data"
  )

# Display the map
print(idp_map)

# Summary statistics
cat("Summary of IDP Distribution:\n")
cat("Total sections with IDPs:", sum(haiti_idp_map$total_idp_persons > 0), "\n")
cat("Total IDP persons:", sum(haiti_idp_map$total_idp_persons, na.rm = TRUE), "\n")
cat("Total IDP households:", sum(haiti_idp_map$total_idp_households, na.rm = TRUE), "\n")
cat("Total IDP sites:", sum(haiti_idp_map$total_idp_sites, na.rm = TRUE), "\n")

# Show top 10 sections by IDP population
top_sections <- haiti_idp_map %>%
  sf::st_drop_geometry() %>%
  dplyr::filter(total_idp_persons > 0) %>%
  dplyr::arrange(desc(total_idp_persons)) %>%
  dplyr::select(ADM3_EN, section_name, commune_name, dept_name, total_idp_persons, total_idp_sites) %>%
  head(10)

print("Top 10 Sections by IDP Population:")
print(top_sections)

# Show some statistics by department
dept_summary <- haiti_idp_map %>%
  sf::st_drop_geometry() %>%
  dplyr::filter(total_idp_persons > 0) %>%
  dplyr::group_by(dept_name) %>%
  dplyr::summarise(
    sections_with_idps = n(),
    total_idp_persons = sum(total_idp_persons, na.rm = TRUE),
    total_idp_households = sum(total_idp_households, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  dplyr::arrange(desc(total_idp_persons))

print("IDP Distribution by Department:")
print(dept_summary)
```

```{r, results='asis'}

#| label: Generate-final-map
#| code-summary: "Generate the final map including predictions & population data"
#| echo: false
#| warning: false

# Load required libraries
library(leaflet)
library(sf)
library(raster)
library(exactextractr)
library(dplyr)
library(htmltools)
library(viridis)

baseline_raster <- if(class(Aeae_ensemble_mean)[1] == "RasterBrick") {
  Aeae_ensemble_mean[[1]]  # Take first layer if it's a brick
} else {
  Aeae_ensemble_mean
}

wl15_raster <- if(class(Aeae_WL1_5_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL1_5_suitability_mean[[1]]
} else {
  Aeae_WL1_5_suitability_mean
}

wl2_raster <- if(class(Aeae_WL2_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL2_suitability_mean[[1]]
} else {
  Aeae_WL2_suitability_mean
}

wl3_raster <- if(class(Aeae_WL3_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL3_suitability_mean[[1]]
} else {
  Aeae_WL3_suitability_mean
}

wl4_raster <- if(class(Aeae_WL4_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL4_suitability_mean[[1]]
} else {
  Aeae_WL4_suitability_mean
}

# Prepare the raster stack with all variables
raster_stack <- stack(
  # Baseline ensemble suitability 
  baseline_raster,
  # Future warming levels
  wl15_raster,
  wl2_raster, 
  wl3_raster,
  wl4_raster,
  # Population raster
  GPW2020_cropped
)

# Set proper names for the stack
names(raster_stack) <- c(
  "baseline_suitability",
  "suitability_1_5C", 
  "suitability_2C",
  "suitability_3C", 
  "suitability_4C",
  "population_2020"
)

# Calculate zonal statistics using exact_extract on the existing haiti_idp_map
zonal_stats <- exact_extract(
  raster_stack,
  haiti_idp_map,
  function(values, coverage_fraction) {
    # Calculate mean values weighted by coverage fraction for suitability
    suitability_means <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      weighted.mean(x, coverage_fraction, na.rm = TRUE)
    })
    
    # Calculate sum for population (total population per section)
    population_sum <- sum(values[, 6] * coverage_fraction, na.rm = TRUE)
    
    # Return results
    result <- data.frame(
      mean_baseline_suitability = suitability_means[1],
      mean_suitability_1_5C = suitability_means[2],
      mean_suitability_2C = suitability_means[3], 
      mean_suitability_3C = suitability_means[4],
      mean_suitability_4C = suitability_means[5],
      total_population_2020 = population_sum
    )
    
    return(result)
  }
)

# Combine zonal statistics with the existing haiti_idp_map (which already has IDP data)
Haiti_adm3_with_stats <- haiti_idp_map %>%
  bind_cols(zonal_stats) %>%
  mutate(
    # Calculate additional metrics
    suitability_change_1_5C = mean_suitability_1_5C - mean_baseline_suitability,
    suitability_change_2C = mean_suitability_2C - mean_baseline_suitability,
    suitability_change_3C = mean_suitability_3C - mean_baseline_suitability,
    suitability_change_4C = mean_suitability_4C - mean_baseline_suitability,
    # Calculate IDP density using existing IDP data
    idp_density = ifelse(total_population_2020 > 0, 
                        (total_idp_persons / total_population_2020) * 100, 0),
    # Round values for display
    across(starts_with("mean_"), ~round(.x, 3)),
    across(starts_with("suitability_change"), ~round(.x, 3)),
    across(starts_with("total_population"), ~round(.x, 0)),
    idp_density = round(idp_density, 2)
  )

# Create color palette for baseline suitability
suitability_pal <- colorNumeric(
  palette = viridis::plasma(100),
  domain = Haiti_adm3_with_stats$mean_baseline_suitability,
  na.color = "transparent"
)

# Create interactive map
interactive_map <- leaflet(Haiti_adm3_with_stats) %>%
  addProviderTiles("OpenStreetMap", group = "OpenStreetMap") %>%
  addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
  addProviderTiles("CartoDB.Positron", group = "Light") %>%
  setView(lng = -72.3, lat = 19.0, zoom = 8) %>%
  
  # Add administrative boundaries with baseline suitability coloring
  addPolygons(
    fillColor = ~suitability_pal(mean_baseline_suitability),
    fillOpacity = 0.7,
    color = "white",
    weight = 0.5,
    opacity = 1,
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM3_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      
      "<b>Administrative Info:</b><br>",
      "Section: ", ADM3_EN, "<br>",
      "Commune: ", ADM2_EN, "<br>", 
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM3_PCODE, "<br><br>",
      
      "<b>Aedes aegypti Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (", 
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (", 
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>", 
      "3°C warming: ", mean_suitability_3C, " (", 
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (", 
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      
      "</div>"
    ),
    label = ~paste0(ADM3_EN, " (", ADM2_EN, ")")
  ) %>%
  
  # Add layer controls
  addLayersControl(
    baseGroups = c("OpenStreetMap", "Satellite", "Light"),
    options = layersControlOptions(collapsed = FALSE),
    position = "topleft"
  ) %>%
  
  # Add legend
  addLegend(
    pal = suitability_pal,
    values = ~mean_baseline_suitability,
    title = HTML("Suitability"),
    position = "bottomright",
    opacity = 1
  )

# Summary statistics
cat("\n=== ZONAL STATISTICS SUMMARY ===\n")
cat("Total administrative sections:", nrow(Haiti_adm3_with_stats), "\n")
cat("Sections with population data:", sum(Haiti_adm3_with_stats$total_population_2020 > 0, na.rm = TRUE), "\n")
cat("Sections with IDPs:", sum(Haiti_adm3_with_stats$total_idp_persons > 0, na.rm = TRUE), "\n\n")

# Store the enhanced dataset
assign("Haiti_adm3_interactive", Haiti_adm3_with_stats, envir = .GlobalEnv)
assign("haiti_interactive_map", interactive_map, envir = .GlobalEnv)

# Display the interactive map
print(interactive_map)
```

```         
```

## Save Document

Save this as `haiti-enm.qmd` and run:

``` bash
quarto render haiti-enm.qmd
```
