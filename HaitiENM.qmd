---
title: "Ecological Niche Modeling for Aedes aegypti in Haiti"
subtitle: "Climate-driven species distribution analysis"
author: "Ian Pshea-Smith"
date: today
format:
  html:
    theme: journal
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    smooth-scroll: true
    fig-width: 10
    fig-height: 6
    embed-resources: true
    citations-hover: true
    footnotes-hover: true
output-file: Aeae_SDMs_About.html
execute:
  warning: false
  message: false
  cache: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  out.width = "100%"
)
```

## Introduction {#sec-intro}

This analysis focuses on ecological niche modeling for *Aedes aegypti* mosquitoes in Haiti, utilizing publicly available data to understand species distribution patterns under current and future climate scenarios, and providing an assessment of the degree to which the population at risk might change under three scenarios of climate change.

## Libraries and Setup {#sec-setup}

```{r}
#| label: libraries
#| code-summary: "Load required packages"
#| include: false

# Core data manipulation and analysis
  library(readr)
  library(dplyr)
  library(sf)

# Spatial data and modeling
  library(raster)
  library(dismo)
  library(gbm)
  library(exactextractr)
  library(geodata)

# Visualization
  library(ggplot2)
  library(ggfortify)
  library(RColorBrewer)
  library(viridis)
  library(gridExtra)
  library(leaflet)

# Misc. packages
  library(lctools)
  library(pROC)
  library(jsonlite)
  library(parallel)
  library(doParallel)
  library(purrr)
```

## Visualization Setup {#sec-viz-setup}

```{r}
#| label: plotting-setup
#| code-summary: "Custom plotting functions and color schemes"
#| include: false

# Custom color palette
  plasma_colors <- c("#5805dfff", "#8d01efff", "#bf09eeff", "#ec2cdbff", 
                   "#ff51bdff", "#ff769fff", "#ff9d82ff", "#ffc666ff", "#fff44aff")

# Custom plotting function
  plot_minimal <- function(raster, title) {
    plot(raster, 
         col = plasma_colors, 
         main = title, 
         legend = TRUE, 
         axes = FALSE, 
         box = FALSE, 
         legend.args = list(
           text = "Value", 
           side = 4, 
           font = 2, 
           line = 2.5, 
           cex = 0.8
         ))
  }
```

## Data Import and Processing {#sec-data-import}

```{r}
#| label: data-import-simple
#| code-summary: "Import data directly from GitHub repository"
#| include: false

# Create data directory if it doesn't exist
  if (!dir.exists("data")) {
    dir.create("data", recursive = TRUE)
  }

# GitHub base URL for raw files
  github_base <- "https://raw.githubusercontent.com/IanPsheaSmith/HaitiAeaeENM/main/HaitiShapefiles/"

# Download trap site data
  if (!file.exists("data/HaitiTrapsBin.csv")) {
    download.file(paste0(github_base, "HaitiTrapsBin.csv"), 
                  "data/HaitiTrapsBin.csv", mode = "wb")
  }
  HaitiTrapsBin <- read_csv("data/HaitiTrapsBin.csv")

# Convert to spatial objects
  HaitiTrapsBin_sf <- st_as_sf(HaitiTrapsBin, 
                              coords = c("Longitude", "Latitude"), 
                              crs = 4326)
  HaitiTrapsBin_utm <- st_transform(HaitiTrapsBin_sf, crs = 32618)
```

```{r}
#| label: shapefiles-import
#| code-summary: "Download and import shapefiles"
#| include: false
  
  # Haiti boundary shapefile components
    haiti_files <- c("HaitiPolygon.shp", "HaitiPolygon.shx", "HaitiPolygon.dbf", 
                     "HaitiPolygon.prj", "HaitiPolygon.cpg")
  
  # Download missing files using vectorized operations
    haiti_local_paths <- paste0("data/", haiti_files)
    haiti_missing <- haiti_files[!file.exists(haiti_local_paths)]
    haiti_urls <- paste0(github_base, haiti_missing)
    haiti_destinations <- paste0("data/", haiti_missing)
  
    if(length(haiti_missing) > 0) {
      mapply(download.file, haiti_urls, haiti_destinations, 
             MoreArgs = list(mode = "wb", quiet = TRUE))
    }
  
  # Load Haiti boundary
    haitiPolygon <- st_read("data/HaitiPolygon.shp")
    haitiPolygon_geo <- st_transform(haitiPolygon, crs = 4326)
  
  # Administrative boundaries shapefile components  
    adm3_files <- c("hti_admbnda_adm3_cnigs_20181129.shp", 
                    "hti_admbnda_adm3_cnigs_20181129.shx",
                    "hti_admbnda_adm3_cnigs_20181129.dbf", 
                    "hti_admbnda_adm3_cnigs_20181129.prj",
                    "hti_admbnda_adm3_cnigs_20181129.CPG")
  
  # Download missing administrative boundary files
    adm3_local_paths <- paste0("data/", adm3_files)
    adm3_missing <- adm3_files[!file.exists(adm3_local_paths)]
    adm3_urls <- paste0(github_base, adm3_missing)
    adm3_destinations <- paste0("data/", adm3_missing)
  
    if(length(adm3_missing) > 0) {
      mapply(download.file, adm3_urls, adm3_destinations, 
             MoreArgs = list(mode = "wb", quiet = TRUE))
    }
  
  # Load administrative boundaries
    Haiti_adm3 <- st_read("data/hti_admbnda_adm3_cnigs_20181129.shp")
    Haiti_adm3$ID <- as.numeric(as.factor(Haiti_adm3$ADM3_EN))
  
  # Haiti without waterways shapefile components (same approach as others)
    haiti_no_water_files <- c("Haiti_no_Wtrwy.shp", "Haiti_no_Wtrwy.shx", "Haiti_no_Wtrwy.dbf", 
                              "Haiti_no_Wtrwy.prj", "Haiti_no_Wtrwy.cpg")
  
  # Download missing waterway files (same pattern as above)
    water_local_paths <- paste0("data/", haiti_no_water_files)
    water_missing <- haiti_no_water_files[!file.exists(water_local_paths)]
    water_urls <- paste0(github_base, water_missing)
    water_destinations <- paste0("data/", water_missing)
  
    if(length(water_missing) > 0) {
      mapply(download.file, water_urls, water_destinations, 
             MoreArgs = list(mode = "wb", quiet = TRUE))
    }
```

## Study Site Maps {#sec-visuals}

```{r}
#| label: species-visualization
#| code-summary: "Process Aedes aegypti data and create visualizations"
#| fig-cap: "Haiti administrative boundaries and Aedes aegypti presence locations"
  
  # Create Aedes aegypti presence dataset first
    AeaePoints <- HaitiTrapsBin[HaitiTrapsBin$Aeae == 1, ]
    AeaePoints <- AeaePoints[, !(names(AeaePoints) %in% c("Cxq", "Aealb"))]
  
  # Convert to spatial objects
    AeaePoints_sf <- st_as_sf(AeaePoints, coords = c("Longitude", "Latitude"), crs = 4326)
    AeaePoints_utm <- st_transform(AeaePoints_sf, crs = 32618)
  
  # Static visualization using first chunk's style with Aedes aegypti points
    static_plot <- ggplot() +
      geom_sf(data = haitiPolygon_geo, fill = "white", color = "grey", size = 0.25) +
      geom_sf(data = Haiti_adm3, fill = NA, color = NA, size = 0.3) +
      geom_sf(data = AeaePoints_sf, color = "#702963", size = 1.5, alpha = 0.7) +
      theme_minimal() +
      theme(
        axis.ticks = element_blank(),
        panel.grid = element_line(color = "grey90", size = 0.3)
      )
    
    static_plot
  
```

```{r}
#| label: Interactive visualization with mapgl
#| code-summary: "Create interactive map of study sites in Haiti"
#| fig-cap: "Haiti administrative boundaries and Aedes aegypti presence locations"
   
  # Convert sf objects to coordinates for mapgl
    aeae_coords <- st_coordinates(AeaePoints_sf)
    aeae_data <- data.frame(
      longitude = aeae_coords[,1],
      latitude = aeae_coords[,2],
      species = "Aedes aegypti"
    )
  
  # Interactive visualization with leaflet
    interactive_map <- leaflet() %>%
      addTiles() %>%  # Add default OpenStreetMap tiles
      setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
      
      # Add Haiti boundary
      addPolygons(
        data = haitiPolygon_geo,
        fillColor = "white",
        fillOpacity = 0.1,
        color = "grey",
        weight = 1,
        popup = "Haiti"
      ) %>%
      
      # Add Aedes aegypti points
      addCircleMarkers(
        data = AeaePoints_sf,
        radius = 5,
        fillColor = "#702963",
        color = "white",
        weight = 1,
        opacity = 1,
        fillOpacity = 0.8,
        popup = ~paste("Aedes aegypti presence<br>",
                       "Site ID:", row.names(AeaePoints))
      ) %>%
      
      # Add legend
      addLegend(
        position = "bottomright",
        colors = "#702963",
        labels = "<span style='font-style: italic;'>Aedes aegypti</span> presence",
        title = "Species Distribution"
      )
  
    interactive_map
  
  # Print summary information
    cat("Dataset Summary:\n")
    cat("- Aedes aegypti presence sites:", nrow(AeaePoints), "\n")
    cat("- Presence rate:", round(nrow(AeaePoints)/nrow(HaitiTrapsBin)*100, 1), "%\n")
```

## Raster Data Download {#sec-rasters}

```{r}
#| label: bioclim-download
#| code-summary: "Download bioclimatic variables for Haiti"
#| include: false

# Define where to download the rasters (using relative path)
  haiti_bioclim_path <- "data/bioclim"

# Create directory if it doesn't exist
  if (!dir.exists(haiti_bioclim_path)) {
    dir.create(haiti_bioclim_path, recursive = TRUE)
  }

# Download 30-second (0.5-minute) resolution bioclimatic variables for Haiti
  Haiti_bioclim_data <- worldclim_country(
    country = "HTI",       # ISO code for Haiti
    var = "bio",           # Download bioclimatic variables
    res = 0.5,             # 0.5-minute resolution (30 seconds)
    path = haiti_bioclim_path
  )

# Rename & change to "raster" format using your working approach
  Haiti_bioclim_rast <- raster() # Create an empty raster, then switch formatting and add to a raster stack
  Haiti_bioclim_rast$AnnMeanTemp <- raster(Haiti_bioclim_data$wc2.1_30s_bio_1)
  Haiti_bioclim_rast$MeanDiurnRange <- raster(Haiti_bioclim_data$wc2.1_30s_bio_2)
  Haiti_bioclim_rast$Isothermality <- raster(Haiti_bioclim_data$wc2.1_30s_bio_3)
  Haiti_bioclim_rast$TempSeasonality <- raster(Haiti_bioclim_data$wc2.1_30s_bio_4)    
  Haiti_bioclim_rast$MaxTempWarmMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_5)
  Haiti_bioclim_rast$MinTempColdMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_6)
  Haiti_bioclim_rast$TempAnnRnge <- raster(Haiti_bioclim_data$wc2.1_30s_bio_7)
  Haiti_bioclim_rast$MTempWetQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_8)
  Haiti_bioclim_rast$MTempDryQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_9)
  Haiti_bioclim_rast$MTempWarmQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_10)
  Haiti_bioclim_rast$MTempCldQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_11)
  Haiti_bioclim_rast$AnnPrcp <- raster(Haiti_bioclim_data$wc2.1_30s_bio_12)
  Haiti_bioclim_rast$PrcpWetMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_13)
  Haiti_bioclim_rast$PrcpDryMnth <- raster(Haiti_bioclim_data$wc2.1_30s_bio_14)
  Haiti_bioclim_rast$PrcpSeasonality <- raster(Haiti_bioclim_data$wc2.1_30s_bio_15)
  Haiti_bioclim_rast$PrcpWetQurt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_16)
  Haiti_bioclim_rast$PrcpDryQrt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_17)
  Haiti_bioclim_rast$PrcpWrmQrt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_18)
  Haiti_bioclim_rast$PrcpCldQrt <- raster(Haiti_bioclim_data$wc2.1_30s_bio_19)

# Check the result
  print(paste("Created raster stack with", nlayers(Haiti_bioclim_rast), "bioclimatic layers"))
  print("Layer names:")
  print(names(Haiti_bioclim_rast))
```

```{r}
#| label: bioclim-processing
#| code-summary: "Process and crop bioclimatic rasters using Haiti boundary without waterways"
#| include: false

# Load Haiti without waterways shapefile 
  haiti_wo_lakes <- st_read("data/Haiti_no_Wtrwy.shp")

# Reproject the shapefile to match the CRS of the bioclim rasters
  haiti_wo_lakes_reproj <- st_transform(haiti_wo_lakes, crs = st_crs(Haiti_bioclim_rast))

# Convert to Spatial object for raster operations (required for mask/crop)
  haiti_wo_lakes_sp <- as(haiti_wo_lakes_reproj, "Spatial")

# Crop and mask Haiti_bioclim_rast using the reprojected shapefile
  Haiti_bioclim_shaped <- mask(crop(Haiti_bioclim_rast, haiti_wo_lakes_sp), haiti_wo_lakes_sp)

# Create final covariate stack for modeling
  BRT_All_Covars <- Haiti_bioclim_shaped

# Display results
  print(paste("Final covariate stack contains", nlayers(BRT_All_Covars), "layers"))
  print("Available covariates:")
  print(names(BRT_All_Covars))

# Show extent and resolution
  print(paste("Extent:", paste(as.vector(extent(BRT_All_Covars)), collapse = ", ")))
  print(paste("Resolution:", paste(res(BRT_All_Covars), collapse = " x ")))
  
# Plot rasters
  plot_minimal(BRT_All_Covars)
```

# Future Climate Scenarios

```{r}
#| label: setup-python-environment
#| echo: false
#| eval: false
#| include: false

# Only install if not already present
if (!requireNamespace("reticulate", quietly = TRUE)) {
  options(repos = c(CRAN = "https://cloud.r-project.org/"))
  install.packages("reticulate")
}

library(reticulate)

```

```{r}
#| label: python-package-environment
#| echo: false
#| include: false

# Python setup
library(reticulate)

# Check Python installation
py_config()

# Install packages one by one to catch errors
install_packages <- function() {
    # Basic packages first
    py_install("requests", pip = TRUE)
    py_install("pandas", pip = TRUE)
    py_install("numpy", pip = TRUE)
    py_install("xarray", pip = TRUE)
    py_install("geopandas", pip = TRUE)
    
    # Then spatial packages
    tryCatch({
        py_install("rasterio", pip = TRUE)
    }, error = function(e) {
        message("rasterio failed, trying conda-forge")
        conda_install("r-reticulate", "rasterio", channel = "conda-forge")
    })
    
    # Verify what's installed
    message("Checking installations:")
    message("requests: ", py_module_available("requests"))
    message("pandas: ", py_module_available("pandas"))
    message("numpy: ", py_module_available("numpy"))
}

install_packages()
```



```{python}
#| label: python-cmip6-download
#| echo: false
#| include: false

import xarray as xr
import numpy as np
import pandas as pd
import geopandas as gpd
import rasterio
from rasterio.mask import mask
import requests
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# Configuration
# ============================================================================

class CMIP6Downloader:
    def __init__(self, output_dir="cmip6_haiti_python", cache_dir="cache"):
        self.output_dir = Path(output_dir)
        self.cache_dir = Path(cache_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        
        # Alternative CMIP6 data sources
        self.sources = {
            'worldclim': 'https://geodata.ucdavis.edu/cmip6/30s/',
            'pangeo': 'https://storage.googleapis.com/cmip6/',
            'esgf': 'https://esgf-node.llnl.gov/search/cmip6/'
        }
        
        # Haiti bounding box
        self.haiti_bounds = {
            'min_lon': -74.5,
            'max_lon': -71.5,
            'min_lat': 18.0,
            'max_lat': 20.1
        }
    
    def smart_download(self, url, local_path, chunk_size=8192*1024, max_retries=3):
        """Download with resume capability and chunking"""
        
        local_path = Path(local_path)
        temp_path = local_path.with_suffix('.tmp')
        
        # Check if already downloaded
        if local_path.exists():
            print(f"  ✓ Already exists: {local_path.name}")
            return True
        
        headers = {}
        mode = 'wb'
        resume_pos = 0
        
        # Check for partial download
        if temp_path.exists():
            resume_pos = temp_path.stat().st_size
            headers['Range'] = f'bytes={resume_pos}-'
            mode = 'ab'
            print(f"  ↻ Resuming from {resume_pos/1024**2:.1f} MB")
        
        for attempt in range(max_retries):
            try:
                with requests.get(url, headers=headers, stream=True, timeout=30) as r:
                    r.raise_for_status()
                    
                    # Get total size
                    total_size = int(r.headers.get('content-length', 0)) + resume_pos
                    
                    with open(temp_path, mode) as f:
                        downloaded = resume_pos
                        for chunk in r.iter_content(chunk_size=chunk_size):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                                
                                # Progress
                                if total_size > 0:
                                    pct = (downloaded / total_size) * 100
                                    print(f"\r  ⬇ {pct:.1f}% ({downloaded/1024**3:.2f} GB / "
                                          f"{total_size/1024**3:.2f} GB)", end='', flush=True)
                
                print()  # New line after progress
                # Move temp to final
                temp_path.rename(local_path)
                print(f"  ✓ Downloaded: {local_path.name}")
                return True
                
            except Exception as e:
                print(f"\n  ✗ Attempt {attempt+1} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))  # Exponential backoff
                    
        return False
    
    def download_with_crop(self, url, model, scenario, period, variable='bioc'):
        """Download and immediately crop to Haiti"""
        
        filename = f"wc2.1_30s_{variable}_{model}_{scenario}_{period}.tif"
        global_path = self.cache_dir / filename
        haiti_path = self.output_dir / f"haiti_{filename}"
        
        # Check if Haiti version exists
        if haiti_path.exists():
            print(f"  ✓ Haiti subset exists: {haiti_path.name}")
            return str(haiti_path)
        
        # Download global file if needed
        if not global_path.exists():
            full_url = f"{self.sources['worldclim']}{model}/{scenario}/{filename}"
            success = self.smart_download(full_url, global_path)
            if not success:
                return None
        
        # Crop to Haiti
        print(f"  ✂ Cropping to Haiti...")
        try:
            with rasterio.open(global_path) as src:
                # Create bounding box
                from rasterio.mask import mask
                from shapely.geometry import box
                
                haiti_box = box(self.haiti_bounds['min_lon'], 
                              self.haiti_bounds['min_lat'],
                              self.haiti_bounds['max_lon'], 
                              self.haiti_bounds['max_lat'])
                
                # Crop
                out_image, out_transform = mask(src, [haiti_box], crop=True)
                out_meta = src.meta.copy()
                
                # Update metadata
                out_meta.update({
                    "driver": "GTiff",
                    "height": out_image.shape[1],
                    "width": out_image.shape[2],
                    "transform": out_transform,
                    "compress": "deflate"
                })
                
                # Write Haiti subset
                with rasterio.open(haiti_path, "w", **out_meta) as dest:
                    dest.write(out_image)
            
            # Size comparison
            global_size = global_path.stat().st_size / 1024**3
            haiti_size = haiti_path.stat().st_size / 1024**2
            print(f"  ✓ Size: {global_size:.2f} GB → {haiti_size:.2f} MB "
                  f"({100*(1-haiti_size/(global_size*1024)):.1f}% reduction)")
            
            # Delete global file to save space
            global_path.unlink()
            print(f"  ✓ Deleted global file")
            
            return str(haiti_path)
            
        except Exception as e:
            print(f"  ✗ Crop failed: {e}")
            return None

# Alternative: Use intake-esm for direct CMIP6 access
def get_cmip6_from_pangeo(model, scenario, variable='tas'):
    """Access CMIP6 data from Pangeo cloud catalog"""
    import intake
    
    # Open CMIP6 catalog
    col_url = "https://storage.googleapis.com/cmip6/pangeo-cmip6.json"
    col = intake.open_esm_datastore(col_url)
    
    # Search for specific data
    query = col.search(
        experiment_id=scenario,
        table_id='Amon',
        variable_id=variable,
        source_id=model
    )
    
    # Load as xarray dataset
    ds_dict = query.to_dataset_dict(aggregate=True)
    
    return ds_dict

# Alternative: Use OpenDAP for streaming access
def stream_cmip6_opendap(opendap_url, haiti_bounds):
    """Stream data via OpenDAP without full download"""
    
    # Open remote dataset
    ds = xr.open_dataset(opendap_url)
    
    # Subset to Haiti bounds immediately
    ds_haiti = ds.sel(
        lon=slice(haiti_bounds['min_lon'], haiti_bounds['max_lon']),
        lat=slice(haiti_bounds['min_lat'], haiti_bounds['max_lat'])
    )
    
    # Load only Haiti subset into memory
    ds_haiti = ds_haiti.load()
    
    return ds_haiti

```

```{python}
#| label: python-cmip6-processing
#| echo: false
#| include: false

# ============================================================================
# Main Processing Pipeline
# ============================================================================

def run_haiti_climate_pipeline(models, scenarios, periods, use_opendap=False):
    """Main pipeline using multiple download strategies"""
    
    downloader = CMIP6Downloader()
    results = []
    
    for model in models:
        for scenario in scenarios:
            for period in periods:
                print(f"\n{'='*60}")
                print(f"Processing: {model} - {scenario} - {period}")
                print('='*60)
                
                # Strategy 1: Try WorldClim with smart download
                haiti_file = downloader.download_with_crop(
                    url=None,  # Built in method
                    model=model,
                    scenario=scenario,
                    period=period
                )
                
                if haiti_file:
                    results.append({
                        'model': model,
                        'scenario': scenario,
                        'period': period,
                        'file': haiti_file,
                        'source': 'worldclim'
                    })
                    continue
                
                # Strategy 2: Try Pangeo cloud data
                if use_opendap:
                    print("  Trying Pangeo cloud catalog...")
                    try:
                        ds = get_cmip6_from_pangeo(model, scenario.replace('ssp', 'ssp'))
                        if ds:
                            # Process and save
                            haiti_file = process_pangeo_data(ds, model, scenario, period)
                            results.append({
                                'model': model,
                                'scenario': scenario,
                                'period': period,
                                'file': haiti_file,
                                'source': 'pangeo'
                            })
                            continue
                    except Exception as e:
                        print(f"  Pangeo failed: {e}")
                
                print(f"  ✗ Could not obtain data for this combination")
    
    return pd.DataFrame(results)

# Parallel download with threading
def parallel_download_batch(download_tasks, max_workers=3):
    """Download multiple files in parallel with controlled concurrency"""
    
    downloader = CMIP6Downloader()
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_task = {
            executor.submit(
                downloader.download_with_crop,
                None,  # URL built internally
                task['model'],
                task['scenario'],
                task['period']
            ): task for task in download_tasks
        }
        
        # Process completed tasks
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                result = future.result()
                if result:
                    print(f"✓ Completed: {task['model']} {task['scenario']} {task['period']}")
                    results.append(result)
            except Exception as e:
                print(f"✗ Failed: {task['model']} {task['scenario']} {task['period']}: {e}")
    
    return results

# ============================================================================
# Execute - FULL DATASET
# ============================================================================
if __name__ == "__main__":
    # Complete list of available models
    models = [
        'ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 
        'CanESM5', 'CanESM5-CanOE', 'CMCC-ESM2', 'CNRM-CM6-1', 
        'CNRM-ESM2-1', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FIO-ESM-2-0', 
        'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 
        'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'MIROC-ES2L', 
        'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'UKESM1-0-LL'
    ]
    
    # All SSP scenarios
    scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']
    
    # All available time periods
    periods = [
        '2021-2040', '2041-2060', '2061-2080', '2081-2100'
    ]
    
    print(f"Starting full download pipeline:")
    print(f"  Models: {len(models)} ({', '.join(models[:3])}...)")
    print(f"  Scenarios: {len(scenarios)} ({', '.join(scenarios)})")
    print(f"  Periods: {len(periods)} ({', '.join(periods)})")
    print(f"  Total combinations: {len(models) * len(scenarios) * len(periods)}")
    print(f"  Estimated time: {(len(models) * len(scenarios) * len(periods)) * 2} minutes")
    
    # Add progress tracking
    import time
    start_time = time.time()
    
    # Run the full pipeline
    results_df = run_haiti_climate_pipeline(models, scenarios, periods, use_opendap=False)
    
    # Calculate runtime
    runtime = time.time() - start_time
    hours = runtime // 3600
    minutes = (runtime % 3600) // 60
    
    # Save comprehensive results
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results_file = f'haiti_climate_data_complete_{timestamp}.csv'
    results_df.to_csv(results_file, index=False)
    
    # Print final summary
    print(f"\n{'='*80}")
    print(f"PIPELINE COMPLETE!")
    print(f"{'='*80}")
    print(f"Runtime: {int(hours)}h {int(minutes)}m")
    print(f"Successfully downloaded: {len(results_df)}/{len(models) * len(scenarios) * len(periods)} datasets")
    print(f"Success rate: {len(results_df)/(len(models) * len(scenarios) * len(periods))*100:.1f}%")
    print(f"Results saved to: {results_file}")
    
    # Show breakdown by model
    if len(results_df) > 0:
        print(f"\nSuccess by model:")
        model_success = results_df.groupby('model').size().sort_values(ascending=False)
        for model, count in model_success.head(10).items():
            print(f"  {model}: {count}/{len(scenarios)*len(periods)} datasets")
```

```{python}
#| label: python-cmip6-processing-2
#| echo: false
#| include: false

import os
import time
import requests
from pathlib import Path

# Configuration
OUTPUT_DIR = Path(r"C:\Users\ianpsheasmith\OneDrive - University of Florida\Documents - Haiti Vector\General\cmip6_haiti_python")
TEMP_DIR = OUTPUT_DIR / "temp"
TEMP_DIR.mkdir(exist_ok=True)

# All expected models and scenarios
ALL_MODELS = [
    'ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 
    'CanESM5', 'CanESM5-CanOE', 'CMCC-ESM2', 'CNRM-CM6-1', 
    'CNRM-ESM2-1', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FIO-ESM-2-0', 
    'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 
    'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'MIROC-ES2L', 
    'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'UKESM1-0-LL'
]

SCENARIOS = ['ssp126', 'ssp245', 'ssp370', 'ssp585']
PERIODS = ['2021-2040', '2041-2060', '2061-2080', '2081-2100']

def check_existing_files():
    """Check which files already exist"""
    existing = []
    for file in OUTPUT_DIR.glob("haiti_*.tif"):
        existing.append(file.name)
    return existing

def identify_missing_files():
    """Identify which files need to be downloaded"""
    existing = check_existing_files()
    missing = []
    
    for model in ALL_MODELS:
        for scenario in SCENARIOS:
            for period in PERIODS:
                filename = f"haiti_wc2.1_30s_bioc_{model}_{scenario}_{period}.tif"
                if filename not in existing:
                    missing.append({
                        'model': model,
                        'scenario': scenario,
                        'period': period,
                        'filename': filename
                    })
    
    return missing

def download_file(url, output_path, max_retries=3):
    """Download with retry logic and resume capability"""
    
    for attempt in range(max_retries):
        try:
            # Check if we can resume
            headers = {}
            if output_path.exists():
                resume_size = output_path.stat().st_size
                headers['Range'] = f'bytes={resume_size}-'
                print(f"  Resuming from {resume_size/1024**2:.1f} MB")
            
            response = requests.get(url, headers=headers, stream=True, timeout=1800)
            
            # Write file
            mode = 'ab' if output_path.exists() else 'wb'
            with open(output_path, mode) as f:
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                for chunk in response.iter_content(chunk_size=8192*1024):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            pct = (downloaded / total_size) * 100
                            print(f"\r  Progress: {pct:.1f}%", end='', flush=True)
                
                print()  # New line
                return True
                
        except Exception as e:
            print(f"\n  Attempt {attempt+1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(10)
    
    return False

def download_missing_files(max_files=None):
    """Main function to download missing files"""
    
    print("="*60)
    print("CMIP6 Missing Files Downloader")
    print("="*60)
    
    # Check what's missing
    missing = identify_missing_files()
    existing = check_existing_files()
    
    print(f"\nInventory:")
    print(f"  Existing files: {len(existing)}")
    print(f"  Missing files: {len(missing)}")
    print(f"  Total expected: 400")
    print(f"  Completion: {len(existing)/400*100:.1f}%")
    
    if len(missing) == 0:
        print("\nAll files downloaded!")
        return
    
    # Show missing by model
    missing_by_model = {}
    for m in missing:
        model = m['model']
        if model not in missing_by_model:
            missing_by_model[model] = 0
        missing_by_model[model] += 1
    
    print("\nMissing files by model:")
    for model, count in sorted(missing_by_model.items()):
        print(f"  {model}: {count} files")
    
    # Limit downloads if requested
    if max_files:
        missing = missing[:max_files]
        print(f"\nLimiting to {max_files} downloads")
    
    # Download missing files
    print(f"\nStarting downloads for {len(missing)} files...")
    print("="*60)
    
    success_count = 0
    fail_count = 0
    
    for i, file_info in enumerate(missing, 1):
        print(f"\n[{i}/{len(missing)}] {file_info['filename']}")
        
        # Build URL
        model = file_info['model']
        scenario = file_info['scenario']
        period = file_info['period']
        
        base_filename = f"wc2.1_30s_bioc_{model}_{scenario}_{period}.tif"
        url = f"https://geodata.ucdavis.edu/cmip6/30s/{model}/{scenario}/{base_filename}"
        
        # Download to temp first
        temp_path = TEMP_DIR / base_filename
        
        print(f"  URL: {url}")
        print(f"  Downloading...")
        
        if download_file(url, temp_path):
            # If successful, move to final location
            # Here we would normally crop to Haiti, but for now just rename
            final_path = OUTPUT_DIR / file_info['filename']
            
            # For now, just copy the file (we'll need to add cropping later)
            temp_path.rename(final_path)
            
            print(f"   Success, Saved as {file_info['filename']}")
            success_count += 1
        else:
            print(f"   Failed to download")
            fail_count += 1
            if temp_path.exists():
                temp_path.unlink()  # Clean up partial download
        
        # Pause between downloads
        if i < len(missing):
            print(f"  Pausing 10 seconds...")
            time.sleep(10)
    
    # Final summary
    print("\n" + "="*60)
    print("Download Summary:")
    print(f"  Successful: {success_count}")
    print(f"  Failed: {fail_count}")
    print(f"  New total: {len(existing) + success_count}/400 files")
    print(f"  New completion: {(len(existing) + success_count)/400*100:.1f}%")
    print("="*60)

if __name__ == "__main__":
    download_missing_files()
    
import os
import glob
from pathlib import Path
import rasterio
from rasterio.windows import from_bounds
import pandas as pd
import time
import shutil

def cleanup_and_validate_rasters(data_dir, size_threshold_small=1000, size_threshold_large=2000):
    """
    Clean up downloaded rasters with better file handling
    """
    
    print(f"Looking for .tif files in: {os.path.abspath(data_dir)}")
    
    # Get all .tif files
    tif_files = glob.glob(os.path.join(data_dir, "*.tif"))
    print(f"Found {len(tif_files)} .tif files to validate")
    
    removed_files = []
    recropped_files = []
    valid_files = []
    failed_files = []
    
    # Haiti bounding box (left, bottom, right, top)
    haiti_bbox = (-74.5, 18.0, -71.6, 20.1)
    
    for tif_file in tif_files:
        file_size_kb = os.path.getsize(tif_file) / 1024
        file_name = os.path.basename(tif_file)
        
        print(f"\nProcessing: {file_name} ({file_size_kb:.1f} KB)")
        
        # Remove files that are too small (failed downloads)
        if file_size_kb < size_threshold_small:
            print(f"   Removing (too small): {file_size_kb:.1f} KB")
            try:
                os.remove(tif_file)
                removed_files.append(file_name)
            except PermissionError:
                print(f"     Could not remove (file in use), marking for manual deletion")
                failed_files.append(file_name)
            continue
            
        # Check if file needs re-cropping (too large, likely global)
        if file_size_kb > size_threshold_large:
            print(f"   Re-cropping (too large): {file_size_kb:.1f} KB")
            
            # Try multiple times with delays for file locking issues
            success = False
            for attempt in range(3):
                try:
                    # Wait a bit for file locks to clear
                    if attempt > 0:
                        time.sleep(2)
                        print(f"    Retry attempt {attempt + 1}/3...")
                    
                    # Read the raster
                    with rasterio.open(tif_file) as src:
                        bounds = src.bounds
                        width = bounds.right - bounds.left
                        height = bounds.top - bounds.bottom
                        
                        if width > 10 or height > 10:
                            print(f"    Large extent detected: {width:.1f}° × {height:.1f}°")
                            
                            # Create window for Haiti cropping
                            window = from_bounds(*haiti_bbox, src.transform)
                            
                            # Read the windowed data
                            out_image = src.read(window=window)
                            out_transform = src.window_transform(window)
                            
                            # Update metadata
                            out_meta = src.meta.copy()
                            out_meta.update({
                                "driver": "GTiff",
                                "height": out_image.shape[1],
                                "width": out_image.shape[2],
                                "transform": out_transform,
                                "compress": "lzw"
                            })
                    
                    # Create temp file with different name to avoid conflicts
                    temp_file = os.path.join(data_dir, f"temp_cropped_{int(time.time())}_{file_name}")
                    
                    # Write cropped version
                    with rasterio.open(temp_file, "w", **out_meta) as dest:
                        dest.write(out_image)
                    
                    # Close all file handles and wait
                    time.sleep(1)
                    
                    # Replace original with cropped version
                    if os.path.exists(temp_file):
                        # Try to remove original
                        try:
                            os.remove(tif_file)
                            time.sleep(0.5)
                            os.rename(temp_file, tif_file)
                            
                            new_size_kb = os.path.getsize(tif_file) / 1024
                            print(f"     Cropped: {file_size_kb:.1f} KB → {new_size_kb:.1f} KB")
                            recropped_files.append(file_name)
                            valid_files.append(file_name)
                            success = True
                            break
                            
                        except (PermissionError, OSError) as e:
                            print(f"     Could not replace original file: {e}")
                            # Keep the cropped version with a new name
                            cropped_name = file_name.replace('.tif', '_cropped.tif')
                            final_path = os.path.join(data_dir, cropped_name)
                            if os.path.exists(final_path):
                                os.remove(final_path)
                            os.rename(temp_file, final_path)
                            print(f"     Saved as: {cropped_name}")
                            recropped_files.append(cropped_name)
                            valid_files.append(cropped_name)
                            failed_files.append(f"{file_name} (original too large, see {cropped_name})")
                            success = True
                            break
                    
                except Exception as e:
                    print(f"     Attempt {attempt + 1} failed: {e}")
                    if os.path.exists(temp_file):
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                    continue
            
            if not success:
                print(f"     Failed to process after 3 attempts")
                failed_files.append(file_name)
                continue
        else:
            print(f"   Valid size: {file_size_kb:.1f} KB")
            valid_files.append(file_name)
    
    # Summary report
    print(f"\n{'='*80}")
    print(f"CLEANUP COMPLETE")
    print(f"{'='*80}")
    print(f"Files removed (too small): {len(removed_files)}")
    print(f"Files re-cropped (too large): {len(recropped_files)}")
    print(f"Valid files remaining: {len(valid_files)}")
    print(f"Failed to process: {len(failed_files)}")
    
    if failed_files:
        print(f"\nFiles that need manual attention:")
        for f in failed_files:
            print(f"  - {f}")
    
    return {
        'removed': removed_files,
        'recropped': recropped_files,
        'valid': valid_files,
        'failed': failed_files
    }

# ============================================================================
# Execute cleanup
# ============================================================================
if __name__ == "__main__":
    data_directory = r"C:\Users\ianpsheasmith\OneDrive - University of Florida\Documents - Haiti Vector\General\cmip6_haiti_python"
    
    print("Starting raster cleanup and validation...")
    print("Note: Large files will be processed with retry logic for file locking issues")
    
    results = cleanup_and_validate_rasters(data_directory)
    
    if results:
        print(f"\nCleanup completed!")
        if results['failed']:
            print(f"Some files couldn't be processed automatically - check the failed files list above")
    else:
        print("No .tif files found - check for errors")
```

```{r}
#| label: parallel-processing-setup
#| code-summary: "Configure parallel processing for ensemble modeling"
#| echo: false
#| include: false
  
  # Configure parallel processing - use half of available cores
    n_cores <- max(1, floor(detectCores() / 2))
    cl <- makeCluster(n_cores)
    registerDoParallel(cl)
    cat("Configured parallel processing with", n_cores, "cores (half of available)\n")
  
  # Export necessary objects and functions to cluster
    clusterEvalQ(cl, {
        library(dismo)
        library(gbm)
        library(raster)
        library(sp)
        library(dplyr)
    })

```

Pseudo-absence Creation

```{r}
#| label: pseudo-absence-functions
#| code-summary: "Define functions for pseudo-absence generation and data preparation"
#| include: false  

  # Define function to generate spatially-stratified pseudo-absence points
    generate_pseudo_absences_aeae <- function(presence_points, raster_stack,
                                            buffer_radius = 717, n_pseudo = 60, seed_val = 123) {
        set.seed(seed_val)
        presence_sp <- SpatialPoints(presence_points, proj4string = crs(raster_stack))
        presence_buffers <- buffer(presence_sp, width = buffer_radius)
        buffer_mask <- rasterize(presence_buffers, raster_stack, field = 1, background = NA)
        valid_area <- mask(raster_stack[[1]], buffer_mask, inverse = TRUE)
        pseudo_points <- randomPoints(valid_area, n = n_pseudo)
        pseudo_df <- as.data.frame(pseudo_points)
        colnames(pseudo_df) <- c("x", "y")
        return(pseudo_df)
    }
  
  # Define function to prepare modeling dataset with scaling
    prepare_modeling_data <- function(presence_data, pseudo_absence_data) {
        presence_labeled <- cbind(Presence = 1, presence_data)
        pseudo_labeled <- cbind(Presence = 0, pseudo_absence_data)
        combined_data <- rbind(presence_labeled, pseudo_labeled)
        combined_data <- as.data.frame(combined_data)
        combined_data$Presence <- as.numeric(combined_data$Presence)
        clean_data <- na.omit(combined_data)
        
        numeric_cols <- sapply(clean_data, is.numeric)
        if (any(numeric_cols)) {
            for (col in names(clean_data)[numeric_cols]) {
                if (col != "Presence") {
                    clean_data[[col]][is.infinite(clean_data[[col]])] <- NA
                }
            }
            clean_data <- na.omit(clean_data)
        }
        
        # Scale all predictor variables (excluding Presence)
          predictor_cols <- names(clean_data)[names(clean_data) != "Presence"]
          scaled_predictors <- scale(clean_data[, predictor_cols])
          
        # Combine scaled predictors with Presence variable
          scaled_data <- data.frame(
              Presence = clean_data$Presence,
              scaled_predictors
          )
        
        # Store scaling parameters
          scaling_params <- list(
              center = attr(scaled_predictors, "scaled:center"),
              scale = attr(scaled_predictors, "scaled:scale")
          )
          attr(scaled_data, "scaling_params") <- scaling_params
          return(scaled_data)
    }
  
    cat("Pseudo-absence generation functions defined\n")

```

```{r}
#| label: generate-pseudo-absence-datasets
#| code-summary: "Generate pseudo-absence datasets with NA handling and presence point relocation"
#| include: false

# Function to relocate presence points that fall on NA values
relocate_na_presence_points <- function(presence_points, raster_stack, search_radius = 100) {
    # Extract environmental values
    env_values <- raster::extract(raster_stack, presence_points)
    na_rows <- which(apply(env_values, 1, function(x) any(is.na(x))))
    
    if (length(na_rows) == 0) {
        return(presence_points)
    }
    
    cat("Found", length(na_rows), "presence points with NA environmental values\n")
    cat("Relocating to nearest valid cells...\n")
    
    # Create a copy of presence points
    adjusted_points <- presence_points
    
    # Process each problematic point
    relocated_info <- purrr::map_dfr(na_rows, function(idx) {
        original_point <- presence_points[idx, ]
        
        # Create a small buffer around the point to search for valid cells
        point_sp <- SpatialPoints(matrix(c(original_point$x, original_point$y), ncol = 2),
                                 proj4string = crs(raster_stack))
        buffer_zone <- buffer(point_sp, width = search_radius)
        
        # Sample candidate points within the buffer
        candidate_points <- spsample(buffer_zone, n = 100, type = "regular")
        candidate_coords <- coordinates(candidate_points)
        
        # Check which candidates have valid environmental data
        candidate_env <- raster::extract(raster_stack, candidate_coords)
        valid_candidates <- !apply(candidate_env, 1, function(x) any(is.na(x)))
        
        if (any(valid_candidates)) {
            # Find the closest valid point
            valid_coords <- candidate_coords[valid_candidates, , drop = FALSE]
            distances <- sqrt((valid_coords[, 1] - original_point$x)^2 + 
                            (valid_coords[, 2] - original_point$y)^2)
            closest_idx <- which.min(distances)
            new_coords <- valid_coords[closest_idx, ]
            
            adjusted_points[idx, ] <- data.frame(x = new_coords[1], y = new_coords[2])
            
            return(data.frame(
                point_id = idx,
                original_x = original_point$x,
                original_y = original_point$y,
                new_x = new_coords[1],
                new_y = new_coords[2],
                distance_moved = distances[closest_idx],
                relocated = TRUE
            ))
        } else {
            return(data.frame(
                point_id = idx,
                original_x = original_point$x,
                original_y = original_point$y,
                new_x = NA,
                new_y = NA,
                distance_moved = NA,
                relocated = FALSE
            ))
        }
    })
    
    # Print relocation results
    successfully_relocated <- sum(relocated_info$relocated, na.rm = TRUE)
    cat("Successfully relocated", successfully_relocated, "of", length(na_rows), "points\n")
    
    if (successfully_relocated < length(na_rows)) {
        failed_points <- relocated_info[!relocated_info$relocated, ]
        cat("Warning: Could not relocate", nrow(failed_points), "points\n")
        # Remove points that couldn't be relocated
        adjusted_points <- adjusted_points[-failed_points$point_id, ]
    }
    
    return(list(
        adjusted_points = adjusted_points,
        relocation_info = relocated_info
    ))
}

# Enhanced function that ensures we get exactly n_pseudo valid points
generate_pseudo_absences_aeae_fixed <- function(presence_points, raster_stack,
                                                buffer_radius = 717, n_pseudo = 60, 
                                                seed_val = 123, max_attempts = 10) {
    set.seed(seed_val)
    presence_sp <- SpatialPoints(presence_points, proj4string = crs(raster_stack))
    presence_buffers <- buffer(presence_sp, width = buffer_radius)
    buffer_mask <- rasterize(presence_buffers, raster_stack, field = 1, background = NA)
    valid_area <- mask(raster_stack[[1]], buffer_mask, inverse = TRUE)
    
    # Generate extra points to account for NAs
    generate_batch <- function(n_needed, valid_area_raster, raster_stack_full) {
        n_to_generate <- ceiling(n_needed * 1.5)
        candidate_points <- randomPoints(valid_area_raster, n = n_to_generate)
        candidate_df <- as.data.frame(candidate_points)
        colnames(candidate_df) <- c("x", "y")
        
        # Check which points have valid environmental data
        env_values <- raster::extract(raster_stack_full, candidate_df)
        valid_rows <- !apply(env_values, 1, function(x) any(is.na(x)))
        
        candidate_df[valid_rows, ]
    }
    
    # Iteratively generate points until we have enough
    all_valid_points <- data.frame()
    attempt <- 0
    
    while (nrow(all_valid_points) < n_pseudo && attempt < max_attempts) {
        attempt <- attempt + 1
        n_needed <- n_pseudo - nrow(all_valid_points)
        new_points <- generate_batch(n_needed, valid_area, raster_stack)
        all_valid_points <- rbind(all_valid_points, new_points)
    }
    
    # Return exactly n_pseudo points
    if (nrow(all_valid_points) >= n_pseudo) {
        return(all_valid_points[1:n_pseudo, ])
    } else {
        warning(paste("Could only generate", nrow(all_valid_points), "valid pseudo-absence points"))
        return(all_valid_points)
    }
}

# Set parameters
n_replicates <- 30
n_pseudo_per_replicate <- 60
buffer_radius <- 717  # meters

# Prepare presence points coordinates
AeaePoints_df <- data.frame(
    x = AeaePoints$Longitude,
    y = AeaePoints$Latitude
)

presence_env_test <- raster::extract(BRT_All_Covars, AeaePoints_df)
na_rows <- which(apply(presence_env_test, 1, function(x) any(is.na(x))))

if (length(na_rows) > 0) {
    cat("Points with NA values (indices):", na_rows, "\n")
    cat("Coordinates of NA points:\n")
    print(AeaePoints_df[na_rows, ])
    
    # Check which variables have NAs
    na_vars <- purrr::map_chr(na_rows, function(idx) {
        na_cols <- which(is.na(presence_env_test[idx, ]))
        paste(names(BRT_All_Covars)[na_cols], collapse = ", ")
    })
    cat("Variables with NA values for each point:\n")
    purrr::walk2(na_rows, na_vars, ~ cat("  Point", .x, ":", .y, "\n"))
}

# Relocate presence points that fall on NA values
cat("\nRelocating presence points with NA values...\n")
relocation_result <- relocate_na_presence_points(AeaePoints_df, BRT_All_Covars)
AeaePoints_df_adjusted <- relocation_result$adjusted_points

cat("Adjusted presence points:", nrow(AeaePoints_df_adjusted), 
    "(originally", nrow(AeaePoints_df), ")\n\n")

# Function to process a single replicate
process_replicate <- function(i, presence_points_adjusted, raster_stack, 
                            buffer_radius, n_pseudo_per_rep) {
    seed_val <- 1999 + i
    cat("Processing replicate", i, "of 30... ")
    
    # Generate pseudo-absence points with NA handling
    pseudo_points <- generate_pseudo_absences_aeae_fixed(
        presence_points = presence_points_adjusted,
        raster_stack = raster_stack,
        buffer_radius = buffer_radius,
        n_pseudo = n_pseudo_per_rep,
        seed_val = seed_val
    )
    
    # Extract environmental data for presence points
    presence_env <- raster::extract(raster_stack, presence_points_adjusted)
    presence_data <- cbind(presence_points_adjusted, presence_env)
    
    # Extract environmental data for pseudo-absence points
    pseudo_env <- raster::extract(raster_stack, pseudo_points)
    pseudo_data <- cbind(pseudo_points, pseudo_env)
    
    # Prepare the combined modellling dataset with scaling
    modeling_data <- prepare_modeling_data(
        presence_data = presence_data,
        pseudo_absence_data = pseudo_data
    )
    
    # Store metadata about the replicate
    replicate_info <- list(
        pseudo_points = pseudo_points,
        seed = seed_val,
        n_pseudo = nrow(pseudo_points),
        n_presence = sum(modeling_data$Presence == 1),
        n_absence = sum(modeling_data$Presence == 0),
        total_points = nrow(modeling_data)
    )
    
    cat("completed (", nrow(modeling_data), "points )\n")
    
    return(list(
        dataset = modeling_data,
        info = replicate_info
    ))
}

# Generate all replicates using purrr::map
replicate_results <- purrr::map(
    1:n_replicates,
    ~ process_replicate(.x, 
                       AeaePoints_df_adjusted, 
                       BRT_All_Covars,
                       buffer_radius,
                       n_pseudo_per_replicate)
)

# Extract datasets and assign to global environment
purrr::walk2(
    replicate_results,
    1:n_replicates,
    ~ {
        dataset_name <- paste0("AeaeData_PseudoAbs_", .y)
        assign(dataset_name, .x$dataset, envir = .GlobalEnv)
    }
)

# Store replicate information
pseudo_absence_replicates <- purrr::map(replicate_results, ~ .x$info)

# Summary statistics
point_counts <- purrr::map_dbl(replicate_results, ~ .x$info$total_points)
presence_counts <- purrr::map_dbl(replicate_results, ~ .x$info$n_presence)
absence_counts <- purrr::map_dbl(replicate_results, ~ .x$info$n_absence)

cat("\n===============================================\n")
cat("PSEUDO-ABSENCE GENERATION COMPLETE\n")
cat("===============================================\n")
cat("Successfully created", n_replicates, "datasets\n\n")

cat("Dataset statistics:\n")
cat("  Total points per dataset:", unique(point_counts), "\n")
cat("  Presence points per dataset:", unique(presence_counts), "\n")
cat("  Pseudo-absence points per dataset:", unique(absence_counts), "\n")

# Get one example dataset for variable names
example_data <- replicate_results[[1]]$dataset
cat("\nEnvironmental variables:\n")
variable_names <- names(example_data)[!(names(example_data) %in% c("Presence", "x", "y"))]
cat("  ", paste(variable_names, collapse = "\n  "), "\n")

# Verification
dataset_names <- paste0("AeaeData_PseudoAbs_", 1:n_replicates)
datasets_exist <- purrr::map_lgl(dataset_names, exists)
cat("\nDataset verification:", sum(datasets_exist), "/", n_replicates, 
    "datasets successfully created\n")

# Store relocation info if points were relocated
if (exists("relocation_result") && !is.null(relocation_result$relocation_info)) {
    assign("AeaePoints_relocation_info", relocation_result$relocation_info, envir = .GlobalEnv)
    cat("\nRelocation information saved to 'AeaePoints_relocation_info'\n")
}
```

```{r}
#| label: table-2-covariate-comparison-table
#| code-summary: "Generate comparison table of environmental covariates at presence vs pseudo-absence locations"

library(tidyverse)
library(kableExtra)
library(purrr)

# Function to create variable metadata with interpretable names and units
create_variable_metadata <- function() {
	tibble::tibble(
		variable = c(
			"AnnMeanTemp", "MeanDiurnRange", "Isothermality", "TempSeasonality",
			"MaxTempWarmMnth", "MinTempColdMnth", "TempAnnRnge", "MTempWetQurt",
			"MTempDryQurt", "MTempWarmQurt", "MTempCldQurt", "AnnPrcp",
			"PrcpWetMnth", "PrcpDryMnth", "PrcpSeasonality", "PrcpWetQurt",
			"PrcpDryQrt", "PrcpWrmQrt", "PrcpCldQrt"
		),
		interpretable_name = c(
			"Annual Mean Temperature",
			"Mean Diurnal Range",
			"Isothermality",
			"Temperature Seasonality",
			"Max Temperature of Warmest Month",
			"Min Temperature of Coldest Month",
			"Temperature Annual Range",
			"Mean Temperature of Wettest Quarter",
			"Mean Temperature of Driest Quarter",
			"Mean Temperature of Warmest Quarter",
			"Mean Temperature of Coldest Quarter",
			"Annual Precipitation",
			"Precipitation of Wettest Month",
			"Precipitation of Driest Month",
			"Precipitation Seasonality",
			"Precipitation of Wettest Quarter",
			"Precipitation of Driest Quarter",
			"Precipitation of Warmest Quarter",
			"Precipitation of Coldest Quarter"
		),
		unit = c(
			"°C", "°C", "%", "SD × 100",
			"°C", "°C", "°C", "°C",
			"°C", "°C", "°C", "mm",
			"mm", "mm", "CV", "mm",
			"mm", "mm", "mm"
		)
	)
}

# Extract presence points data (same for all 30 datasets)
presence_data <- AeaeData_PseudoAbs_1 %>%
	filter(Presence == 1) %>%
	dplyr::select(-Presence, -x, -y)

# Combine all pseudo-absence points from 30 datasets
pseudo_absence_data <- purrr::map_dfr(1:30, function(i) {
	dataset_name <- paste0("AeaeData_PseudoAbs_", i)
	get(dataset_name, envir = .GlobalEnv) %>%
		filter(Presence == 0) %>%
		dplyr::select(-Presence, -x, -y)
})

# Calculate statistics for each variable
calculate_statistics <- function(data, var_name) {
	values <- data[[var_name]]
	tibble::tibble(
		variable = var_name,
		mean = mean(values, na.rm = TRUE),
		median = median(values, na.rm = TRUE),
		sd = sd(values, na.rm = TRUE)
	)
}

# Get all variable names
var_names <- names(presence_data)

# Calculate statistics for presence points
presence_stats <- purrr::map_dfr(var_names, 
	~ calculate_statistics(presence_data, .x)) %>%
	mutate(point_type = "Presence")

# Calculate statistics for pseudo-absence points
pseudo_absence_stats <- purrr::map_dfr(var_names, 
	~ calculate_statistics(pseudo_absence_data, .x)) %>%
	mutate(point_type = "Pseudo-absence")

# Combine and format the results
comparison_table <- bind_rows(presence_stats, pseudo_absence_stats) %>%
	pivot_wider(
		names_from = point_type,
		values_from = c(mean, median, sd),
		names_glue = "{point_type}_{.value}"
	) %>%
	left_join(create_variable_metadata(), by = "variable") %>%
	dplyr::select(
		interpretable_name,
		unit,
		Presence_mean,
		Presence_median,
		Presence_sd,
		`Pseudo-absence_mean`,
		`Pseudo-absence_median`,
		`Pseudo-absence_sd`
	) %>%
	mutate(
		across(starts_with("Presence") | starts_with("Pseudo"), 
			~ round(.x, 2))
	)

# Create formatted HTML table
comparison_table %>%
	kable(
		col.names = c(
			"Environmental Variable",
			"Unit",
			"Mean",
			"Median",
			"SD",
			"Mean",
			"Median",
			"SD"
		),
		caption = "Table 1. Comparison of environmental covariate values at presence and pseudo-absence locations for <i>Aedes aegypti</i> in Haiti",
		align = c("l", "c", rep("r", 6)),
		format = "html",
		escape = FALSE
	) %>%
	kable_styling(
		bootstrap_options = c("striped", "hover", "condensed", "responsive"),
		full_width = FALSE,
		font_size = 14
	) %>%
	add_header_above(
		c(" " = 2, 
		  "Presence Points (n = 60)" = 3, 
		  "Pseudo-absence Points (n = 1,800)" = 3),
		bold = TRUE
	) %>%
	row_spec(0, bold = TRUE, background = "#f0f0f0") %>%
	column_spec(1, bold = TRUE, width = "250px") %>%
	column_spec(2, italic = TRUE, width = "80px") %>%
	column_spec(3:8, width = "80px") %>%
	footnote(
		general = paste0(
			"Statistics calculated across the 60 presence points (consistent across all replicates) and ",
			nrow(pseudo_absence_data), " total pseudo-absence points ",
			"(60 points × 30 replicates). ",
			"Values represent unscaled environmental conditions extracted at each location."
		),
		general_title = "",
		footnote_as_chunk = TRUE
	)

# Print summary information
cat("\n===============================================\n")
cat("COVARIATE COMPARISON TABLE GENERATED\n")
cat("===============================================\n")
cat("Presence points analyzed:", nrow(presence_data), "\n")
cat("Pseudo-absence points analyzed:", nrow(pseudo_absence_data), 
    "(", nrow(pseudo_absence_data)/30, "per replicate )\n")
cat("Environmental variables compared:", length(var_names), "\n\n")

comparison_table
```

```{r}
#| label: visualize-pseudo-absence-distribution-interactive
#| code-summary: "Interactive leaflet map with toggleable pseudo-absence replicates"
#| fig-cap: "Interactive map showing presence points and toggleable pseudo-absence replicates"
  
  # Create color palette for different replicates
    n_replicates_to_show <- min(30, length(pseudo_absence_replicates))
    replicate_colors <- brewer.pal(min(11, n_replicates_to_show), "Spectral")
    if (n_replicates_to_show > 11) {
      # If more than 11 replicates, create additional colors
      additional_colors <- rainbow(n_replicates_to_show - 11, start = 0.7, end = 1)
      replicate_colors <- c(replicate_colors, additional_colors)
    }
  
  # Initialize the leaflet map
    interactive_pseudo_map <- leaflet() %>%
      addTiles() %>%
      setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
      
      # Add Haiti boundary
        addPolygons(
          data = haiti_wo_lakes_reproj,
          fillColor = "lightgray",
          fillOpacity = 0.1,
          color = "black",
          weight = 2,
          popup = "Haiti (excluding water bodies)"
        ) %>%
      
      # Add presence points (always visible)
        addCircleMarkers(
          data = AeaePoints_sf,
          radius = 6,
          fillColor = "#702963",
          color = "white",
          weight = 2,
          opacity = 1,
          fillOpacity = 0.9,
          popup = ~paste("<b>Aedes aegypti presence</b><br>",
                         "Site ID:", row.names(AeaePoints)),
          group = "Presence Points"
        ) %>%
      
      # Add legend for presence points
      addLegend(
        position = "topright",
        colors = "#702963",
        labels = "Aedes aegypti presence",
        title = "Species Data",
        opacity = 1
      )
  
  # Add each pseudo-absence replicate as a separate layer group
    for (i in 1:n_replicates_to_show) {
      rep_data <- pseudo_absence_replicates[[i]]
      pseudo_df <- rep_data$pseudo_points
      
      # Convert to sf object for leaflet
      pseudo_sf <- st_as_sf(pseudo_df, coords = c("x", "y"), crs = 4326)
      
      # Color for this replicate
      rep_color <- replicate_colors[i]
      
      interactive_pseudo_map <- interactive_pseudo_map %>%
        addCircleMarkers(
          data = pseudo_sf,
          radius = 4,
          fillColor = rep_color,
          color = "white",
          weight = 1,
          opacity = 0.8,
          fillOpacity = 0.7,
          popup = paste0("<b>Pseudo-absence Replicate ", i, "</b><br>",
                         "Seed: ", rep_data$seed, "<br>",
                         "Total pseudo-absence points: ", nrow(pseudo_df)),
          group = paste("Replicate", i)
        )
    }
  
  # Add layers control to toggle between replicates
    layer_groups <- c("Presence Points", paste("Replicate", 1:n_replicates_to_show))
    
    interactive_pseudo_map <- interactive_pseudo_map %>%
      addLayersControl(
        overlayGroups = layer_groups,
        options = layersControlOptions(collapsed = FALSE),
        position = "bottomleft"
      ) %>%
      
      # Hide all pseudo-absence layers initially except the first one
      hideGroup(paste("Replicate", 2:n_replicates_to_show)) %>%
      
      # Add a secondary legend for pseudo-absence replicates
      addLegend(
        position = "bottomright",
        colors = replicate_colors[1:min(5, n_replicates_to_show)],
        labels = paste("Replicate", 1:min(5, n_replicates_to_show)),
        title = "Pseudo-absence Sets<br>(Use layer control to toggle)",
        opacity = 0.7
      )
  
  # Print the interactive map
    interactive_pseudo_map

```

```{r}
#| label: Ensemble-BRT-modeling
#| code-summary: "Iteratively run the BRTs acros the 30 replicates"

# Load required libraries
suppressPackageStartupMessages({
    library(gbm)
    library(dplyr)
    library(purrr)
    library(raster)
    library(parallel)
    library(doParallel)
    library(sf)
    library(sp)
})

# Helper function for null coalescing
`%||%` <- function(x, y) if (is.null(x) || length(x) == 0 || any(is.na(x))) y else x

# Progress tracking function
update_progress <- function(current, total, prefix = "Progress") {
    percent <- round((current / total) * 100, 1)
    bar_length <- 40
    filled_length <- round(bar_length * current / total)
    bar <- paste0(c(rep("█", filled_length), rep("░", bar_length - filled_length)), collapse = "")
    cat("\r", prefix, ": [", bar, "] ", percent, "% (", current, "/", total, ")", sep = "")
    if (current == total) cat("\n")
    flush.console()
}

# Configure parallel processing
cat("=== HAITI AEDES AEGYPTI ENSEMBLE MODELING ===\n")
cl <- makeCluster(15)
registerDoParallel(cl)

# Prepare Haiti boundary for masking
cat("Preparing Haiti boundary for masking...\n")
if (exists("Haiti_adm3")) {
    Haiti_boundary_union <- st_union(Haiti_adm3)
    Haiti_boundary_sp <- as(Haiti_boundary_union, "Spatial")
} else {
    stop("Haiti_adm3 shapefile not found. Please load it first.")
}

# Enhanced BRT fitting function with variable selection and coordinate removal
fit_brt_with_selection <- function(data, weights, predictors = NULL) {
    tryCatch({
        # Remove coordinate columns
        coordinate_cols <- c("Longitude", "Latitude", "x", "y", "X", "Y", "lon", "lat")
        coord_present <- names(data)[names(data) %in% coordinate_cols]
        
        if (length(coord_present) > 0) {
            data <- data[, !names(data) %in% coordinate_cols, drop = FALSE]
            cat("    Removed coordinate columns:", paste(coord_present, collapse = ", "), "\n")
        }
        
        non_response_cols <- names(data)[names(data) != "Presence"]
        if (length(non_response_cols) == 0) {
            return(NULL)
        }
        
        if (!is.null(predictors)) {
            available_predictors <- predictors[predictors %in% names(data)]
            if (length(available_predictors) == 0) {
                return(NULL)
            }
            model_data <- data[, c("Presence", available_predictors)]
        } else {
            model_data <- data
        }
        
        # Initial model fit
        initial_model <- gbm(
            Presence ~ ., 
            data = model_data,
            n.trees = 5000,
            distribution = "bernoulli",
            interaction.depth = 5, 
            shrinkage = 0.005, 
            bag.fraction = 0.5,
            cv.folds = 10, 
            weights = weights, 
            n.minobsinnode = 10, 
            verbose = FALSE
        )
        
        if (is.null(initial_model)) return(NULL)
        
        # Variable selection with iterative removal
        current_model <- initial_model
        current_predictors <- names(model_data)[names(model_data) != "Presence"]
        iteration <- 0
        max_iterations <- 20
        
        while (iteration < max_iterations) {
            iteration <- iteration + 1
            
            rel_inf <- summary(current_model, plot = FALSE)
            low_importance_vars <- rel_inf[rel_inf$rel.inf < 5.0, "var"]
            
            if (length(low_importance_vars) == 0 || length(current_predictors) <= 3) {
                break
            }
            
            # Remove variable with lowest importance
            var_to_remove <- low_importance_vars[which.min(
                rel_inf[rel_inf$var %in% low_importance_vars, "rel.inf"])]
            current_predictors <- setdiff(current_predictors, var_to_remove)
            
            # Refit model with reduced variables
            reduced_data <- data[, c("Presence", current_predictors), drop = FALSE]
            
            updated_model <- tryCatch({
                gbm(
                    Presence ~ ., 
                    data = reduced_data, 
                    distribution = "bernoulli",
                    n.trees = 5000,
                    interaction.depth = 5, 
                    shrinkage = 0.005, 
                    bag.fraction = 0.5,
                    cv.folds = 10, 
                    weights = weights, 
                    n.minobsinnode = 10, 
                    verbose = FALSE
                )
            }, error = function(e) {
                current_predictors <<- c(current_predictors, var_to_remove)
                return(current_model)
            })
            
            if (!is.null(updated_model)) {
                current_model <- updated_model
            }
        }
        
        return(current_model)
        
    }, error = function(e) {
        return(NULL)
    })
}

# AUC calculation function
calculate_auc <- function(observed, predicted) {
    tryCatch({
        if (requireNamespace("pROC", quietly = TRUE)) {
            as.numeric(pROC::auc(pROC::roc(observed, predicted, quiet = TRUE)))
        } else {
            # Manual AUC calculation
            pred_order <- order(predicted, decreasing = TRUE)
            pos_ranks <- which(observed[pred_order] == 1)
            n_pos <- sum(observed == 1)
            n_neg <- sum(observed == 0)
            if (n_pos == 0 || n_neg == 0) return(NA_real_)
            auc_val <- (sum(pos_ranks) - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)
            as.numeric(auc_val)
        }
    }, error = function(e) NA_real_)
}

# Enhanced model fitting function for a single replicate with detailed diagnostics
fit_single_replicate <- function(replicate_num, dataset_names, total_replicates) {
    dataset_name <- dataset_names[replicate_num]
    
    # Update progress
    update_progress(replicate_num - 1, total_replicates, "Model Fitting")
    
    if (!exists(dataset_name)) {
        update_progress(replicate_num, total_replicates, "Model Fitting")
        return(list(
            replicate = replicate_num,
            dataset_name = dataset_name,
            success = FALSE,
            error = "Dataset not found"
        ))
    }
    
    tryCatch({
        # Load and prepare data
        raw_data <- get(dataset_name)
        clean_data <- raw_data %>%
            mutate(Presence = as.numeric(Presence)) %>%
            na.omit()
        
        # Comprehensive coordinate column removal with diagnostic reporting
        coordinate_cols <- c("Longitude", "Latitude", "x", "y", "X", "Y", "lon", "lat")
        found_coords <- names(clean_data)[names(clean_data) %in% coordinate_cols]
        
        if (length(found_coords) > 0) {
            cat("  Replicate", replicate_num, "- Removing coordinates:", paste(found_coords, collapse = ", "), "\n")
            clean_data <- clean_data[, !names(clean_data) %in% coordinate_cols, drop = FALSE]
        }
        
        # Verify no coordinate columns remain
        remaining_coords <- names(clean_data)[names(clean_data) %in% coordinate_cols]
        if (length(remaining_coords) > 0) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = paste("Failed to remove coordinates:", paste(remaining_coords, collapse = ", "))
            ))
        }
        
        # Data quality checks
        predictor_cols <- names(clean_data)[names(clean_data) != "Presence"]
        n_presences <- sum(clean_data$Presence)
        n_absences <- sum(clean_data$Presence == 0)
        
        cat("  Replicate", replicate_num, "- Predictors:", length(predictor_cols), 
            "| Presence:", n_presences, "| Absence:", n_absences, "\n")
        
        if (n_presences <= 15) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "Insufficient presence records"
            ))
        }
        
        if (length(predictor_cols) == 0) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "No predictor variables remaining"
            ))
        }
        
        # Train/test split
        set.seed(1999 + replicate_num)
        train_indices <- sample(seq_len(nrow(clean_data)),
                                size = floor(0.75 * nrow(clean_data)))
        train_data <- clean_data[train_indices, ]
        test_data <- clean_data[-train_indices, ]
        
        # Check training data adequacy
        train_presence <- sum(train_data$Presence == 1)
        train_absence <- sum(train_data$Presence == 0)
        
        if (train_presence < 5 || train_absence < 5) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "Insufficient training data"
            ))
        }
        
        # Apply differential weighting (presence = 1.0, absence = 0.5)
        train_weights <- ifelse(train_data$Presence == 1, 1.0, 0.5)
        
        # Fit BRT model with variable selection
        final_model <- fit_brt_with_selection(train_data, train_weights)
        
        if (is.null(final_model)) {
            update_progress(replicate_num, total_replicates, "Model Fitting")
            return(list(
                replicate = replicate_num,
                dataset_name = dataset_name,
                success = FALSE,
                error = "Model fitting failed"
            ))
        }
        
        # Calculate optimal number of trees
        optimal_trees <- gbm.perf(final_model, method = "cv", plot.it = FALSE)
        
        # Extract variable importance
        importance_summary <- summary(final_model, plot = FALSE)
        final_variables <- importance_summary$var
        
        # Calculate performance metrics
        train_preds <- predict(final_model, train_data, 
                               n.trees = optimal_trees, type = "response")
        test_preds <- predict(final_model, test_data, 
                              n.trees = optimal_trees, type = "response")
        
        train_rmse <- sqrt(mean((train_data$Presence - train_preds)^2))
        test_rmse <- sqrt(mean((test_data$Presence - test_preds)^2))
        train_auc <- calculate_auc(train_data$Presence, train_preds)
        test_auc <- calculate_auc(test_data$Presence, test_preds)
        
        # Cross-validation AUC from model
        cv_auc <- if (!is.null(final_model$cv.statistics)) {
            as.numeric(final_model$cv.statistics$discrimination.mean)
        } else {
            NA_real_
        }
        
        # Extract scaling parameters
        scaling_params <- attr(raw_data, "scaling_params")
        
        update_progress(replicate_num, total_replicates, "Model Fitting")
        
        # Return successful result (without spatial predictions)
        return(list(
            replicate = replicate_num,
            dataset_name = dataset_name,
            success = TRUE,
            model = final_model,
            optimal_trees = optimal_trees,
            variable_importance = importance_summary,
            final_variables = final_variables,
            performance = list(
                train_rmse = train_rmse,
                test_rmse = test_rmse,
                train_auc = train_auc,
                test_auc = test_auc,
                cv_auc = cv_auc,
                cv_error = min(final_model$cv.error)
            ),
            n_variables_final = length(final_variables),
            n_presences = n_presences,
            scaling_params = scaling_params
        ))
        
    }, error = function(e) {
        update_progress(replicate_num, total_replicates, "Model Fitting")
        return(list(
            replicate = replicate_num,
            dataset_name = dataset_name,
            success = FALSE,
            error = as.character(e$message)
        ))
    })
}

# Enhanced spatial prediction function with detailed error reporting
make_spatial_prediction <- function(model_result, replicate_index, total_replicates) {
    # Update progress for spatial predictions
    update_progress(replicate_index - 1, total_replicates, "Spatial Predictions")
    
    result <- list(
        replicate = model_result$replicate,
        success = FALSE,
        raster = NULL,
        error = NULL
    )
    
    tryCatch({
        model <- model_result$model
        optimal_trees <- model_result$optimal_trees
        scaling_params <- model_result$scaling_params
        final_variables <- model_result$final_variables
        
        if (!exists("BRT_All_Covars")) {
            result$error <- "BRT_All_Covars not found"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Check variable matching (coordinates should already be excluded from final_variables)
        available_predictors <- final_variables[final_variables %in% names(BRT_All_Covars)]
        
        if (length(available_predictors) == 0) {
            result$error <- paste("No matching variables. Model vars:", 
                                  paste(final_variables, collapse = ", "))
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        if (length(available_predictors) != length(final_variables)) {
            missing_vars <- setdiff(final_variables, available_predictors)
            cat("  Replicate", model_result$replicate, "- Note: Missing raster variables:", 
                paste(missing_vars, collapse = ", "), "\n")
        }
        
        # Select relevant covariates
        selected_covars <- BRT_All_Covars[[available_predictors]]
        
        # Extract raster values
        raster_values <- getValues(selected_covars)
        na_mask <- apply(is.na(raster_values), 1, any)
        valid_rows <- !na_mask
        
        if (sum(valid_rows) == 0) {
            result$error <- "No valid raster cells"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Verify scaling parameters exist and match
        if (is.null(scaling_params$center) || is.null(scaling_params$scale)) {
            result$error <- "Scaling parameters missing"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Check scaling parameter alignment
        scaling_vars_center <- names(scaling_params$center)
        scaling_vars_scale <- names(scaling_params$scale)
        
        if (!all(available_predictors %in% scaling_vars_center) || 
            !all(available_predictors %in% scaling_vars_scale)) {
            result$error <- "Scaling parameter names don't match variables"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Scale values using stored parameters
        valid_values <- raster_values[valid_rows, , drop = FALSE]
        scaled_values <- scale(valid_values,
                               center = scaling_params$center[available_predictors],
                               scale = scaling_params$scale[available_predictors])
        
        # Prepare prediction data
        pred_data <- as.data.frame(scaled_values)
        names(pred_data) <- available_predictors
        
        # Verify column alignment with model
        model_vars <- all.vars(model$Terms)[-1]  # Exclude response variable
        if (!all(model_vars %in% names(pred_data))) {
            result$error <- "Prediction data columns don't match model variables"
            update_progress(replicate_index, total_replicates, "Spatial Predictions")
            return(result)
        }
        
        # Make predictions
        predictions_valid <- predict(model, pred_data, 
                                     n.trees = optimal_trees, type = "response")
        
        # Create full prediction vector
        predictions_full <- rep(NA, nrow(raster_values))
        predictions_full[valid_rows] <- predictions_valid
        
        # Create prediction rasters
        pred_raster <- selected_covars[[1]]
        values(pred_raster) <- predictions_full
        
        # Apply Haiti boundary mask
        haiti_sp_transformed <- spTransform(Haiti_boundary_sp, 
                                            CRSobj = crs(pred_raster))
        pred_raster_masked <- tryCatch({
            mask(crop(pred_raster, haiti_sp_transformed), haiti_sp_transformed)
        }, error = function(e) {
            pred_raster
        })
        
        result$success <- TRUE
        result$raster <- pred_raster_masked
        result$error <- NULL
        
        update_progress(replicate_index, total_replicates, "Spatial Predictions")
        return(result)
        
    }, error = function(e) {
        result$error <- as.character(e$message)
        update_progress(replicate_index, total_replicates, "Spatial Predictions")
        return(result)
    })
}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#                    EXECUTE COMPLETE MODELING PIPELINE            #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# Check dataset availability and run initial diagnostics
dataset_names <- paste0("AeaeData_PseudoAbs_", 1:n_replicates)
existing_datasets <- dataset_names[map_lgl(dataset_names, exists)]

cat("Initial Dataset Diagnostics:\n")
cat("Found", length(existing_datasets), "datasets out of", n_replicates, "expected\n")

if (length(existing_datasets) == 0) {
    stop("No pseudo-absence datasets found")
}

# Quick diagnostic on first dataset to verify coordinate removal will work
if (length(existing_datasets) > 0) {
    first_data <- get(existing_datasets[1])
    coordinate_cols <- c("Longitude", "Latitude", "x", "y", "X", "Y", "lon", "lat")
    coords_found <- names(first_data)[names(first_data) %in% coordinate_cols]
    cat("Coordinates found in datasets:", paste(coords_found, collapse = ", "), "\n")
    cat("Environmental predictors available:", sum(!names(first_data) %in% c("Presence", coordinate_cols)), "\n")
}

# FIT ALL MODELS WITH PROGRESS TRACKING
model_results <- map(seq_along(existing_datasets), 
                     ~fit_single_replicate(.x, existing_datasets, length(existing_datasets)))

# Analyze model fitting results
successful_models <- model_results[map_lgl(model_results, ~.x$success)]
failed_models <- model_results[!map_lgl(model_results, ~.x$success)]

n_successful <- length(successful_models)
n_failed <- length(failed_models)

cat("  Successful models:", n_successful, "/", length(existing_datasets), "\n")
cat("  Failed models:", n_failed, "/", length(existing_datasets), "\n")

if (n_failed > 0) {
    cat("\nFailure analysis:\n")
    failure_reasons <- map_chr(failed_models, ~.x$error %||% "Unknown error")
    failure_table <- table(failure_reasons)
    print(failure_table)
}

# Display variable importance summary from successful models
if (n_successful > 0) {
    cat("\nVariable importance summary (top 5 variables across successful models):\n")
    all_importance <- map_dfr(successful_models, function(result) {
        if (!is.null(result$variable_importance)) {
            result$variable_importance %>%
                mutate(replicate = result$replicate) %>%
                head(5)
        }
    })
    
    if (nrow(all_importance) > 0) {
        importance_summary <- all_importance %>%
            group_by(var) %>%
            summarise(
                mean_importance = mean(rel.inf),
                frequency = n(),
                .groups = "drop"
            ) %>%
            arrange(desc(mean_importance)) %>%
            head(10)
        
        print(importance_summary)
    }
}

if (n_successful > 0) {
    spatial_results <- map2(successful_models, seq_along(successful_models), 
                            ~make_spatial_prediction(.x, .y, length(successful_models)))
    
    # Analyze spatial prediction results
    valid_predictions <- map(spatial_results[map_lgl(spatial_results, ~.x$success)], 
                             ~.x$raster)
    failed_spatial <- spatial_results[!map_lgl(spatial_results, ~.x$success)]
    
    n_spatial_success <- length(valid_predictions)
    
    cat("\nSpatial prediction results:\n")
    cat("  Successful spatial predictions:", n_spatial_success, "/", n_successful, "\n")
    
    # Analyze spatial failures
    if (length(failed_spatial) > 0) {
        cat("\nSpatial prediction failure analysis:\n")
        error_types <- map_chr(failed_spatial, ~.x$error %||% "Unknown error")
        error_table <- table(error_types)
        print(error_table)
    }
    
    # Update model results with spatial prediction status
    successful_models_with_spatial <- map2(successful_models, spatial_results,
                                           function(model_result, spatial_result) {
                                               model_result$predictions_raster <- spatial_result$raster
                                               model_result$has_spatial <- spatial_result$success
                                               model_result$spatial_error <- spatial_result$error
                                               return(model_result)
                                           })
    if (n_spatial_success > 0) {
        # Validate spatial consistency
        first_pred <- valid_predictions[[1]]
        pred_dims <- dim(first_pred)
        pred_extent <- extent(first_pred)
        pred_crs <- crs(first_pred)
        
        cat("Ensemble spatial properties:\n")
        cat("  Dimensions:", pred_dims[1], "x", pred_dims[2], "\n")
        cat("  Extent:", as.character(pred_extent), "\n")
        cat("  Valid predictions for ensemble:", n_spatial_success, "\n")
        
        # Check dimensional consistency
        dimension_check <- map_lgl(valid_predictions, function(pred) {
            identical(dim(pred), pred_dims) && 
                identical(extent(pred), pred_extent) &&
                compareCRS(crs(pred), pred_crs)
        })
        
        if (!all(dimension_check)) {
            cat("Inconsistent prediction dimensions\n")
            valid_predictions <- valid_predictions[dimension_check]
            n_spatial_success <- length(valid_predictions)
            cat("Proceeding with", n_spatial_success, "dimensionally consistent predictions\n")
        }
        
        # Create ensemble statistics based on available predictions
        if (n_spatial_success == 0) {
            cat("Error: No predictions with consistent dimensions\n")
        } else if (n_spatial_success == 1) {
            cat("Note: Only 1 valid prediction - using as ensemble\n")
            ensemble_mean <- valid_predictions[[1]]
            ensemble_sd <- ensemble_mean * 0
            ensemble_median <- ensemble_mean
            ensemble_cv <- ensemble_mean * 0
            ensemble_ci_lower <- ensemble_mean
            ensemble_ci_upper <- ensemble_mean
        } else {
            cat("Creating ensemble statistics from", n_spatial_success, "predictions...\n")
            
            ensemble_stack <- tryCatch({
                stack(valid_predictions)
            }, error = function(e) {
                cat("Error creating stack, attempting with explicit naming...\n")
                pred_list <- valid_predictions
                names(pred_list) <- paste0("pred_", seq_along(pred_list))
                stack(pred_list)
            })
            
            if (!is.null(ensemble_stack)) {
                ensemble_mean <- calc(ensemble_stack, fun = mean, na.rm = TRUE)
                ensemble_sd <- calc(ensemble_stack, fun = sd, na.rm = TRUE)
                ensemble_median <- calc(ensemble_stack, fun = median, na.rm = TRUE)
                ensemble_cv <- ensemble_sd / ensemble_mean
                ensemble_ci_lower <- ensemble_mean - 1.96 * ensemble_sd
                ensemble_ci_upper <- ensemble_mean + 1.96 * ensemble_sd
            }
        }
        
        # Store all products in global environment
        if (exists("ensemble_mean") && !is.null(ensemble_mean)) {
            assign("Aeae_ensemble_mean", ensemble_mean, envir = .GlobalEnv)
            assign("Aeae_ensemble_sd", ensemble_sd, envir = .GlobalEnv)
            assign("Aeae_ensemble_median", ensemble_median, envir = .GlobalEnv)
            assign("Aeae_ensemble_cv", ensemble_cv, envir = .GlobalEnv)
            assign("Aeae_ensemble_ci_lower", ensemble_ci_lower, envir = .GlobalEnv)
            assign("Aeae_ensemble_ci_upper", ensemble_ci_upper, envir = .GlobalEnv)
            assign("Aeae_successful_results", successful_models_with_spatial, envir = .GlobalEnv)
            assign("Aeae_individual_predictions", valid_predictions, envir = .GlobalEnv)
            
            # Create comprehensive performance summary
            performance_summary <- map_dfr(successful_models_with_spatial, function(result) {
                data.frame(
                    replicate = result$replicate,
                    dataset_name = result$dataset_name,
                    test_rmse = result$performance$test_rmse %||% NA,
                    test_auc = result$performance$test_auc %||% NA,
                    cv_auc = result$performance$cv_auc %||% NA,
                    n_variables_final = result$n_variables_final %||% NA,
                    has_spatial = result$has_spatial %||% FALSE,
                    spatial_error = if(is.null(result$spatial_error)) NA else result$spatial_error,
                    stringsAsFactors = FALSE
                )
            })
            
            assign("Aeae_performance_summary", performance_summary, envir = .GlobalEnv)
            
            # Calculate and display summary statistics
            mean_values <- values(ensemble_mean)
            mean_values_clean <- mean_values[!is.na(mean_values)]
            
            cat("\n", paste(rep("=", 60), collapse = ""), "\n")
            cat("HAITI AEDES AEGYPTI ENSEMBLE MODELING COMPLETE\n")
            cat(paste(rep("=", 60), collapse = ""), "\n")
            cat("Total datasets processed:", length(existing_datasets), "\n")
            cat("Successful model fits:", n_successful, "/", length(existing_datasets), 
                " (", round(n_successful/length(existing_datasets)*100, 1), "%)\n")
            cat("Models with spatial predictions:", n_spatial_success, "/", n_successful,
                " (", round(n_spatial_success/n_successful*100, 1), "%)\n")
            cat("Mean habitat suitability:", round(mean(mean_values_clean), 4), "\n")
            cat("Suitability range:", round(min(mean_values_clean), 4), "to", 
                round(max(mean_values_clean), 4), "\n")
            
            if (nrow(performance_summary) > 0) {
                spatial_success <- performance_summary[performance_summary$has_spatial == TRUE, ]
                if (nrow(spatial_success) > 0) {
                    cat("Mean test AUC (spatial models):", 
                        round(mean(spatial_success$test_auc, na.rm = TRUE), 3), "\n")
                    cat("Mean final variables per model:", 
                        round(mean(spatial_success$n_variables_final, na.rm = TRUE), 1), "\n")
                }
            }
            
            cat("\nGlobal Environment Products Created:\n")
            cat("- Aeae_ensemble_mean: Mean habitat suitability\n")
            cat("- Aeae_ensemble_sd: Prediction uncertainty\n") 
            cat("- Aeae_ensemble_median: Median suitability\n")
            cat("- Aeae_ensemble_cv: Coefficient of variation\n")
            cat("- Aeae_ensemble_ci_lower/upper: 95% confidence intervals\n")
            cat("- Aeae_individual_predictions: List of", length(valid_predictions), "individual model rasters\n")
            cat("- Aeae_successful_results: Detailed results for all successful models\n")
            cat("- Aeae_performance_summary: Model performance metrics\n")
            cat(paste(rep("=", 60), collapse = ""), "\n")
            
        } else {
            cat("Failed to create ensemble statistics\n")
        }
        
    } else {
        cat("No models produced valid spatial predictions\n")
    }
} else {
    cat("No successful models to create ensemble\n")
}

# Clean up parallel processing
stopCluster(cl)

```

```{r}
#| label: visualize-ensemble-suitability-interactive
#| code-summary: "Interactive leaflet map with habitat suitability predictions and individual models"
#| fig-cap: "Interactive map showing habitat suitability ensemble and individual BRT model predictions"

# Load required libraries
library(leaflet)
library(RColorBrewer)
library(htmlwidgets)

# Create color palette for suitability data
suitability_palette <- colorNumeric(
    palette = viridis::plasma(100),
    domain = c(1, 0),
    na.color = "transparent"
)

# Initialize the leaflet map
interactive_suitability_map <- leaflet() %>%
    addTiles() %>%
    setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
    
    # Add Haiti boundary
    addPolygons(
        data = haiti_wo_lakes_reproj,
        fillColor = "lightgray",
        fillOpacity = 0.1,
        color = "black",
        weight = 2,
        popup = "Haiti (excluding water bodies)",
        group = "Haiti Boundary"
    )

# Add ensemble suitability layers
interactive_suitability_map <- interactive_suitability_map %>%
    
    # Ensemble Mean
    addRasterImage(
        Aeae_ensemble_mean,
        colors = suitability_palette,
        opacity = 0.8,
        group = "Ensemble Mean"
    ) %>%
    
    # Ensemble Median
    addRasterImage(
        Aeae_ensemble_median,
        colors = suitability_palette,
        opacity = 0.8,
        group = "Ensemble Median"
    ) %>%
    
    # Confidence Intervals
    addRasterImage(
        Aeae_ensemble_ci_lower,
        colors = suitability_palette,
        opacity = 0.8,
        group = "95% CI Lower Bound"
    ) %>%
    
    addRasterImage(
        Aeae_ensemble_ci_upper,
        colors = suitability_palette,
        opacity = 0.8,
        group = "95% CI Upper Bound"
    )

# Add individual model predictions
cat("Adding individual model layers to suitability map...\n")
for (i in 1:length(Aeae_individual_predictions)) {
    # Update progress
    if (i %% 5 == 0) {
        cat("  Added", i, "of", length(Aeae_individual_predictions), "individual models\n")
    }
    
    interactive_suitability_map <- interactive_suitability_map %>%
        addRasterImage(
            Aeae_individual_predictions[[i]],
            colors = suitability_palette,
            opacity = 0.8,
            group = paste("Individual Model", i)
        )
}

# Add presence points (always visible)
interactive_suitability_map <- interactive_suitability_map %>%
    addCircleMarkers(
        data = AeaePoints_sf,
        radius = 6,
        fillColor = "#702963",
        color = "white",
        weight = 2,
        opacity = 1,
        fillOpacity = 0.9,
        popup = ~paste("<b>Aedes aegypti presence</b><br>",
                       "Site ID:", row.names(AeaePoints)),
        group = "Presence Points"
    )

# Create layer group names for layer control
ensemble_suitability_layers <- c(
    "Ensemble Mean",
    "Ensemble Median", 
    "95% CI Lower Bound",
    "95% CI Upper Bound"
)

individual_layers <- paste("Individual Model", 1:length(Aeae_individual_predictions))

all_suitability_layers <- c("Haiti Boundary", "Presence Points", ensemble_suitability_layers, individual_layers)

# Add layers control with organized groups
interactive_suitability_map <- interactive_suitability_map %>%
    addLayersControl(
        baseGroups = c("OpenStreetMap"),
        overlayGroups = all_suitability_layers,
        options = layersControlOptions(collapsed = TRUE),
        position = "topleft"
    ) %>%
    
    # Show only ensemble mean and presence points initially
    hideGroup(c(ensemble_suitability_layers[-1], individual_layers)) %>%
    
    # Add single legend for habitat suitability (compact format)
    addLegend(
        position = "bottomright",
        pal = suitability_palette,
        values = values(Aeae_ensemble_mean),
        title = "Suitability",
        opacity = 0.8,
        labFormat = labelFormat(digits = 2),
        className = "info legend-small"
    )

# Add custom control instructions
interactive_suitability_map <- interactive_suitability_map %>%
    addControl(
        html = '<div style="background: white; padding: 10px; border-radius: 5px; font-size: 12px; max-width: 200px;">
                    <strong>Habitat Suitability Explorer</strong><br>
                    • <strong>Layer Control:</strong> Top-left to toggle layers<br>
                    • <strong>Ensemble layers:</strong> Statistical summaries across all models<br>
                    • <strong>Individual models:</strong> Single BRT predictions (1-30)<br>
                    • <span style="color: #702963;">●</span> <strong>Purple dots:</strong> Observed presence points<br>
                </div>',
        position = "topright"
    )

# Print summary of what was created
cat("Available layers:\n")
cat("  Ensemble suitability layers:", length(ensemble_suitability_layers), "\n")
cat("  Individual model layers:", length(individual_layers), "\n")
cat("  Reference layers: Haiti boundary, presence points\n")
cat("  Total interactive layers:", length(all_suitability_layers), "\n\n")

# Display the interactive map
interactive_suitability_map

```

```{r}
#| label: visualize-ensemble-uncertainty-interactive
#| code-summary: "Interactive leaflet map with model uncertainty metrics"
#| fig-cap: "Interactive map showing model uncertainty and prediction variability across Haiti"

# Load required libraries
library(leaflet)
library(RColorBrewer)
library(htmlwidgets)

# Create color palette for uncertainty data
uncertainty_palette <- colorNumeric(
    palette = viridis::inferno(100),
    domain = values(Aeae_ensemble_cv),
    na.color = "transparent"
)

# Create secondary palette for standard deviation (if different scaling needed)
sd_palette <- colorNumeric(
    palette = viridis::magma(100),
    domain = values(Aeae_ensemble_sd),
    na.color = "transparent"
)

# Initialize the leaflet map with multiple base layers
interactive_uncertainty_map <- leaflet() %>%
    addTiles(group = "OpenStreetMap") %>%
    addProviderTiles(providers$Esri.WorldTopoMap, group = "Topographic") %>%
    addProviderTiles(providers$OpenTopoMap, group = "OpenTopoMap") %>%
    setView(lng = -72.285, lat = 18.971, zoom = 8) %>%
    
    # Add Haiti boundary
    addPolygons(
        data = haiti_wo_lakes_reproj,
        fillColor = "lightgray",
        fillOpacity = 0.1,
        color = "black",
        weight = 2,
        popup = "Haiti (excluding water bodies)",
        group = "Haiti Boundary"
    )

# Add ensemble uncertainty layers
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    
    # Ensemble Coefficient of Variation
    addRasterImage(
        Aeae_ensemble_cv,
        colors = uncertainty_palette,
        opacity = 0.8,
        group = "Coefficient of Variation (CV)"
    ) %>%
    
    # Ensemble Standard Deviation
    addRasterImage(
        Aeae_ensemble_sd,
        colors = sd_palette,
        opacity = 0.8,
        group = "Standard Deviation"
    )

# Add presence points (always visible)
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    addCircleMarkers(
        data = AeaePoints_sf,
        radius = 6,
        fillColor = "#702963",
        color = "white",
        weight = 2,
        opacity = 1,
        fillOpacity = 0.9,
        popup = ~paste("<b>Aedes aegypti presence</b><br>",
                       "Site ID:", row.names(AeaePoints),
                       "<br>Click to see local uncertainty"),
        group = "Presence Points"
    )

# Create layer group names for layer control
uncertainty_layers <- c(
    "Coefficient of Variation (CV)",
    "Standard Deviation"
)

all_uncertainty_layers <- c("Haiti Boundary", "Presence Points", uncertainty_layers)

# Add layers control with base maps
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    addLayersControl(
        baseGroups = c("OpenStreetMap", "Topographic", "OpenTopoMap"),
        overlayGroups = all_uncertainty_layers,
        options = layersControlOptions(collapsed = TRUE),
        position = "topleft"
    ) %>%
    
    # Show CV by default
    hideGroup("Standard Deviation") %>%
    
    # Add legend for coefficient of variation (primary uncertainty metric) - smaller format
    addLegend(
        position = "bottomright",
        pal = uncertainty_palette,
        values = values(Aeae_ensemble_cv),
        title = "CV",
        opacity = 0.8,
        labFormat = labelFormat(digits = 2),
        className = "info legend-small"
    ) %>%
    
    # Add secondary legend for standard deviation - smaller format
    addLegend(
        position = "bottomleft",
        pal = sd_palette,
        values = values(Aeae_ensemble_sd),
        title = "Std Dev",
        opacity = 0.8,
        labFormat = labelFormat(digits = 2),
        className = "info legend-small"
    )

# Calculate uncertainty statistics for the information panel
cv_values <- values(Aeae_ensemble_cv)
cv_clean <- cv_values[!is.na(cv_values)]
sd_values <- values(Aeae_ensemble_sd)
sd_clean <- sd_values[!is.na(sd_values)]

mean_cv <- round(mean(cv_clean), 3)
max_cv <- round(max(cv_clean), 3)
mean_sd <- round(mean(sd_clean), 3)
max_sd <- round(max(sd_clean), 3)

# Add custom control instructions (compact version)
interactive_uncertainty_map <- interactive_uncertainty_map %>%
    addControl(
        html = paste0('<div style="background: rgba(255,255,255,0.9); padding: 6px; border-radius: 3px; font-size: 9px; max-width: 140px; font-family: Arial, sans-serif;">
                    <strong>Uncertainty</strong><br>
                    Dark = Models agree<br>
                    Bright = Models disagree<br>
                    CV: ', mean_cv, ' avg, ', max_cv, ' max<br>
                    SD: ', mean_sd, ' avg, ', max_sd, ' max
                </div>'),
        position = "topright"
    )

# Print summary of what was created
cat("Uncertainty metrics available:\n")
cat("  Coefficient of Variation - relative uncertainty measure\n")
cat("  Standard Deviation - absolute model disagreement\n")
cat("Summary statistics:\n")
cat("  Mean CV:", mean_cv, "| Max CV:", max_cv, "\n")
cat("  Mean SD:", mean_sd, "| Max SD:", max_sd, "\n")
cat("Total interactive layers:", length(all_uncertainty_layers), "\n\n")


interactive_uncertainty_map
```

```{r}
#| label: acceptable-models-inventory-windows-path
#| code-summary: "Identify acceptable CMIP6 models and inventory downloaded data"
#| echo: false
#| include: false

library(tidyverse)
library(stringr)

# Define the acceptable models (moderate sensitivity, not too hot/cold)
acceptable_models <- c(
    'ACCESS-CM2',
    'ACCESS-ESM1-5',
    'AWI-CM-1-1-MR',
    'BCC-CSM2-MR',
    'BCC-ESM1',
    'CAMS-CSM1-0',
    'CAS-ESM2-0',
    'CESM2',
    'CESM2-FV2',
    'CESM2-WACCM',
    'CESM2-WACCM-FV2',
    'CMCC-CM2-SR5',
    'CNRM-CM6-1',
    'CNRM-ESM2-1',
    'FGOALS-f3-L',
    'FGOALS-g3',
    'GFDL-CM4',
    'GFDL-ESM4',
    'GISS-E2-1-G',
    'GISS-E2-1-H',
    'GISS-E2-2-G',
    'IITM-ESM',
    'KACE-1-0-G',
    'MCM-UA-1-0',
    'MIROC-ES2L',
    'MIROC6',
    'MPI-ESM-1-2-HAM',
    'MPI-ESM1-2-HR',
    'MPI-ESM1-2-LR',
    'MRI-ESM2-0',
    'NorCPM1',
    'NorESM2-LM'
)

# Inventory your downloaded Haiti CMIP6 data (Windows path)
cmip6_data_dir <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/cmip6_haiti_python"

# Check if directory exists
if (!dir.exists(cmip6_data_dir)) {
    stop("CMIP6 data directory not found: ", cmip6_data_dir)
}

# Get all downloaded files
downloaded_files <- list.files(
    cmip6_data_dir, 
    pattern = "\\.tif$",  # More permissive pattern initially
    full.names = TRUE
)

cat("Found", length(downloaded_files), "total .tif files\n")

# Parse filenames to extract model, SSP, and period information
file_inventory <- purrr::map_dfr(downloaded_files, function(filepath) {
    filename <- basename(filepath)
    
    # Try multiple potential filename patterns
    # Pattern 1: haiti_wc2.1_30s_bioc_{MODEL}_{SSP}_{PERIOD}.tif
    parts <- stringr::str_match(
        filename, 
        "haiti_wc2\\.1_30s_bioc_(.+?)_(ssp\\d+)_(\\d{4}-\\d{4})\\.tif"
    )
    
    # Pattern 2: wc2.1_30s_bioc_{MODEL}_{SSP}_{PERIOD}.tif (without "haiti_" prefix)
    if (is.na(parts[1])) {
        parts <- stringr::str_match(
            filename,
            "wc2\\.1_30s_bioc_(.+?)_(ssp\\d+)_(\\d{4}-\\d{4})\\.tif"
        )
    }
    
    # Pattern 3: More flexible pattern for model names with hyphens
    if (is.na(parts[1])) {
        parts <- stringr::str_match(
            filename,
            "(?:haiti_)?wc2\\.1_30s_bioc_([A-Za-z0-9-]+)_(ssp\\d+)_(\\d{4}-\\d{4})\\.tif"
        )
    }
    
    # Return tibble only if we successfully parsed the filename
    if (!is.na(parts[1]) && length(parts) >= 4) {
        # Extract period start and end years
        period_years <- stringr::str_split(parts[4], "-")[[1]]
        
        tibble::tibble(
            filepath = filepath,
            filename = filename,
            model = parts[2],
            ssp = parts[3],
            time_period = parts[4],
            period_start = as.integer(period_years[1]),
            period_end = as.integer(period_years[2]),
            file_exists = file.exists(filepath),
            file_size_mb = file.info(filepath)$size / 1024^2
        )
    } else {
        # Return NULL for failed matches - will be removed by map_dfr
        NULL
    }
})

# Check parsing results
cat("\nFilename parsing results:\n")
cat("  Successfully parsed:", nrow(file_inventory), "files\n")
cat("  Failed to parse:", length(downloaded_files) - nrow(file_inventory), "files\n")

# Show examples of unparsed files if any
if (nrow(file_inventory) < length(downloaded_files)) {
    unparsed_files <- setdiff(
        basename(downloaded_files), 
        file_inventory$filename
    )
    cat("\nExamples of unparsed filenames:\n")
    cat("  ", head(unparsed_files, 5), sep = "\n  ")
}

# Filter to only acceptable models
acceptable_inventory <- file_inventory %>%
    dplyr::filter(model %in% acceptable_models) %>%
    dplyr::arrange(model, ssp, period_start)

# Summary statistics
cat("\n=== CMIP6 Data Inventory ===\n")
cat("Total files found:", length(downloaded_files), "\n")
cat("Files successfully parsed:", nrow(file_inventory), "\n")
cat("Files from acceptable models:", nrow(acceptable_inventory), "\n")
cat("Unique models in parsed files:", 
    length(unique(file_inventory$model)), "\n")
cat("Unique acceptable models found:", 
    length(unique(acceptable_inventory$model)), "\n\n")

# Show which acceptable models we found
found_acceptable_models <- sort(unique(acceptable_inventory$model))
cat("Acceptable models found:\n")
cat("  ", found_acceptable_models, sep = "\n  ")
cat("\n")

# Check for missing models
missing_models <- setdiff(acceptable_models, found_acceptable_models)
if (length(missing_models) > 0) {
    cat("\nAcceptable models NOT found in data:\n")
    cat("  ", missing_models, sep = "\n  ")
    cat("\n")
}

# Show SSPs and time periods
if (nrow(acceptable_inventory) > 0) {
    cat("SSPs represented:", 
        paste(sort(unique(acceptable_inventory$ssp)), collapse = ", "), "\n")
    cat("Time periods:", 
        paste(sort(unique(acceptable_inventory$time_period)), collapse = ", "), "\n\n")
    
    # Summary by model
    model_summary <- acceptable_inventory %>%
        dplyr::group_by(model) %>%
        dplyr::summarise(
            n_files = n(),
            n_ssps = n_distinct(ssp),
            n_periods = n_distinct(time_period),
            total_size_gb = sum(file_size_mb) / 1024,
            .groups = "drop"
        ) %>%
        dplyr::arrange(desc(n_files))
    
    cat("Files per acceptable model:\n")
    print(model_summary, n = Inf)
    
    # Check completeness - should have 4 SSPs × 4 periods = 16 files per model
    complete_models <- model_summary %>%
        dplyr::filter(n_files == 16)
    
    cat("\nComplete models (16 files each):", nrow(complete_models), "of", nrow(model_summary), "\n")
    
    if (nrow(model_summary) > nrow(complete_models)) {
        incomplete_models <- model_summary %>%
            dplyr::filter(n_files < 16)
        
        cat("\nIncomplete models:\n")
        print(incomplete_models, n = Inf)
    }
}

# Store for later use
assign("acceptable_cmip6_inventory", acceptable_inventory, envir = .GlobalEnv)
assign("all_cmip6_inventory", file_inventory, envir = .GlobalEnv)

# Diagnostic: show all unique models found (even non-acceptable ones)
cat("\n=== All Models Found (for diagnostic purposes) ===\n")
all_models_found <- sort(unique(file_inventory$model))
cat("  ", all_models_found, sep = "\n  ")
cat("\nTotal unique models:", length(all_models_found), "\n")

# Additional diagnostic: check for any files with unexpected patterns
cat("\n=== File Pattern Analysis ===\n")
if (nrow(file_inventory) > 0) {
    cat("Example filenames (first 5):\n")
    cat("  ", head(file_inventory$filename, 5), sep = "\n  ")
}
```

```{r}
#| label: load-warming-levels-from-github
#| code-summary: "Load CMIP6 warming levels data from custom GitHub repository"
#| echo: false
#| include: false


library(readr)
library(dplyr)
library(tidyr)

# Load warming levels data from your GitHub repository
warming_levels_url <- paste0(
    "https://raw.githubusercontent.com/IanPsheaSmith/",
    "HaitiAeaeENM/main/WarmingLevelsList/",
    "cmip6_warming_levels_Hauser2022.csv"
)

cat("Loading warming levels data from GitHub...\n")
cat("URL:", warming_levels_url, "\n\n")

warming_levels_raw <- tryCatch({
    readr::read_csv(warming_levels_url, show_col_types = FALSE)
}, error = function(e) {
    cat("Failed to download from GitHub:\n")
    cat("  Error:", e$message, "\n")
    return(NULL)
})

if (!is.null(warming_levels_raw)) {
    
    cat("✓ Successfully loaded warming levels data\n")
    cat("Dimensions:", nrow(warming_levels_raw), "rows ×", 
        ncol(warming_levels_raw), "columns\n\n")
    
    # Display structure
    cat("Column names:\n")
    cat("  ", names(warming_levels_raw), sep = "\n  ")
    cat("\n")
    
    cat("First few rows:\n")
    print(head(warming_levels_raw, 5))
    cat("\n")
    
    # Process for our needs
    warming_levels_processed <- warming_levels_raw %>%
        # Filter to only acceptable models and SSPs we have data for
        dplyr::filter(
            model %in% acceptable_models,
            exp %in% c("ssp126", "ssp245", "ssp370", "ssp585")
        ) %>%
        # Calculate period midpoint for matching with our data
        dplyr::mutate(
            period_midpoint = (start_year + end_year) / 2,
            period_range = paste0(start_year, "-", end_year)
        ) %>%
        dplyr::select(
            model, 
            ensemble,
            exp, 
            warming_level, 
            start_year, 
            end_year,
            period_midpoint,
            period_range
        )
    
    cat("=== Warming Levels Data Summary ===\n")
    cat("Total rows after filtering:", nrow(warming_levels_processed), "\n")
    cat("Acceptable models found:", 
        length(unique(warming_levels_processed$model)), "of 32\n")
    
    # Check which acceptable models we have
    found_models <- unique(warming_levels_processed$model)
    cat("\nAcceptable models with warming level data:\n")
    cat("  ", sort(found_models), sep = "\n  ")
    
    missing_models <- setdiff(acceptable_models, found_models)
    if (length(missing_models) > 0) {
        cat("\n\nAcceptable models WITHOUT warming level data:\n")
        cat("  ", missing_models, sep = "\n  ")
    }
    
    cat("\n\nSSPs available:", 
        paste(sort(unique(warming_levels_processed$exp)), collapse = ", "), "\n")
    cat("Warming levels available:", 
        paste(sort(unique(warming_levels_processed$warming_level)), 
              collapse = "°C, "), "°C\n\n")
    
    # Focus on key warming levels: 1.5, 2.0, 3.0, 4.0
    key_warming_levels <- c(1.5, 2.0, 3.0, 4.0)
    
    warming_levels_key <- warming_levels_processed %>%
        dplyr::filter(warming_level %in% key_warming_levels)
    
    cat("Focusing on key warming levels (1.5, 2.0, 3.0, 4.0°C):\n")
    cat("Total rows:", nrow(warming_levels_key), "\n\n")
    
    # Summary: when each warming level is reached
    wl_timing_summary <- warming_levels_key %>%
        dplyr::group_by(warming_level, exp) %>%
        dplyr::summarise(
            n_models = n_distinct(model),
            earliest_midpoint = min(period_midpoint),
            latest_midpoint = max(period_midpoint),
            median_midpoint = median(period_midpoint),
            .groups = "drop"
        ) %>%
        dplyr::arrange(warming_level, exp)
    
    cat("Timing of warming levels by SSP:\n")
    print(wl_timing_summary, n = Inf)
    
    # Check coverage for each acceptable model
    cat("\n\nCoverage check (models × warming levels × SSPs):\n")
    coverage_summary <- warming_levels_key %>%
        dplyr::group_by(model) %>%
        dplyr::summarise(
            n_wl_ssp_combinations = n(),
            n_warming_levels = n_distinct(warming_level),
            n_ssps = n_distinct(exp),
            warming_levels = paste(sort(unique(warming_level)), collapse = ", "),
            .groups = "drop"
        ) %>%
        dplyr::arrange(desc(n_wl_ssp_combinations))
    
    print(coverage_summary, n = Inf)
    
    # Expected: 4 warming levels × 4 SSPs = 16 combinations per model
    cat("\n")
    complete_models <- coverage_summary %>%
        dplyr::filter(n_wl_ssp_combinations == 16)
    
    cat("Models with complete coverage (16 combinations):", 
        nrow(complete_models), "of", nrow(coverage_summary), "\n")
    
    # Store in global environment
    assign("cmip6_warming_levels", warming_levels_key, envir = .GlobalEnv)
    assign("cmip6_warming_levels_all", warming_levels_processed, envir = .GlobalEnv)
    assign("cmip6_wl_timing_summary", wl_timing_summary, envir = .GlobalEnv)
    assign("cmip6_wl_coverage", coverage_summary, envir = .GlobalEnv)
    
} else {
    cat("\n Failed to load warming levels data\n")
}
```

```{r}
#| label: map-files-to-warming-levels
#| code-summary: "Match downloaded CMIP6 files to appropriate warming levels"
#| include: false
#| echo: false

library(dplyr)
library(tidyr)

# The downloaded files have these time periods:
# 2021-2040, 2041-2060, 2061-2080, 2081-2100

# Calculate midpoints for the downloaded periods
file_periods <- acceptable_cmip6_inventory %>%
    dplyr::mutate(
        period_midpoint = (period_start + period_end) / 2
    )

cat("Downloaded file periods:\n")
period_summary <- file_periods %>%
    dplyr::distinct(time_period, period_start, period_end, period_midpoint)
print(period_summary)
cat("\n")

# Function to find the best warming level match for each file
find_best_warming_level <- function(model, ssp, file_midpoint, wl_data, 
                                     max_year_difference = 15) {
    
    # Get all warming levels for this model/SSP combination
    model_wl <- wl_data %>%
        dplyr::filter(
            model == !!model,
            exp == !!ssp
        )
    
    if (nrow(model_wl) == 0) {
        return(tibble::tibble(
            warming_level = NA_real_,
            wl_period_midpoint = NA_real_,
            wl_start_year = NA_integer_,
            wl_end_year = NA_integer_,
            year_difference = NA_real_,
            match_quality = "no_data"
        ))
    }
    
    # Calculate distance between file midpoint and each warming level midpoint
    model_wl <- model_wl %>%
        dplyr::mutate(
            year_difference = abs(period_midpoint - !!file_midpoint)
        ) %>%
        dplyr::arrange(year_difference)
    
    # Get the closest match
    best_match <- model_wl %>%
        dplyr::slice(1) %>%
        dplyr::select(
            warming_level,
            wl_period_midpoint = period_midpoint,
            wl_start_year = start_year,
            wl_end_year = end_year,
            year_difference
        ) %>%
        dplyr::mutate(
            match_quality = dplyr::case_when(
                year_difference <= 5 ~ "excellent",
                year_difference <= 10 ~ "good",
                year_difference <= 15 ~ "fair",
                TRUE ~ "poor"
            )
        )
    
    return(best_match)
}

# Apply matching for all files
cat("Matching files to warming levels...\n")

files_with_wl <- file_periods %>%
    dplyr::rowwise() %>%
    dplyr::mutate(
        wl_match = list(find_best_warming_level(
            model, ssp, period_midpoint, cmip6_warming_levels
        ))
    ) %>%
    dplyr::ungroup() %>%
    tidyr::unnest(wl_match)

cat("✓ Matching complete\n\n")

# Summary of matches
cat("=== Matching Summary ===\n")

match_quality_summary <- files_with_wl %>%
    dplyr::group_by(match_quality) %>%
    dplyr::summarise(
        n_files = n(),
        pct_files = round(n() / nrow(files_with_wl) * 100, 1),
        .groups = "drop"
    ) %>%
    dplyr::arrange(desc(n_files))

print(match_quality_summary)
cat("\n")

# Files that couldn't be matched
unmatched_files <- files_with_wl %>%
    dplyr::filter(is.na(warming_level))

if (nrow(unmatched_files) > 0) {
    cat("Files without warming level matches:", nrow(unmatched_files), "\n")
    cat("Breakdown by model/SSP:\n")
    unmatched_summary <- unmatched_files %>%
        dplyr::group_by(model, ssp) %>%
        dplyr::summarise(
            n_files = n(),
            periods = paste(sort(unique(time_period)), collapse = ", "),
            .groups = "drop"
        )
    print(unmatched_summary, n = Inf)
    cat("\n")
}

# Successfully matched files
matched_files <- files_with_wl %>%
    dplyr::filter(!is.na(warming_level))

cat("Successfully matched files:", nrow(matched_files), "of", nrow(files_with_wl), "\n\n")

# Summary by warming level
wl_file_summary <- matched_files %>%
    dplyr::group_by(warming_level) %>%
    dplyr::summarise(
        n_files = n(),
        n_models = n_distinct(model),
        n_ssps = n_distinct(ssp),
        mean_year_diff = round(mean(year_difference, na.rm = TRUE), 1),
        median_year_diff = round(median(year_difference, na.rm = TRUE), 1),
        .groups = "drop"
    ) %>%
    dplyr::arrange(warming_level)

cat("Files available for each warming level:\n")
print(wl_file_summary, n = Inf)
cat("\n")

# Detailed breakdown by warming level and SSP
wl_ssp_breakdown <- matched_files %>%
    dplyr::group_by(warming_level, ssp) %>%
    dplyr::summarise(
        n_models = n_distinct(model),
        n_files = n(),
        models = paste(sort(unique(model)), collapse = ", "),
        .groups = "drop"
    ) %>%
    dplyr::arrange(warming_level, ssp)

cat("Breakdown by warming level and SSP:\n")
print(wl_ssp_breakdown, n = Inf)
cat("\n")

# Check if we have sufficient coverage for ensemble creation
sufficient_coverage <- wl_file_summary %>%
    dplyr::filter(n_models >= 5)

# Store results
assign("cmip6_files_with_wl", files_with_wl, envir = .GlobalEnv)
assign("cmip6_matched_files", matched_files, envir = .GlobalEnv)
assign("cmip6_wl_file_summary", wl_file_summary, envir = .GlobalEnv)

cat("  File-to-warming-level mapping complete\n")
cat("  All files with matches: cmip6_files_with_wl\n")
cat("  Successfully matched files: cmip6_matched_files\n")
cat("  Summary by warming level: cmip6_wl_file_summary\n\n")

cat("=== Recommendation ===\n")
if (nrow(sufficient_coverage) >= 3) {
    cat("Sufficient data for", nrow(sufficient_coverage), 
        "warming levels\n")
    cat("  Warming levels ready for ensemble creation:\n")
    cat("    ", paste(sufficient_coverage$warming_level, "°C", sep = ""), 
        sep = "\n    ")
} else {
    cat("Limited coverage - only", nrow(sufficient_coverage), 
        "warming levels have ≥5 models\n")
}
```

```{r}
#| label: create-supplementary-table-warming-levels
#| code-summary: "Export warming level assignments to supplementary table"

library(dplyr)
library(readr)

# Define output directory
output_dir <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Figures"

# Verify directory exists
if (!dir.exists(output_dir)) {
    cat("Output directory does not exist. Creating it...\n")
    dir.create(output_dir, recursive = TRUE)
}

cat("Output directory:", output_dir, "\n\n")

# ============================================================================
# COMPREHENSIVE VERSION (for reviewers/supplementary material)
# ============================================================================

supplementary_table_comprehensive <- cmip6_matched_files %>%
    dplyr::mutate(
        # Construct the WL reference period from start/end years
        WL_Reference_Period = paste0(wl_start_year, "-", wl_end_year)
    ) %>%
    dplyr::select(
        Model = model,
        SSP = ssp,
        Time_Period = time_period,
        Period_Start_Year = period_start,
        Period_End_Year = period_end,
        Period_Midpoint = period_midpoint,
        Assigned_Warming_Level_C = warming_level,
        WL_Reference_Period,
        WL_Reference_Start = wl_start_year,
        WL_Reference_End = wl_end_year,
        WL_Reference_Midpoint = wl_period_midpoint,
        Year_Difference = year_difference,
        Match_Quality = match_quality
    ) %>%
    # Add descriptive columns
    dplyr::mutate(
        SSP_Description = dplyr::case_when(
            SSP == "ssp126" ~ "Low emissions (SSP1-2.6)",
            SSP == "ssp245" ~ "Intermediate emissions (SSP2-4.5)",
            SSP == "ssp370" ~ "High emissions (SSP3-7.0)",
            SSP == "ssp585" ~ "Very high emissions (SSP5-8.5)",
            TRUE ~ SSP
        ),
        Warming_Level_Description = paste0(
            Assigned_Warming_Level_C, "°C above pre-industrial baseline"
        ),
        Match_Quality_Description = dplyr::case_when(
            Match_Quality == "excellent" ~ "Excellent (≤5 years difference)",
            Match_Quality == "good" ~ "Good (6-10 years difference)",
            Match_Quality == "fair" ~ "Fair (11-15 years difference)",
            Match_Quality == "poor" ~ "Poor (>15 years difference)",
            TRUE ~ Match_Quality
        )
    ) %>%
    # Sort by warming level, SSP, then model
    dplyr::arrange(
        Assigned_Warming_Level_C,
        SSP,
        Model,
        Period_Start_Year
    )

# Save comprehensive version
comprehensive_file <- file.path(
    output_dir, 
    "Supplementary_Table_X_CMIP6_Warming_Level_Assignments_Comprehensive.csv"
)

readr::write_csv(supplementary_table_comprehensive, comprehensive_file)

cat("✓ Comprehensive table saved:\n")
cat("  ", comprehensive_file, "\n\n")

# ============================================================================
# SIMPLIFIED VERSION (more readable for main supplement)
# ============================================================================

supplementary_table_simple <- cmip6_matched_files %>%
    dplyr::mutate(
        # Construct the WL reference period from start/end years
        WL_Reference_Period = paste0(wl_start_year, "-", wl_end_year)
    ) %>%
    dplyr::select(
        Model = model,
        `Emission Scenario` = ssp,
        `Data Time Period` = time_period,
        `Warming Level (°C)` = warming_level,
        `WL Reference Period` = WL_Reference_Period,
        `Year Difference` = year_difference,
        `Match Quality` = match_quality
    ) %>%
    dplyr::mutate(
        `Emission Scenario` = dplyr::case_when(
            `Emission Scenario` == "ssp126" ~ "SSP1-2.6",
            `Emission Scenario` == "ssp245" ~ "SSP2-4.5",
            `Emission Scenario` == "ssp370" ~ "SSP3-7.0",
            `Emission Scenario` == "ssp585" ~ "SSP5-8.5",
            TRUE ~ `Emission Scenario`
        )
    ) %>%
    dplyr::arrange(
        `Warming Level (°C)`,
        `Emission Scenario`,
        Model
    )

# Save simplified version
simple_file <- file.path(
    output_dir,
    "Supplementary_Table_X_CMIP6_Warming_Level_Assignments.csv"
)

readr::write_csv(supplementary_table_simple, simple_file)

cat("✓ Simplified table saved:\n")
cat("  ", simple_file, "\n\n")

# ============================================================================
# SUMMARY STATISTICS TABLE (for text reference)
# ============================================================================

summary_stats_table <- cmip6_matched_files %>%
    dplyr::group_by(
        Warming_Level = warming_level
    ) %>%
    dplyr::summarise(
        N_Model_Scenario_Combinations = n(),
        N_Unique_Models = n_distinct(model),
        N_Scenarios = n_distinct(ssp),
        Models_Included = paste(sort(unique(model)), collapse = "; "),
        Scenarios_Included = paste(sort(unique(ssp)), collapse = ", "),
        Mean_Year_Difference = round(mean(year_difference), 1),
        Median_Year_Difference = round(median(year_difference), 1),
        Match_Quality_Range = paste(
            sort(unique(match_quality)), 
            collapse = ", "
        ),
        .groups = "drop"
    ) %>%
    dplyr::arrange(Warming_Level)

# Save summary stats
summary_file <- file.path(
    output_dir,
    "Supplementary_Table_Summary_Stats_by_Warming_Level.csv"
)

readr::write_csv(summary_stats_table, summary_file)

cat("✓ Summary statistics table saved:\n")
cat("  ", summary_file, "\n\n")

# ============================================================================
# MODEL INVENTORY TABLE (which models are in each warming level)
# ============================================================================

model_inventory <- cmip6_matched_files %>%
    dplyr::group_by(Model = model) %>%
    dplyr::summarise(
        N_Warming_Levels = n_distinct(warming_level),
        Warming_Levels_Included = paste(
            sort(unique(warming_level)), 
            collapse = ", "
        ),
        N_Scenario_Period_Combinations = n(),
        SSPs_Included = paste(
            sort(unique(ssp)), 
            collapse = ", "
        ),
        Time_Periods_Included = paste(
            sort(unique(time_period)), 
            collapse = "; "
        ),
        .groups = "drop"
    ) %>%
    dplyr::arrange(Model)

# Save model inventory
inventory_file <- file.path(
    output_dir,
    "Supplementary_Table_Model_Inventory.csv"
)

readr::write_csv(model_inventory, inventory_file)

cat("✓ Model inventory table saved:\n")
cat("  ", inventory_file, "\n\n")

# ============================================================================
# DISPLAY SUMMARY
# ============================================================================

cat("=== Tables Created ===\n\n")

cat("1. COMPREHENSIVE TABLE (for detailed review):\n")
cat("   Rows:", nrow(supplementary_table_comprehensive), "\n")
cat("   Columns:", ncol(supplementary_table_comprehensive), "\n")
cat("   File:", basename(comprehensive_file), "\n\n")

cat("2. SIMPLIFIED TABLE (recommended for main supplement):\n")
cat("   Rows:", nrow(supplementary_table_simple), "\n")
cat("   Columns:", ncol(supplementary_table_simple), "\n")
cat("   File:", basename(simple_file), "\n\n")

cat("3. SUMMARY STATISTICS (for manuscript reference):\n")
cat("   Warming levels:", nrow(summary_stats_table), "\n")
cat("   File:", basename(summary_file), "\n\n")

cat("4. MODEL INVENTORY (for methods section):\n")
cat("   Models:", nrow(model_inventory), "\n")
cat("   File:", basename(inventory_file), "\n\n")

# Display preview of simplified table
cat("=== Preview of Simplified Table (first 15 rows) ===\n\n")
print(head(supplementary_table_simple, 15))

cat("\n=== Preview of Summary Statistics ===\n\n")
print(summary_stats_table, n = Inf)

cat("\n=== Preview of Model Inventory ===\n\n")
print(model_inventory, n = Inf)

cat("\n✓ All supplementary tables successfully created!\n")
cat("Location:", output_dir, "\n")
```

```{r}
#| label: create-warming-level-ensembles
#| code-summary: "Generate ensemble climate rasters for each warming level"
#| echo: false
#| include: false


library(raster)
library(dplyr)
library(purrr)


# Function to create ensemble for a specific warming level
create_wl_ensemble <- function(target_wl, matched_files_data, 
                                max_year_diff = 15,
                                match_quality_threshold = "fair") {
    
    cat("Processing warming level:", target_wl, "°C\n")
    
    # Quality thresholds mapping
    quality_levels <- c("excellent", "good", "fair", "poor")
    quality_cutoff <- which(quality_levels == match_quality_threshold)
    acceptable_quality <- quality_levels[1:quality_cutoff]
    
    # Filter files for this warming level with quality control
    wl_files <- matched_files_data %>%
        dplyr::filter(
            warming_level == target_wl,
            year_difference <= max_year_diff,
            match_quality %in% acceptable_quality
        )
    
    if (nrow(wl_files) == 0) {
        cat("  ✗ No files found for", target_wl, "°C\n\n")
        return(NULL)
    }
    
    cat("  Models available:", n_distinct(wl_files$model), "\n")
    cat("  Total files:", nrow(wl_files), "\n")
    cat("  SSPs represented:", paste(sort(unique(wl_files$ssp)), collapse = ", "), "\n")
    cat("  Match quality:", paste(sort(unique(wl_files$match_quality)), collapse = ", "), "\n")
    cat("  Mean year difference:", round(mean(wl_files$year_difference), 1), "years\n")
    
    # Load all rasters for this warming level
    cat("  Loading", nrow(wl_files), "raster files...\n")
    
    raster_list <- list()
    reference_raster <- NULL
    
    for (i in seq_len(nrow(wl_files))) {
        file_info <- wl_files[i, ]
        
        tryCatch({
            # Check if file exists and is valid
            if (!file.exists(file_info$filepath)) {
                cat("    ✗ File not found:", basename(file_info$filepath), "\n")
                next
            }
            
            # Load raster brick (19 bioclim variables)
            r <- raster::brick(file_info$filepath)
            
            # Check if raster loaded properly
            if (is.null(r) || nlayers(r) == 0) {
                cat("    ✗ Invalid raster:", basename(file_info$filepath), "\n")
                next
            }
            
            # Set reference raster from first successful load
            if (is.null(reference_raster)) {
                reference_raster <- r[[1]]  # Use first layer as template
                cat("    ✓ Set reference extent from:", basename(file_info$filepath), "\n")
            }
            
            # Resample to match reference if needed
            if (!compareRaster(r, reference_raster, extent = TRUE, rowcol = TRUE, crs = TRUE, stopiffalse = FALSE)) {
                cat("    → Resampling to match reference:", basename(file_info$filepath), "\n")
                r <- raster::resample(r, reference_raster, method = "bilinear")
            }
            
            # Track metadata
            attr(r, "model") <- file_info$model
            attr(r, "ssp") <- file_info$ssp
            attr(r, "period") <- file_info$time_period
            
            raster_list[[length(raster_list) + 1]] <- r
            
        }, error = function(e) {
            cat("    ✗ Failed to load:", basename(file_info$filepath), "-", e$message, "\n")
        })
    }
    
    if (length(raster_list) == 0) {
        cat("  ✗ Failed to load any valid rasters\n\n")
        return(NULL)
    }
    
    cat("  Successfully loaded:", length(raster_list), "rasters\n")
    
    # Create ensemble statistics for each bioclim variable
    cat("  Creating ensemble statistics across 19 bioclim variables...\n")
    
    # Get variable names from first raster
    var_names <- names(raster_list[[1]])
    n_vars <- length(var_names)
    
    # Initialize output lists
    ensemble_means <- list()
    ensemble_sds <- list()
    ensemble_medians <- list()
    ensemble_cvs <- list()
    
    # Process each bioclim variable separately
    for (v in seq_len(n_vars)) {
        var_name <- var_names[v]
        
        # Extract this variable from all models
        var_layers <- purrr::map(raster_list, ~ .x[[v]])
        
        # Stack layers for this variable
        var_stack <- raster::stack(var_layers)
        
        # Calculate statistics
        var_mean <- calc(var_stack, fun = mean, na.rm = TRUE)
        var_sd <- calc(var_stack, fun = sd, na.rm = TRUE)
        var_median <- calc(var_stack, fun = median, na.rm = TRUE)
        
        # Calculate coefficient of variation (handle division by zero)
        var_cv <- overlay(var_sd, var_mean, fun = function(sd, mean) {
            ifelse(abs(mean) > 1e-10, sd / abs(mean), NA)
        })
        
        # Name the layers
        names(var_mean) <- var_name
        names(var_sd) <- paste0(var_name, "_sd")
        names(var_median) <- paste0(var_name, "_median")
        names(var_cv) <- paste0(var_name, "_cv")
        
        # Store in lists
        ensemble_means[[v]] <- var_mean
        ensemble_sds[[v]] <- var_sd
        ensemble_medians[[v]] <- var_median
        ensemble_cvs[[v]] <- var_cv
    }
    
    # Stack all variables into final raster stacks
    ensemble_mean <- raster::stack(ensemble_means)
    ensemble_sd <- raster::stack(ensemble_sds)
    ensemble_median <- raster::stack(ensemble_medians)
    ensemble_cv <- raster::stack(ensemble_cvs)
    
    cat("  Ensemble creation complete\n\n")
    
    # Return comprehensive result
    return(list(
        warming_level = target_wl,
        mean = ensemble_mean,
        sd = ensemble_sd,
        median = ensemble_median,
        cv = ensemble_cv,
        n_models = n_distinct(wl_files$model),
        n_files = length(raster_list),
        models_used = sort(unique(sapply(raster_list, function(x) attr(x, "model")))),
        ssps_used = sort(unique(sapply(raster_list, function(x) attr(x, "ssp")))),
        file_metadata = wl_files %>%
            dplyr::select(model, ssp, time_period, warming_level, 
                         year_difference, match_quality)
    ))
}

# Generate ensembles for all 4 warming levels
warming_levels_to_process <- c(1.5, 2.0, 3.0, 4.0)

cat("Creating ensembles for warming levels:", 
    paste(warming_levels_to_process, "°C", collapse = ", "), "\n\n")
cat(paste(rep("=", 60), collapse = ""), "\n\n")

wl_ensembles <- purrr::map(
    warming_levels_to_process,
    ~ create_wl_ensemble(
        target_wl = .x,
        matched_files_data = cmip6_matched_files,
        max_year_diff = 15,
        match_quality_threshold = "fair"
    )
)

# Name the list elements
names(wl_ensembles) <- paste0("wl_", warming_levels_to_process, "C")

# Remove any NULL results
wl_ensembles <- purrr::compact(wl_ensembles)

# Store results
assign("cmip6_wl_ensembles", wl_ensembles, envir = .GlobalEnv)

cat("  Warming level ensemble creation complete\n")
cat("  Available ensembles:", length(wl_ensembles), "warming levels\n")
cat("  Stored as: cmip6_wl_ensembles\n")

# Summary
if (length(wl_ensembles) > 0) {
    cat("\nEnsemble Summary:\n")
    for (i in seq_along(wl_ensembles)) {
        ens <- wl_ensembles[[i]]
        cat("  ", ens$warming_level, "°C:", ens$n_files, "files from", 
            ens$n_models, "models\n")
    }
}
```

```{r}
#| label: enforce-100pct-coverage-and-predict
#| code-summary: "Require 100% variable coverage and run predictions with corrected names"
#| echo: false
#| include: false

library(raster)
library(dplyr)
library(purrr)
library(sf)

# 1) Rename ensemble covariates to exactly match BRT expectations (including WarmQurt/WetQurt)
create_complete_brt_mapping <- function() {
  c(
    "1" = "AnnMeanTemp",
    "2" = "MeanDiurnRange",
    "3" = "Isothermality",
    "4" = "TempSeasonality",
    "5" = "MaxTempWarmMnth",
    "6" = "MinTempColdMnth",
    "7" = "TempAnnRnge",
    "8" = "MTempWetQurt",
    "9" = "MTempDryQurt",
    "10" = "MTempWarmQurt",  # WarmQurt (exactly as in BRTs)
    "11" = "MTempCldQurt",
    "12" = "AnnPrcp",
    "13" = "PrcpWetMnth",
    "14" = "PrcpDryMnth",
    "15" = "PrcpSeasonality",
    "16" = "PrcpWetQurt",    # WetQurt (exactly as in BRTs)
    "17" = "PrcpDryQrt",
    "18" = "PrcpWrmQrt",
    "19" = "PrcpCldQrt"
  )
}

rename_to_brt_expectations <- function(wl_raster) {
  current_names <- names(wl_raster)
  brt_map <- create_complete_brt_mapping()
  bio_numbers <- gsub(".*_([0-9]+)$", "\\1", current_names)
  new_names <- brt_map[bio_numbers]
  new_names[is.na(new_names)] <- current_names[is.na(new_names)]
  names(wl_raster) <- new_names
  wl_raster
}

# 2) Recompute model coverage correctly (avoid the earlier percent bug and name collisions)
compute_coverage <- function(brt_models, wl_vars) {
  purrr::map_dfr(seq_along(brt_models), function(i) {
    model_vars <- brt_models[[i]]$final_variables
    matching <- intersect(model_vars, wl_vars)
    missing_names <- setdiff(model_vars, wl_vars)
    tibble::tibble(
      model_id = i,
      total_vars = length(model_vars),
      matching_vars = length(matching),
      missing_vars_n = length(missing_names),
      pct_available = round((length(matching) / length(model_vars)) * 100, 1),
      missing_var_list = if (length(missing_names) == 0) "" else paste(missing_names, collapse = ", ")
    )
  })
}

# 3) Prediction function requiring 100% coverage, no robust scaling tweaks
predict_warming_level_100pct <- function(wl_ensemble, brt_models, haiti_boundary) {
  wl <- wl_ensemble$warming_level
  cat("Generating predictions for", wl, "°C (100% coverage required)...\n")

  wl_covars <- rename_to_brt_expectations(wl_ensemble$mean)
  wl_vars <- names(wl_covars)

  coverage <- compute_coverage(brt_models, wl_vars)
  # Require 100% coverage
  eligible_ids <- coverage %>% filter(pct_available == 100) %>% pull(model_id)

  cat("  Eligible BRT models with 100% coverage:", length(eligible_ids), "of", nrow(coverage), "\n")
  if (length(eligible_ids) == 0) {
    cat("  ✗ No models meet 100% coverage. Aborting.\n\n")
    return(NULL)
  }

  predictions <- purrr::map(eligible_ids, function(i) {
    mr <- brt_models[[i]]
    final_vars <- mr$final_variables
    # Preserve model variable order
    use_vars <- final_vars[final_vars %in% wl_vars]

    # Select covariates and values
    covars <- wl_covars[[use_vars]]
    vals <- getValues(covars)
    na_mask <- apply(is.na(vals), 1, any)
    valid_rows <- !na_mask
    if (sum(valid_rows) == 0) return(NULL)

    valid_vals <- vals[valid_rows, , drop = FALSE]

    # Scale with original parameters (no modifications)
    centers <- mr$scaling_params$center[use_vars]
    scales  <- mr$scaling_params$scale[use_vars]
    if (any(is.na(centers)) || any(is.na(scales))) return(NULL)

    scaled <- scale(valid_vals, center = centers, scale = scales)
    if (any(!is.finite(scaled))) return(NULL)

    pred_df <- as.data.frame(scaled)
    names(pred_df) <- use_vars

    # Predict
    preds_valid <- tryCatch(
      predict(mr$model, pred_df, n.trees = mr$optimal_trees, type = "response"),
      error = function(e) NULL
    )
    if (is.null(preds_valid) || any(!is.finite(preds_valid))) return(NULL)

    # Rebuild raster
    preds_full <- rep(NA_real_, nrow(vals))
    preds_full[valid_rows] <- preds_valid
    r <- covars[[1]]
    values(r) <- preds_full

    # Mask to Haiti
    haiti_sp <- as(st_union(haiti_boundary), "Spatial")
    if (!identical(crs(r), crs(haiti_sp))) {
      haiti_sp <- spTransform(haiti_sp, CRSobj = crs(r))
    }
    mask(crop(r, haiti_sp), haiti_sp)
  })

  predictions <- purrr::compact(predictions)
  cat("  Successfully predicted:", length(predictions), "BRT models\n")
  if (length(predictions) == 0) {
    cat("  ✗ No successful predictions after filtering.\n\n")
    return(NULL)
  }

  # Ensemble
  stack_pred <- raster::stack(predictions)
  ens_mean   <- calc(stack_pred, mean, na.rm = TRUE)
  ens_sd     <- calc(stack_pred, sd, na.rm = TRUE)
  ens_median <- calc(stack_pred, median, na.rm = TRUE)
  ens_cv     <- overlay(ens_sd, ens_mean, fun = function(sd, mean) ifelse(abs(mean) > 1e-10, sd / abs(mean), NA))

  names(ens_mean)   <- paste0("suitability_wl", wl)
  names(ens_sd)     <- paste0("suitability_sd_wl", wl)
  names(ens_median) <- paste0("suitability_median_wl", wl)
  names(ens_cv)     <- paste0("suitability_cv_wl", wl)

  cat("Prediction ensemble complete (", length(predictions), "models )\n\n")
  list(
    warming_level = wl,
    mean = ens_mean,
    sd = ens_sd,
    median = ens_median,
    cv = ens_cv,
    n_brt_predictions = length(predictions),
    individual_predictions = predictions
  )
}

# 4) Run for all warming levels with 100% coverage
cat("Running predictions for all warming levels (100% coverage)...\n")
wl_predictions_100 <- purrr::map(wl_ensembles, ~ predict_warming_level_100pct(.x, Aeae_successful_results, Haiti_adm3))
wl_predictions_100 <- purrr::compact(wl_predictions_100)

assign("cmip6_wl_predictions_100pct", wl_predictions_100, envir = .GlobalEnv)

# Store per-warming-level rasters
purrr::walk(wl_predictions_100, function(pred) {
  wl_name <- paste0("Aeae_WL", gsub("\\.", "_", pred$warming_level))
  assign(paste0(wl_name, "_suitability_mean"),   pred$mean,   envir = .GlobalEnv)
  assign(paste0(wl_name, "_suitability_sd"),     pred$sd,     envir = .GlobalEnv)
  assign(paste0(wl_name, "_suitability_median"), pred$median, envir = .GlobalEnv)
  assign(paste0(wl_name, "_suitability_cv"),     pred$cv,     envir = .GlobalEnv)
  cat("✓", wl_name, "- 100% coverage predictions from", pred$n_brt_predictions, "BRT models\n")
})

# Summary
if (length(wl_predictions_100) > 0) {
  summary_100 <- purrr::map_dfr(wl_predictions_100, function(pred) {
    mv <- values(pred$mean); mv <- mv[!is.na(mv)]
    tibble::tibble(
      warming_level = pred$warming_level,
      n_brt_models = pred$n_brt_predictions,
      mean_suitability = round(mean(mv), 4),
      sd_suitability   = round(sd(mv), 4),
      min_suitability  = round(min(mv), 4),
      max_suitability  = round(max(mv), 4)
    )
  })
  assign("cmip6_wl_predictions_100pct_summary", summary_100, envir = .GlobalEnv)
  print(summary_100, n = Inf)
} else {
  cat("No warming level predictions produced under 100% coverage.\n")
}
```

```{r, results='asis'}
#| label: comprehensive-interactive-map-radio
#| code-summary: "Comprehensive interactive map using native layer control"
#| echo: false

library(leaflet)
library(raster)
library(dplyr)
library(htmltools)
library(htmlwidgets)
library(viridis)


# Create suitability palette
suitability_pal <- colorNumeric(
    palette = viridis::plasma(256),
    domain = c(0, 1),
    na.color = "transparent"
)

# Initialize map
map <- leaflet() %>%
    addProviderTiles("OpenStreetMap", group = "OSM") %>%
    addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
    addProviderTiles("OpenTopoMap", group = "Topographic") %>%
    setView(lng = -72.3, lat = 19.0, zoom = 8)

# Track all layer groups
layer_groups <- c()

# === ADD BASELINE LAYERS ===
cat("Adding baseline layers...\n")

map <- map %>%
    addRasterImage(Aeae_ensemble_mean, colors = suitability_pal, opacity = 0.8, group = "Baseline - Mean") %>%
    addRasterImage(Aeae_ensemble_median, colors = suitability_pal, opacity = 0.8, group = "Baseline - Median")

layer_groups <- c(layer_groups, "Baseline - Mean", "Baseline - Median")

# Baseline BRT models
n_brts <- length(Aeae_individual_predictions)
for (j in 1:n_brts) {
    group_name <- paste0("Baseline - Model ", j)
    map <- map %>% addRasterImage(Aeae_individual_predictions[[j]], colors = suitability_pal, opacity = 0.8, group = group_name)
    layer_groups <- c(layer_groups, group_name)
}

# Baseline uncertainty
sd_pal_b <- colorNumeric(palette = viridis::inferno(256), domain = range(values(Aeae_ensemble_sd), na.rm = TRUE), na.color = "transparent")
cv_pal_b <- colorNumeric(palette = viridis::inferno(256), domain = range(values(Aeae_ensemble_cv), na.rm = TRUE), na.color = "transparent")

map <- map %>%
    addRasterImage(Aeae_ensemble_sd, colors = sd_pal_b, opacity = 0.8, group = "Baseline - Std Dev") %>%
    addRasterImage(Aeae_ensemble_cv, colors = cv_pal_b, opacity = 0.8, group = "Baseline - Coeff Var")

layer_groups <- c(layer_groups, "Baseline - Std Dev", "Baseline - Coeff Var")

# === ADD WARMING LEVEL LAYERS ===
for (i in seq_along(wl_predictions_100)) {
    wl <- wl_predictions_100[[i]]$warming_level
    wl_display <- paste0(wl, "°C")
    
    cat("Adding", wl, "°C layers...\n")
    
    # Mean and Median
    map <- map %>%
        addRasterImage(wl_predictions_100[[i]]$mean, colors = suitability_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Mean")) %>%
        addRasterImage(wl_predictions_100[[i]]$median, colors = suitability_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Median"))
    
    layer_groups <- c(layer_groups, paste0(wl_display, " - Mean"), paste0(wl_display, " - Median"))
    
    # BRT models
    for (j in 1:n_brts) {
        group_name <- paste0(wl_display, " - Model ", j)
        map <- map %>% addRasterImage(wl_predictions_100[[i]]$individual_predictions[[j]], 
                                     colors = suitability_pal, opacity = 0.8, group = group_name)
        layer_groups <- c(layer_groups, group_name)
    }
    
    # Uncertainty
    sd_pal <- colorNumeric(palette = viridis::inferno(256), domain = range(values(wl_predictions_100[[i]]$sd), na.rm = TRUE), na.color = "transparent")
    cv_pal <- colorNumeric(palette = viridis::inferno(256), domain = range(values(wl_predictions_100[[i]]$cv), na.rm = TRUE), na.color = "transparent")
    
    map <- map %>%
        addRasterImage(wl_predictions_100[[i]]$sd, colors = sd_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Std Dev")) %>%
        addRasterImage(wl_predictions_100[[i]]$cv, colors = cv_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Coeff Var"))
    
    layer_groups <- c(layer_groups, paste0(wl_display, " - Std Dev"), paste0(wl_display, " - Coeff Var"))
}

# Use native layer control
map <- map %>%
    addLayersControl(
        baseGroups = c("OSM", "Satellite", "Topographic"),
        overlayGroups = layer_groups,
        options = layersControlOptions(collapsed = TRUE)
    ) %>%
    hideGroup(layer_groups[-1])  # Hide all except first (Baseline - Mean)

# Add legend - create reversed palette just for the legend
map <- map %>%
    addLegend(
        pal = colorNumeric(
            palette = viridis::plasma(256),
            domain = c(1, 0),  # Reverse domain
            na.color = "transparent"
        ),
        values = c(1, 0),  # Reverse values
        title = "Legend",
        position = "bottomleft",
        opacity = 1
    )

# Add info box
info_html <- tags$div(
    style = "background: white; padding: 8px; border-radius: 4px; 
             box-shadow: 0 1px 5px rgba(0,0,0,0.4); font-size: 11px;
             font-family: Arial, sans-serif; max-width: 200px;",
    tags$div(style = "font-weight: bold; margin-bottom: 5px;", "Layer Guide:"),
    tags$div("• Use checkboxes on right to toggle layers"),
    tags$div("• Only check ONE layer at a time for best performance"),
    tags$div("• ", tags$b("Mean/Median:"), " Ensemble predictions"),
    tags$div("• ", tags$b("Model 1-", n_brts, ":"), " Individual BRTs"),
    tags$div("• ", tags$b("Std Dev/Coeff Var:"), " Uncertainty")
)

map <- map %>% addControl(info_html, position = "topright")

# Save and display
assign("comprehensive_interactive_map", map, envir = .GlobalEnv)

cat("\nMap created with", length(layer_groups), "prediction layers\n")
cat("Use the layer control (top left) to switch between layers\n")
cat("Starting with 'Baseline - Mean' visible\n\n")

comprehensive_interactive_map

# Save HTML
output_file <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Ensemble_BRT/Comprehensive_Warming_Map.html"
htmlwidgets::saveWidget(map, file = output_file, selfcontained = TRUE, 
                       title = "Aedes aegypti Climate Predictions")
cat("Saved to:", output_file, "\n")
```

```{r}
#| label: Figure-3
#| code-summary: "Comprehensive interactive map using native layer control"
#| echo: false

library(leaflet)
library(raster)
library(dplyr)
library(htmltools)
library(htmlwidgets)
library(viridis)

# Create suitability palette
suitability_pal <- colorNumeric(
    palette = viridis::plasma(256),
    domain = c(0, 1),
    na.color = "transparent"
)

# Initialize map
map <- leaflet() %>%
    addProviderTiles("OpenStreetMap", group = "OSM") %>%
    addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
    addProviderTiles("OpenTopoMap", group = "Topographic") %>%
    setView(lng = -72.3, lat = 19.0, zoom = 8)

# Track all layer groups
layer_groups <- c()

# === ADD BASELINE LAYERS ===
cat("Adding baseline layers...\n")

map <- map %>%
    addRasterImage(Aeae_ensemble_mean, colors = suitability_pal, opacity = 0.8, group = "Baseline - Mean") %>%
    addRasterImage(Aeae_ensemble_median, colors = suitability_pal, opacity = 0.8, group = "Baseline - Median")

layer_groups <- c(layer_groups, "Baseline - Mean", "Baseline - Median")

# Baseline BRT models
n_brts <- length(Aeae_individual_predictions)
for (j in 1:n_brts) {
    group_name <- paste0("Baseline - Model ", j)
    map <- map %>% addRasterImage(Aeae_individual_predictions[[j]], colors = suitability_pal, opacity = 0.8, group = group_name)
    layer_groups <- c(layer_groups, group_name)
}

# Baseline uncertainty
sd_pal_b <- colorNumeric(palette = viridis::inferno(256), domain = range(values(Aeae_ensemble_sd), na.rm = TRUE), na.color = "transparent")
cv_pal_b <- colorNumeric(palette = viridis::inferno(256), domain = range(values(Aeae_ensemble_cv), na.rm = TRUE), na.color = "transparent")

map <- map %>%
    addRasterImage(Aeae_ensemble_sd, colors = sd_pal_b, opacity = 0.8, group = "Baseline - Std Dev") %>%
    addRasterImage(Aeae_ensemble_cv, colors = cv_pal_b, opacity = 0.8, group = "Baseline - Coeff Var")

layer_groups <- c(layer_groups, "Baseline - Std Dev", "Baseline - Coeff Var")

# === ADD WARMING LEVEL LAYERS ===
for (i in seq_along(wl_predictions_100)) {
    wl <- wl_predictions_100[[i]]$warming_level
    wl_display <- paste0(wl, "°C")
    
    cat("Adding", wl, "°C layers...\n")
    
    # Mean and Median
    map <- map %>%
        addRasterImage(wl_predictions_100[[i]]$mean, colors = suitability_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Mean")) %>%
        addRasterImage(wl_predictions_100[[i]]$median, colors = suitability_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Median"))
    
    layer_groups <- c(layer_groups, paste0(wl_display, " - Mean"), paste0(wl_display, " - Median"))
    
    # BRT models
    for (j in 1:n_brts) {
        group_name <- paste0(wl_display, " - Model ", j)
        map <- map %>% addRasterImage(wl_predictions_100[[i]]$individual_predictions[[j]], 
                                     colors = suitability_pal, opacity = 0.8, group = group_name)
        layer_groups <- c(layer_groups, group_name)
    }
    
    # Uncertainty
    sd_pal <- colorNumeric(palette = viridis::inferno(256), domain = range(values(wl_predictions_100[[i]]$sd), na.rm = TRUE), na.color = "transparent")
    cv_pal <- colorNumeric(palette = viridis::inferno(256), domain = range(values(wl_predictions_100[[i]]$cv), na.rm = TRUE), na.color = "transparent")
    
    map <- map %>%
        addRasterImage(wl_predictions_100[[i]]$sd, colors = sd_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Std Dev")) %>%
        addRasterImage(wl_predictions_100[[i]]$cv, colors = cv_pal, opacity = 0.8, 
                      group = paste0(wl_display, " - Coeff Var"))
    
    layer_groups <- c(layer_groups, paste0(wl_display, " - Std Dev"), paste0(wl_display, " - Coeff Var"))
}

# Use native layer control
map <- map %>%
    addLayersControl(
        baseGroups = c("OSM", "Satellite", "Topographic"),
        overlayGroups = layer_groups,
        options = layersControlOptions(collapsed = FALSE)
    ) %>%
    hideGroup(layer_groups[-1])  # Hide all except first (Baseline - Mean)

# Add legend - create reversed palette just for the legend
map <- map %>%
    addLegend(
        pal = colorNumeric(
            palette = viridis::plasma(256),
            domain = c(1, 0),  # Reverse domain
            na.color = "transparent"
        ),
        values = c(1, 0),  # Reverse values
        title = "Legend",
        position = "bottomleft",
        opacity = 1
    )

# Add info box
info_html <- tags$div(
    style = "background: white; padding: 8px; border-radius: 4px; 
             box-shadow: 0 1px 5px rgba(0,0,0,0.4); font-size: 11px;
             font-family: Arial, sans-serif; max-width: 200px;",
    tags$div(style = "font-weight: bold; margin-bottom: 5px;", "Layer Guide:"),
    tags$div("• Use checkboxes on left to toggle layers"),
    tags$div("• Only check ONE layer at a time for best performance"),
    tags$div("• ", tags$b("Mean/Median:"), " Ensemble predictions"),
    tags$div("• ", tags$b("Model 1-", n_brts, ":"), " Individual BRTs"),
    tags$div("• ", tags$b("Std Dev/Coeff Var:"), " Uncertainty")
)

map <- map %>% addControl(info_html, position = "topright")

# Save and display
assign("comprehensive_interactive_map", map, envir = .GlobalEnv)

cat("\nMap created with", length(layer_groups), "prediction layers\n")
cat("Use the layer control (top left) to switch between layers\n")
cat("Starting with 'Baseline - Mean' visible\n\n")

comprehensive_interactive_map

# Save HTML
output_file <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Ensemble_BRT/Comprehensive_Warming_Map.html"
htmlwidgets::saveWidget(map, file = output_file, selfcontained = TRUE, 
                       title = "Aedes aegypti Climate Predictions")
cat("Saved to:", output_file, "\n")
```

```{r}
#| label: load-GPW2020-data
#| code-summary: "Load in and map gridded population of the world data for Haiti"
#| echo: false
#| include: false

  # Use geodata::population() to download the data
    GPW2020 <- geodata::population(2020, 0.5, path=tempdir())

  # Crop & Mask the raster to Haiti  
    GPW2020_cropped <- mask(crop(raster(GPW2020), haiti_wo_lakes_sp), haiti_wo_lakes_sp)

  # Plot to verify the succesful download   
    plot(GPW2020_cropped)
```

```{r, results='asis'}
#| label: load-IDP-data
#| code-summary: "Load in & map IDP data"
#| echo: false
#| include: true

# Load required libraries
library(readxl)
library(dplyr)
library(sf)
library(ggplot2)
library(viridis)
library(classInt)

# Load the IDP data
idp_data <- read_excel("C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/data/IDP_Data_Cleaned.xlsx")

# Clean column names to remove any invisible characters
names(idp_data) <- trimws(names(idp_data))
names(idp_data) <- iconv(names(idp_data), to = "UTF-8")

# Alternative approach: Use column positions instead of names
idp_cleaned <- idp_data %>%
  # Select columns by position and rename them
  dplyr::select(
    key_informants = 1,           # "Nombre d'informateurs clés interviewés dans la localité"
    evaluation_date = 2,          # "Date de l'évaluation"
    collection_method = 3,        # "Méthode de collecte"
    zone_type = 4,               # "Zone située dans la ZMPP (capital) ou en provinces"
    idp_present = 5,             # "Présence des PDI dans cette communauté ?"
    idp_location_type = 6,       # "Si oui, est-ce en sites ou en familles d'accueil/hors sites ?"
    returnee_present = 7,        # "Présence des retournés anciennes PDI dans cette communauté ?"
    dept_pcode = 8,              # "P-code département"
    dept_name = 9,               # "Nom du département"
    commune_pcode = 10,          # "P-code commune"
    commune_name = 11,           # "Nom commune"
    section_pcode = 12,          # "P-code section communale"
    section_name = 13,           # "Nom section communale"
    quartier_pcode = 14,         # "P-code quartier (uniquement dans la ZMPP)"
    quartier_name = 15,          # "Nom quartier  (uniquement dans la ZMPP)"
    site_pcode = 16,             # "P-code site"
    site_name = 17,              # "Nom du site"
    site_status = 18,            # "Statut de site"
    site_creation_date = 19,     # "Date de création de site"
    site_type = 20,              # "Type de site"
    school_continues = 21,       # School continues question
    sufficient_space = 22,       # Sufficient space question
    site_accessible = 23,        # Site accessible
    management_committee = 24,   # Management committee
    committee_composition = 25,  # Committee composition
    committee_lives_onsite = 26, # Committee lives on site
    closure_threat = 27,         # Closure threat
    idp_households = 28,         # "Ménages déplacés internes"
    idp_total = 29,              # "Personnes déplacées internes (PDI)"
    # Age/gender breakdown for IDPs
    idp_baby_male = 30,          # "B4.a. 1 Bébé (moins d'1 an) - Garçon (PDI)"
    idp_baby_female = 31,        # "B4.a. 2 Bébé (moins d'1 an) - Fille  (PDI)"
    idp_child1_5_male = 32,      # "B4.b. 1 Enfants (1 à 5 ans) - Garçon  (PDI)"
    idp_child1_5_female = 33,    # "B4.b. 2 Enfants (1 à 5 ans) -  Fille  (PDI)"
    idp_child6_11_male = 34,     # "B4.c. 1 Enfants (6 à 11 ans) - Garçon  (PDI)"
    idp_child6_11_female = 35,   # "B4.c. 2 Enfants (6 à 11 ans) - Fille  (PDI)"
    idp_adolescent_male = 36,    # "B4.d. 1 Adolescents (12 à 17 ans) - Garçon  (PDI)"
    idp_adolescent_female = 37,  # "B4.d. 2 Adolescents (12 à 17 ans) - Fille  (PDI)"
    idp_adult_male = 38,         # "B4.e. Adultes (18 à 59 ans) - Homme  (PDI)"
    idp_adult_female = 39,       # "B4.e. Adultes (18 à 59 ans) - Femme  (PDI)"
    idp_elderly_male = 40,       # "B4.f Personnes âgées (+=60ans) - Homme  (PDI)"
    idp_elderly_female = 41,     # "B4.f Personnes âgées (+=60ans)- Femme  (PDI)"
    # Origin information
    origin_dept_pcode = 42,      # Origin department p-code
    origin_dept_name = 43,       # Origin department name
    origin_commune_pcode = 44,   # Origin commune p-code
    origin_commune_name = 45,    # Origin commune name
    displacement_date = 46,      # Displacement date
    displacement_reason = 47,    # Displacement reason
    # Returnee data
    returnee_households = 48,    # "Ménages retournés anciennes PDI"
    returnee_total = 49,         # "Personnes retournées anciennes PDI"
    # Priority needs
    need_food = 74,              # Food need
    need_water = 75,             # Water need
    need_shelter = 76,           # Shelter need
    need_health = 77,            # Health need
    need_nfi = 78,               # NFI need
    need_hygiene = 79,           # Hygiene need
    need_education = 80,         # Education need
    need_work = 81               # Work need
  ) %>%
  # Convert numeric columns
  dplyr::mutate(
    dplyr::across(c(key_informants, idp_households, idp_total, idp_baby_male:idp_elderly_female, 
             returnee_households, returnee_total), as.numeric),
    # Create total children and adult categories - fixed approach
    idp_children_total = (idp_baby_male + idp_baby_female + idp_child1_5_male + 
                         idp_child1_5_female + idp_child6_11_male + idp_child6_11_female +
                         idp_adolescent_male + idp_adolescent_female),
    idp_adults_total = (idp_adult_male + idp_adult_female + idp_elderly_male + idp_elderly_female),
    # Replace NAs with 0 for the totals
    idp_children_total = ifelse(is.na(idp_children_total), 0, idp_children_total),
    idp_adults_total = ifelse(is.na(idp_adults_total), 0, idp_adults_total),
    # Clean the commune p-code for matching
    commune_pcode = trimws(commune_pcode),
    # Convert evaluation_date to Date if needed
    evaluation_date = as.Date(evaluation_date)
  ) %>%
  # Filter for records with IDP presence
  dplyr::filter(idp_present == "oui" | !is.na(idp_total))

# Aggregate data by section (ADM3 level) - this is the correct approach
idp_by_section <- idp_cleaned %>%
  dplyr::group_by(section_pcode, section_name, commune_name, dept_name) %>%
  dplyr::summarise(
    total_idp_sites = n(),
    total_idp_households = sum(idp_households, na.rm = TRUE),
    total_idp_persons = sum(idp_total, na.rm = TRUE),
    total_children = sum(idp_children_total, na.rm = TRUE),
    total_adults = sum(idp_adults_total, na.rm = TRUE),
    total_returnee_households = sum(returnee_households, na.rm = TRUE),
    total_returnee_persons = sum(returnee_total, na.rm = TRUE),
    # Calculate priority needs percentages
    pct_need_food = mean(need_food == "oui", na.rm = TRUE) * 100,
    pct_need_water = mean(need_water == "oui", na.rm = TRUE) * 100,
    pct_need_shelter = mean(need_shelter == "oui", na.rm = TRUE) * 100,
    pct_need_health = mean(need_health == "oui", na.rm = TRUE) * 100,
    .groups = 'drop'
  ) %>%
  # Remove records with no IDPs
  dplyr::filter(total_idp_persons > 0)

# Check the aggregated data
cat("Aggregated IDP data by section:\n")
print(head(idp_by_section))
cat("\nTotal sections with IDPs:", nrow(idp_by_section), "\n")

# Join with spatial data using section_pcode
haiti_idp_map <- Haiti_adm3 %>%
  dplyr::left_join(idp_by_section, by = c("ADM3_PCODE" = "section_pcode")) %>%
  # Replace NA values with 0 for mapping
  dplyr::mutate(
    total_idp_persons = ifelse(is.na(total_idp_persons), 0, total_idp_persons),
    total_idp_households = ifelse(is.na(total_idp_households), 0, total_idp_households),
    total_idp_sites = ifelse(is.na(total_idp_sites), 0, total_idp_sites)
  )

# Create choropleth map
# Calculate natural breaks for non-zero values
idp_values <- haiti_idp_map$total_idp_persons[haiti_idp_map$total_idp_persons > 0]
breaks <- classIntervals(idp_values, n = 6, style = "jenks")
break_points <- breaks$brks

# Create labels for the breaks
break_labels <- c("0", paste0(scales::comma(break_points[-length(break_points)]), " - ", 
                              scales::comma(break_points[-1])))

# Create discrete categories
haiti_idp_map <- haiti_idp_map %>%
  dplyr::mutate(
    idp_category = case_when(
      total_idp_persons == 0 ~ "0",
      total_idp_persons <= break_points[2] ~ break_labels[2],
      total_idp_persons <= break_points[3] ~ break_labels[3],
      total_idp_persons <= break_points[4] ~ break_labels[4],
      total_idp_persons <= break_points[5] ~ break_labels[5],
      total_idp_persons <= break_points[6] ~ break_labels[6],
      TRUE ~ break_labels[7]
    ),
    idp_category = factor(idp_category, levels = break_labels)
  )

# Create choropleth map
idp_map <- ggplot(haiti_idp_map) +
  geom_sf(aes(fill = idp_category), color = "gray80", size = 0.05) +
  scale_fill_viridis_d(
    name = "IDPs\n(Persons)",
    option = "plasma",
    na.value = "grey90",
    direction = 1
  ) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    legend.position = "right"
  ) +
  labs(
    title = "Distribution of Internally Displaced Persons in Haiti",
    subtitle = "By Administrative Level 3 (Section Communale)",
    caption = "Source: IDP Assessment Data"
  )

# Display the map
print(idp_map)

# Summary statistics
cat("Summary of IDP Distribution:\n")
cat("Total sections with IDPs:", sum(haiti_idp_map$total_idp_persons > 0), "\n")
cat("Total IDP persons:", sum(haiti_idp_map$total_idp_persons, na.rm = TRUE), "\n")
cat("Total IDP households:", sum(haiti_idp_map$total_idp_households, na.rm = TRUE), "\n")
cat("Total IDP sites:", sum(haiti_idp_map$total_idp_sites, na.rm = TRUE), "\n")

# Show top 10 sections by IDP population
top_sections <- haiti_idp_map %>%
  sf::st_drop_geometry() %>%
  dplyr::filter(total_idp_persons > 0) %>%
  dplyr::arrange(desc(total_idp_persons)) %>%
  dplyr::select(ADM3_EN, section_name, commune_name, dept_name, total_idp_persons, total_idp_sites) %>%
  head(10)

print("Top 10 Sections by IDP Population:")
print(top_sections)

# Show some statistics by department
dept_summary <- haiti_idp_map %>%
  sf::st_drop_geometry() %>%
  dplyr::filter(total_idp_persons > 0) %>%
  dplyr::group_by(dept_name) %>%
  dplyr::summarise(
    sections_with_idps = n(),
    total_idp_persons = sum(total_idp_persons, na.rm = TRUE),
    total_idp_households = sum(total_idp_households, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  dplyr::arrange(desc(total_idp_persons))

print("IDP Distribution by Department:")
print(dept_summary)
```

```{r}
#| label: Generate-multi-level-maps
#| code-summary: "Generate interactive maps at section, commune, and department levels"
#| echo: false
#| warning: false
#| include: true

# Prepare the raster stack
baseline_raster <- if(class(Aeae_ensemble_mean)[1] == "RasterBrick") {
  Aeae_ensemble_mean[[1]]
} else {
  Aeae_ensemble_mean
}
wl15_raster <- if(class(Aeae_WL1_5_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL1_5_suitability_mean[[1]]
} else {
  Aeae_WL1_5_suitability_mean
}
wl2_raster <- if(class(Aeae_WL2_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL2_suitability_mean[[1]]
} else {
  Aeae_WL2_suitability_mean
}
wl3_raster <- if(class(Aeae_WL3_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL3_suitability_mean[[1]]
} else {
  Aeae_WL3_suitability_mean
}
wl4_raster <- if(class(Aeae_WL4_suitability_mean)[1] == "RasterBrick") {
  Aeae_WL4_suitability_mean[[1]]
} else {
  Aeae_WL4_suitability_mean
}

raster_stack <- stack(
  baseline_raster,
  wl15_raster,
  wl2_raster,
  wl3_raster,
  wl4_raster,
  GPW2020_cropped
)

names(raster_stack) <- c(
  "baseline_suitability",
  "suitability_1_5C",
  "suitability_2C",
  "suitability_3C",
  "suitability_4C",
  "population_2020"
)

# Calculate zonal statistics for section level (ADM3)
zonal_stats <- exact_extract(
  raster_stack,
  haiti_idp_map,
  function(values, coverage_fraction) {
    suitability_means <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      weighted.mean(x, coverage_fraction, na.rm = TRUE)
    })
    population_sum <- sum(values[, 6] * coverage_fraction, na.rm = TRUE)
    result <- data.frame(
      mean_baseline_suitability = suitability_means[1],
      mean_suitability_1_5C = suitability_means[2],
      mean_suitability_2C = suitability_means[3],
      mean_suitability_3C = suitability_means[4],
      mean_suitability_4C = suitability_means[5],
      total_population_2020 = population_sum
    )
    return(result)
  }
)

# Combine zonal statistics with haiti_idp_map
Haiti_adm3_with_stats <- haiti_idp_map %>%
  bind_cols(zonal_stats) %>%
  mutate(
    suitability_change_1_5C = mean_suitability_1_5C - mean_baseline_suitability,
    suitability_change_2C = mean_suitability_2C - mean_baseline_suitability,
    suitability_change_3C = mean_suitability_3C - mean_baseline_suitability,
    suitability_change_4C = mean_suitability_4C - mean_baseline_suitability,
    idp_density = ifelse(total_population_2020 > 0,
                        (total_idp_persons / total_population_2020) * 100, 0),
    across(starts_with("mean_"), ~round(.x, 2)),
    across(starts_with("suitability_change"), ~round(.x, 2)),
    across(starts_with("total_population"), ~round(.x, 0)),
    idp_density = round(idp_density, 2)
  )

# ============== SECTION LEVEL (ADM3) ==============
Haiti_adm3_interactive <- Haiti_adm3_with_stats %>%
  mutate(
    across(starts_with("mean_"), ~round(.x, 2)),
    across(starts_with("suitability_change"), ~round(.x, 2)),
    idp_density = round(idp_density, 2)
  )

# Create color palette based on range across all scenarios
all_suitability_adm3 <- c(
  Haiti_adm3_interactive$mean_baseline_suitability,
  Haiti_adm3_interactive$mean_suitability_1_5C,
  Haiti_adm3_interactive$mean_suitability_2C,
  Haiti_adm3_interactive$mean_suitability_3C,
  Haiti_adm3_interactive$mean_suitability_4C
)

suitability_pal_adm3 <- colorNumeric(
  palette = viridis::plasma(100),
  domain = all_suitability_adm3,
  na.color = "transparent"
)

section_map <- leaflet(Haiti_adm3_interactive) %>%
  addProviderTiles("OpenStreetMap", group = "OpenStreetMap") %>%
  addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
  addProviderTiles("CartoDB.Positron", group = "Light") %>%
  setView(lng = -72.3, lat = 19.0, zoom = 8) %>%
  
  # Baseline suitability layer
  addPolygons(
    fillColor = ~suitability_pal_adm3(mean_baseline_suitability),
    fillOpacity = 0.7,
    color = "white",
    weight = 0.5,
    opacity = 1,
    group = "Baseline",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM3_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Section: ", ADM3_EN, "<br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM3_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM3_EN, " (", ADM2_EN, ")")
  ) %>%
  
  # 1.5°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm3(mean_suitability_1_5C),
    fillOpacity = 0.7,
    color = "white",
    weight = 0.5,
    opacity = 1,
    group = "1.5°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM3_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Section: ", ADM3_EN, "<br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM3_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM3_EN, " (", ADM2_EN, ")")
  ) %>%
  
  # 2°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm3(mean_suitability_2C),
    fillOpacity = 0.7,
    color = "white",
    weight = 0.5,
    opacity = 1,
    group = "2°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM3_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Section: ", ADM3_EN, "<br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM3_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM3_EN, " (", ADM2_EN, ")")
  ) %>%
  
  # 3°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm3(mean_suitability_3C),
    fillOpacity = 0.7,
    color = "white",
    weight = 0.5,
    opacity = 1,
    group = "3°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM3_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Section: ", ADM3_EN, "<br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM3_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM3_EN, " (", ADM2_EN, ")")
  ) %>%
  
  # 4°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm3(mean_suitability_4C),
    fillOpacity = 0.7,
    color = "white",
    weight = 0.5,
    opacity = 1,
    group = "4°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM3_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Section: ", ADM3_EN, "<br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM3_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM3_EN, " (", ADM2_EN, ")")
  ) %>%
  
  addLayersControl(
    baseGroups = c("OpenStreetMap", "Satellite", "Light"),
    overlayGroups = c("Baseline", "1.5°C Warming", "2°C Warming", "3°C Warming", "4°C Warming"),
    options = layersControlOptions(collapsed = FALSE),
    position = "topleft"
  ) %>%
  hideGroup(c("1.5°C Warming", "2°C Warming", "3°C Warming", "4°C Warming")) %>%
  addLegend(
    pal = suitability_pal_adm3,
    values = all_suitability_adm3,
    title = "Mean Suitability<br>(Section Level)",
    position = "bottomright",
    opacity = 1
  )

# ============== COMMUNE LEVEL (ADM2) ==============
# Aggregate IDP data to commune level
Haiti_adm2_idp <- Haiti_adm3_with_stats %>%
  st_drop_geometry() %>%
  group_by(ADM2_EN, ADM2_PCODE, ADM1_EN, ADM1_PCODE) %>%
  summarise(
    total_idp_sites = sum(total_idp_sites, na.rm = TRUE),
    total_idp_households = sum(total_idp_households, na.rm = TRUE),
    total_idp_persons = sum(total_idp_persons, na.rm = TRUE),
    .groups = "drop"
  )

# Get commune boundaries (dissolve sections to communes)
Haiti_adm2_boundaries <- Haiti_adm3_with_stats %>%
  group_by(ADM2_EN, ADM2_PCODE, ADM1_EN, ADM1_PCODE) %>%
  summarise(.groups = "drop")

# Extract raster statistics directly at commune level
zonal_stats_adm2 <- exact_extract(
  raster_stack,
  Haiti_adm2_boundaries,
  function(values, coverage_fraction) {
    suitability_means <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      weighted.mean(x, coverage_fraction, na.rm = TRUE)
    })
    population_sum <- sum(values[, 6] * coverage_fraction, na.rm = TRUE)
    result <- data.frame(
      mean_baseline_suitability = suitability_means[1],
      mean_suitability_1_5C = suitability_means[2],
      mean_suitability_2C = suitability_means[3],
      mean_suitability_3C = suitability_means[4],
      mean_suitability_4C = suitability_means[5],
      total_population_2020 = population_sum
    )
    return(result)
  }
)

Haiti_adm2_interactive <- Haiti_adm2_boundaries %>%
  bind_cols(zonal_stats_adm2) %>%
  left_join(Haiti_adm2_idp, by = c("ADM2_EN", "ADM2_PCODE", "ADM1_EN", "ADM1_PCODE")) %>%
  mutate(
    suitability_change_1_5C = mean_suitability_1_5C - mean_baseline_suitability,
    suitability_change_2C = mean_suitability_2C - mean_baseline_suitability,
    suitability_change_3C = mean_suitability_3C - mean_baseline_suitability,
    suitability_change_4C = mean_suitability_4C - mean_baseline_suitability,
    idp_density = ifelse(total_population_2020 > 0,
                        (total_idp_persons / total_population_2020) * 100, 0),
    across(starts_with("mean_"), ~round(.x, 2)),
    across(starts_with("suitability_change"), ~round(.x, 2)),
    total_population_2020 = round(total_population_2020, 0),
    idp_density = round(idp_density, 2)
  )

# Create color palette based on range across all scenarios
all_suitability_adm2 <- c(
  Haiti_adm2_interactive$mean_baseline_suitability,
  Haiti_adm2_interactive$mean_suitability_1_5C,
  Haiti_adm2_interactive$mean_suitability_2C,
  Haiti_adm2_interactive$mean_suitability_3C,
  Haiti_adm2_interactive$mean_suitability_4C
)

suitability_pal_adm2 <- colorNumeric(
  palette = viridis::plasma(100),
  domain = all_suitability_adm2,
  na.color = "transparent"
)

commune_map <- leaflet(Haiti_adm2_interactive) %>%
  addProviderTiles("OpenStreetMap", group = "OpenStreetMap") %>%
  addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
  addProviderTiles("CartoDB.Positron", group = "Light") %>%
  setView(lng = -72.3, lat = 19.0, zoom = 8) %>%
  
  # Baseline suitability layer
  addPolygons(
    fillColor = ~suitability_pal_adm2(mean_baseline_suitability),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    opacity = 1,
    group = "Baseline",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM2_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM2_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM2_EN, " (", ADM1_EN, ")")
  ) %>%
  
  # 1.5°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm2(mean_suitability_1_5C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    opacity = 1,
    group = "1.5°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM2_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM2_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM2_EN, " (", ADM1_EN, ")")
  ) %>%
  
  # 2°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm2(mean_suitability_2C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    opacity = 1,
    group = "2°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM2_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM2_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM2_EN, " (", ADM1_EN, ")")
  ) %>%
  
  # 3°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm2(mean_suitability_3C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    opacity = 1,
    group = "3°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM2_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM2_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM2_EN, " (", ADM1_EN, ")")
  ) %>%
  
  # 4°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm2(mean_suitability_4C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    opacity = 1,
    group = "4°C Warming",
    highlightOptions = highlightOptions(
      weight = 2,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM2_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Commune: ", ADM2_EN, "<br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM2_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~paste0(ADM2_EN, " (", ADM1_EN, ")")
  ) %>%
  
  addLayersControl(
    baseGroups = c("OpenStreetMap", "Satellite", "Light"),
    overlayGroups = c("Baseline", "1.5°C Warming", "2°C Warming", "3°C Warming", "4°C Warming"),
    options = layersControlOptions(collapsed = FALSE),
    position = "topleft"
  ) %>%
  hideGroup(c("1.5°C Warming", "2°C Warming", "3°C Warming", "4°C Warming")) %>%
  addLegend(
    pal = suitability_pal_adm2,
    values = all_suitability_adm2,
    title = "Mean Suitability<br>(Commune Level)",
    position = "bottomright",
    opacity = 1
  )

# ============== DEPARTMENT LEVEL (ADM1) ==============
# Aggregate IDP data to department level
Haiti_adm1_idp <- Haiti_adm3_with_stats %>%
  st_drop_geometry() %>%
  group_by(ADM1_EN, ADM1_PCODE) %>%
  summarise(
    total_idp_sites = sum(total_idp_sites, na.rm = TRUE),
    total_idp_households = sum(total_idp_households, na.rm = TRUE),
    total_idp_persons = sum(total_idp_persons, na.rm = TRUE),
    .groups = "drop"
  )

# Get department boundaries (dissolve sections to departments)
Haiti_adm1_boundaries <- Haiti_adm3_with_stats %>%
  group_by(ADM1_EN, ADM1_PCODE) %>%
  summarise(.groups = "drop")

# Extract raster statistics directly at department level
zonal_stats_adm1 <- exact_extract(
  raster_stack,
  Haiti_adm1_boundaries,
  function(values, coverage_fraction) {
    suitability_means <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      weighted.mean(x, coverage_fraction, na.rm = TRUE)
    })
    population_sum <- sum(values[, 6] * coverage_fraction, na.rm = TRUE)
    result <- data.frame(
      mean_baseline_suitability = suitability_means[1],
      mean_suitability_1_5C = suitability_means[2],
      mean_suitability_2C = suitability_means[3],
      mean_suitability_3C = suitability_means[4],
      mean_suitability_4C = suitability_means[5],
      total_population_2020 = population_sum
    )
    return(result)
  }
)

Haiti_adm1_interactive <- Haiti_adm1_boundaries %>%
  bind_cols(zonal_stats_adm1) %>%
  left_join(Haiti_adm1_idp, by = c("ADM1_EN", "ADM1_PCODE")) %>%
  mutate(
    suitability_change_1_5C = mean_suitability_1_5C - mean_baseline_suitability,
    suitability_change_2C = mean_suitability_2C - mean_baseline_suitability,
    suitability_change_3C = mean_suitability_3C - mean_baseline_suitability,
    suitability_change_4C = mean_suitability_4C - mean_baseline_suitability,
    idp_density = ifelse(total_population_2020 > 0,
                        (total_idp_persons / total_population_2020) * 100, 0),
    across(starts_with("mean_"), ~round(.x, 2)),
    across(starts_with("suitability_change"), ~round(.x, 2)),
    total_population_2020 = round(total_population_2020, 0),
    idp_density = round(idp_density, 2)
  )

# Create color palette based on range across all scenarios
all_suitability_adm1 <- c(
  Haiti_adm1_interactive$mean_baseline_suitability,
  Haiti_adm1_interactive$mean_suitability_1_5C,
  Haiti_adm1_interactive$mean_suitability_2C,
  Haiti_adm1_interactive$mean_suitability_3C,
  Haiti_adm1_interactive$mean_suitability_4C
)

suitability_pal_adm1 <- colorNumeric(
  palette = viridis::plasma(100),
  domain = all_suitability_adm1,
  na.color = "transparent"
)

department_map <- leaflet(Haiti_adm1_interactive) %>%
  addProviderTiles("OpenStreetMap", group = "OpenStreetMap") %>%
  addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
  addProviderTiles("CartoDB.Positron", group = "Light") %>%
  setView(lng = -72.3, lat = 19.0, zoom = 8) %>%
  
  # Baseline suitability layer
  addPolygons(
    fillColor = ~suitability_pal_adm1(mean_baseline_suitability),
    fillOpacity = 0.7,
    color = "white",
    weight = 1.5,
    opacity = 1,
    group = "Baseline",
    highlightOptions = highlightOptions(
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM1_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM1_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~ADM1_EN
  ) %>%
  
  # 1.5°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm1(mean_suitability_1_5C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1.5,
    opacity = 1,
    group = "1.5°C Warming",
    highlightOptions = highlightOptions(
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM1_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM1_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~ADM1_EN
  ) %>%
  
  # 2°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm1(mean_suitability_2C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1.5,
    opacity = 1,
    group = "2°C Warming",
    highlightOptions = highlightOptions(
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM1_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM1_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~ADM1_EN
  ) %>%
  
  # 3°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm1(mean_suitability_3C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1.5,
    opacity = 1,
    group = "3°C Warming",
    highlightOptions = highlightOptions(
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM1_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM1_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~ADM1_EN
  ) %>%
  
  # 4°C warming layer
  addPolygons(
    fillColor = ~suitability_pal_adm1(mean_suitability_4C),
    fillOpacity = 0.7,
    color = "white",
    weight = 1.5,
    opacity = 1,
    group = "4°C Warming",
    highlightOptions = highlightOptions(
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    ),
    popup = ~paste0(
      "<div style='font-family: Arial; font-size: 12px; max-width: 300px;'>",
      "<h4 style='margin: 5px 0; color: #2c3e50;'>", ADM1_EN, "</h4>",
      "<hr style='margin: 5px 0;'>",
      "<b>Administrative Info:</b><br>",
      "Department: ", ADM1_EN, "<br>",
      "P-code: ", ADM1_PCODE, "<br><br>",
      "<b><i>Aedes aegypti</i> Mean Suitability:</b><br>",
      "Baseline: ", mean_baseline_suitability, "<br>",
      "1.5°C warming: ", mean_suitability_1_5C, " (",
      ifelse(suitability_change_1_5C >= 0, "+", ""), suitability_change_1_5C, ")<br>",
      "2°C warming: ", mean_suitability_2C, " (",
      ifelse(suitability_change_2C >= 0, "+", ""), suitability_change_2C, ")<br>",
      "3°C warming: ", mean_suitability_3C, " (",
      ifelse(suitability_change_3C >= 0, "+", ""), suitability_change_3C, ")<br>",
      "4°C warming: ", mean_suitability_4C, " (",
      ifelse(suitability_change_4C >= 0, "+", ""), suitability_change_4C, ")<br><br>",
      "<b>Population & IDPs:</b><br>",
      "Total Population (2020): ", scales::comma(total_population_2020), "<br>",
      "IDP Population: ", scales::comma(total_idp_persons), "<br>",
      "IDP Households: ", scales::comma(total_idp_households), "<br>",
      "IDP Sites: ", total_idp_sites, "<br>",
      "IDP Density: ", idp_density, "% of population<br>",
      "</div>"
    ),
    label = ~ADM1_EN
  ) %>%
  
  addLayersControl(
    baseGroups = c("OpenStreetMap", "Satellite", "Light"),
    overlayGroups = c("Baseline", "1.5°C Warming", "2°C Warming", "3°C Warming", "4°C Warming"),
    options = layersControlOptions(collapsed = FALSE),
    position = "topleft"
  ) %>%
  hideGroup(c("1.5°C Warming", "2°C Warming", "3°C Warming", "4°C Warming")) %>%
  addLegend(
    pal = suitability_pal_adm1,
    values = all_suitability_adm1,
    title = "Mean Suitability<br>(Department Level)",
    position = "bottomright",
    opacity = 1
  )

assign("haiti_section_map", section_map, envir = .GlobalEnv)
assign("Haiti_adm2_interactive", Haiti_adm2_interactive, envir = .GlobalEnv)
assign("haiti_commune_map", commune_map, envir = .GlobalEnv)
assign("Haiti_adm1_interactive", Haiti_adm1_interactive, envir = .GlobalEnv)
assign("haiti_department_map", department_map, envir = .GlobalEnv)

section_map
commune_map
department_map
```

```{r}
#| label: temporal-suitability-change-plots
#| code-summary: "Line plots showing temporal change in suitability metrics across warming scenarios"
#| echo: false
#| warning: false
#| fig-width: 12
#| fig-height: 6

# Define custom purple color palette
purple_main <- "#6A1B9A"  # Deep purple
purple_light <- "#9C4DCC"  # Light purple for ribbon
purple_dark <- "#4A148C"   # Dark purple for emphasis

# Function to extract suitability metrics from individual predictions
extract_suitability_metrics <- function(individual_predictions, warming_level) {
	
	# Extract values from all individual predictions
	all_values <- purrr::map(individual_predictions, ~values(.x)) %>%
		do.call(cbind, .)
	
	# Remove NAs (areas outside Haiti)
	all_values_clean <- all_values[complete.cases(all_values), ]
	
	# Calculate mean suitability across landscape for each model
	mean_suitability_per_model <- colMeans(all_values_clean, na.rm = TRUE)
	
	# Calculate proportion >0.5 for each model
	prop_above_05_per_model <- colMeans(all_values_clean > 0.5, na.rm = TRUE)
	
	# Return summary statistics
	tibble(
		warming_level = warming_level,
		# Mean suitability metrics
		mean_suitability = mean(mean_suitability_per_model),
		mean_suitability_sd = sd(mean_suitability_per_model),
		mean_suitability_se = sd(mean_suitability_per_model) / sqrt(length(mean_suitability_per_model)),
		# Proportion >0.5 metrics
		prop_above_05 = mean(prop_above_05_per_model),
		prop_above_05_sd = sd(prop_above_05_per_model),
		prop_above_05_se = sd(prop_above_05_per_model) / sqrt(length(prop_above_05_per_model)),
		n_models = length(individual_predictions)
	)
}

# Extract metrics for baseline
cat("Extracting suitability metrics...\n")
cat("  Baseline ensemble (landscape-level)...\n")
baseline_metrics <- extract_suitability_metrics(
	Aeae_individual_predictions, 
	warming_level = 0
)

# Extract metrics for each warming level
cat("  Warming level predictions (landscape-level)...\n")
warming_metrics <- purrr::map_dfr(cmip6_wl_predictions_100pct, function(wl_pred) {
	extract_suitability_metrics(
		wl_pred$individual_predictions,
		warming_level = wl_pred$warming_level
	)
})

# Combine all metrics
temporal_metrics <- bind_rows(baseline_metrics, warming_metrics) %>%
	arrange(warming_level) %>%
	# Create factor for equidistant x-axis spacing
	mutate(
		warming_label = case_when(
			warming_level == 0 ~ "Baseline",
			TRUE ~ paste0("+", warming_level, "°C")
		),
		warming_factor = factor(warming_label, levels = c("Baseline", "+1.5°C", "+2°C", "+3°C", "+4°C"))
	)

# Display summary
cat("\n=== TEMPORAL SUITABILITY METRICS (LANDSCAPE-LEVEL) ===\n")
print(temporal_metrics, n = Inf)

# Store in global environment
assign("temporal_suitability_metrics", temporal_metrics, envir = .GlobalEnv)

# Create Plot 1: Mean Suitability Over Time
plot_mean_suitability <- ggplot(temporal_metrics, 
	aes(x = warming_factor, y = mean_suitability, group = 1)) +
	
	# Error ribbon (±1 SD)
	geom_ribbon(
		aes(ymin = mean_suitability - mean_suitability_sd,
				ymax = mean_suitability + mean_suitability_sd),
		fill = purple_light,
		alpha = 0.3
	) +
	
	# Line connecting points
	geom_line(
		color = purple_main,
		size = 1.2
	) +
	
	# Points at each warming level
	geom_point(
		color = purple_main,
		size = 4,
		shape = 21,
		fill = "white",
		stroke = 2
	) +
	
	# Formatting
	scale_y_continuous(
		limits = c(0, 1),
		expand = expansion(mult = c(0, 0.02)),
		breaks = seq(0, 1, 0.2)
	) +
	labs(
		title = "A) Mean Habitat Suitability",
		x = "Global Warming Level",
		y = "Mean Suitability"
	) +
	theme_minimal(base_size = 13) +
	theme(
		plot.title = element_text(face = "bold", size = 14, hjust = 0),
		panel.grid.minor = element_blank(),
		panel.grid.major.x = element_blank(),
		axis.text.x = element_text(angle = 0, hjust = 0.5),
		axis.title = element_text(face = "bold", size = 12)
	)

# Create Plot 2: Proportion of Landscape >0.5 Suitability
plot_proportion_suitable <- ggplot(temporal_metrics, 
	aes(x = warming_factor, y = prop_above_05 * 100, group = 1)) +
	
	# Error ribbon (±1 SD)
	geom_ribbon(
		aes(ymin = (prop_above_05 - prop_above_05_sd) * 100,
				ymax = (prop_above_05 + prop_above_05_sd) * 100),
		fill = purple_light,
		alpha = 0.3
	) +
	
	# Line connecting points
	geom_line(
		color = purple_main,
		size = 1.2
	) +
	
	# Points at each warming level
	geom_point(
		color = purple_main,
		size = 4,
		shape = 21,
		fill = "white",
		stroke = 2
	) +
	
	# Reference line at 50%
	geom_hline(
		yintercept = 50,
		linetype = "dashed",
		color = "grey50",
		alpha = 0.5
	) +
	
	# Formatting
	scale_y_continuous(
		limits = c(0, 100),
		expand = expansion(mult = c(0, 0.02)),
		breaks = seq(0, 100, 20)
	) +
	labs(
		title = "B) Proportion of Landscape with High Suitability",
		x = "Global Warming Level",
		y = "Proportion of Landscape (%)"
	) +
	theme_minimal(base_size = 13) +
	theme(
		plot.title = element_text(face = "bold", size = 14, hjust = 0),
		panel.grid.minor = element_blank(),
		panel.grid.major.x = element_blank(),
		axis.text.x = element_text(angle = 0, hjust = 0.5),
		axis.title = element_text(face = "bold", size = 12)
	)

# Create combined plot (2 panels vertically)
combined_temporal_plot <- cowplot::plot_grid(
	plot_mean_suitability,
	plot_proportion_suitable,
	ncol = 1,
	align = "v",
	axis = "lr"
)

# Store plots in global environment
assign("plot_temporal_mean_suitability", plot_mean_suitability, envir = .GlobalEnv)
assign("plot_temporal_proportion_suitable", plot_proportion_suitable, envir = .GlobalEnv)
assign("plot_temporal_combined", combined_temporal_plot, envir = .GlobalEnv)

# Display the combined plot
print(combined_temporal_plot)

# Summary statistics for interpretation
cat("\n=== KEY FINDINGS ===\n")
cat("Baseline (current conditions):\n")
cat("  Mean suitability: ", 
		sprintf("%.3f (SD = %.3f)", 
						baseline_metrics$mean_suitability, 
						baseline_metrics$mean_suitability_sd), "\n")
cat("  High suitability area: ", 
		sprintf("%.1f%% (SD = %.1f%%)", 
						baseline_metrics$prop_above_05 * 100, 
						baseline_metrics$prop_above_05_sd * 100), "\n\n")

cat("Highest warming scenario (+4°C):\n")
highest_warming <- temporal_metrics %>% filter(warming_level == max(warming_level))
cat("  Mean suitability: ", 
		sprintf("%.3f (SD = %.3f)", 
						highest_warming$mean_suitability, 
						highest_warming$mean_suitability_sd), "\n")
cat("  High suitability area: ", 
		sprintf("%.1f%% (SD = %.1f%%)", 
						highest_warming$prop_above_05 * 100, 
						highest_warming$prop_above_05_sd * 100), "\n\n")

cat("Net change from baseline to +4°C:\n")
cat("  Mean suitability: ", 
		sprintf("%+.3f (%.1f%% change)", 
						highest_warming$mean_suitability - baseline_metrics$mean_suitability,
						((highest_warming$mean_suitability - baseline_metrics$mean_suitability) / 
						 	baseline_metrics$mean_suitability) * 100), "\n")
cat("  High suitability area: ", 
		sprintf("%+.1f percentage points (%.1f%% change)", 
						(highest_warming$prop_above_05 - baseline_metrics$prop_above_05) * 100,
						((highest_warming$prop_above_05 - baseline_metrics$prop_above_05) / 
						 	baseline_metrics$prop_above_05) * 100), "\n\n")



```

```{r}
#| label: Generate-supplementary-tables
#| code-summary: "Create department and commune-level summary tables"
#| echo: false
#| warning: false

# Load required libraries
library(sf)
library(raster)
library(exactextractr)
library(dplyr)
library(knitr)
library(kableExtra)
library(tidyr)

# Define output directory
output_dir <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Ensemble_BRT"

# Ensure raster_stack is properly defined (from previous code chunk)
# This should contain: baseline, WL1.5, WL2, WL3, WL4, and population

# ============================================================================
# DEPARTMENT LEVEL (ADM1) TABLE
# ============================================================================

# Dissolve administrative boundaries to department level
Haiti_adm1 <- Haiti_adm3_with_stats %>%
  group_by(ADM1_EN, ADM1_PCODE) %>%
  summarise(
    # Sum population and IDP metrics
    total_population_2020 = sum(total_population_2020, na.rm = TRUE),
    total_idp_persons = sum(total_idp_persons, na.rm = TRUE),
    total_idp_households = sum(total_idp_households, na.rm = TRUE),
    total_idp_sites = sum(total_idp_sites, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  st_as_sf()

# Calculate detailed zonal statistics for departments
dept_zonal_stats <- exact_extract(
  raster_stack,
  Haiti_adm1,
  function(values, coverage_fraction) {
    # Calculate mean suitability values for each warming level
    suitability_means <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      weighted.mean(x, coverage_fraction, na.rm = TRUE)
    })
    
    # Calculate proportion of pixels > 0.5 for each warming level
    suitability_props <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      valid_pixels <- !is.na(x)
      if (sum(valid_pixels) == 0) return(NA)
      sum((x > 0.5) * coverage_fraction[valid_pixels], na.rm = TRUE) / 
        sum(coverage_fraction[valid_pixels], na.rm = TRUE)
    })
    
    # Return combined results
    data.frame(
      mean_baseline = suitability_means[1],
      mean_wl15 = suitability_means[2],
      mean_wl2 = suitability_means[3],
      mean_wl3 = suitability_means[4],
      mean_wl4 = suitability_means[5],
      prop_high_baseline = suitability_props[1],
      prop_high_wl15 = suitability_props[2],
      prop_high_wl2 = suitability_props[3],
      prop_high_wl3 = suitability_props[4],
      prop_high_wl4 = suitability_props[5]
    )
  }
)

# Combine department data with zonal statistics
dept_table <- Haiti_adm1 %>%
  st_drop_geometry() %>%
  bind_cols(dept_zonal_stats) %>%
  mutate(
    # Calculate IDP density
    idp_density = ifelse(total_population_2020 > 0,
                        (total_idp_persons / total_population_2020) * 100, 0),
    # Round values for presentation
    across(starts_with("mean_"), ~round(.x, 3)),
    across(starts_with("prop_high_"), ~round(.x * 100, 1)),  # Convert to percentage
    idp_density = round(idp_density, 2)
  ) %>%
  arrange(ADM1_EN) %>%
  dplyr::select(
    Department = ADM1_EN,
    # Population metrics
    Population = total_population_2020,
    IDP_Persons = total_idp_persons,
    IDP_Households = total_idp_households,
    IDP_Sites = total_idp_sites,
    IDP_Density_Pct = idp_density,
    # Baseline metrics
    Baseline_Mean = mean_baseline,
    Baseline_High_Pct = prop_high_baseline,
    # 1.5C warming
    WL15_Mean = mean_wl15,
    WL15_High_Pct = prop_high_wl15,
    # 2C warming
    WL2_Mean = mean_wl2,
    WL2_High_Pct = prop_high_wl2,
    # 3C warming
    WL3_Mean = mean_wl3,
    WL3_High_Pct = prop_high_wl3,
    # 4C warming
    WL4_Mean = mean_wl4,
    WL4_High_Pct = prop_high_wl4
  )

# ============================================================================
# COMMUNE LEVEL (ADM2) TABLE
# ============================================================================

# Dissolve administrative boundaries to commune level
Haiti_adm2 <- Haiti_adm3_with_stats %>%
  group_by(ADM1_EN, ADM2_EN, ADM2_PCODE) %>%
  summarise(
    # Sum population and IDP metrics
    total_population_2020 = sum(total_population_2020, na.rm = TRUE),
    total_idp_persons = sum(total_idp_persons, na.rm = TRUE),
    total_idp_households = sum(total_idp_households, na.rm = TRUE),
    total_idp_sites = sum(total_idp_sites, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  st_as_sf()

# Calculate detailed zonal statistics for communes
commune_zonal_stats <- exact_extract(
  raster_stack,
  Haiti_adm2,
  function(values, coverage_fraction) {
    # Calculate mean suitability values for each warming level
    suitability_means <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      weighted.mean(x, coverage_fraction, na.rm = TRUE)
    })
    
    # Calculate proportion of pixels > 0.5 for each warming level
    suitability_props <- apply(values[, 1:5, drop = FALSE], 2, function(x) {
      valid_pixels <- !is.na(x)
      if (sum(valid_pixels) == 0) return(NA)
      sum((x > 0.5) * coverage_fraction[valid_pixels], na.rm = TRUE) / 
        sum(coverage_fraction[valid_pixels], na.rm = TRUE)
    })
    
    # Return combined results
    data.frame(
      mean_baseline = suitability_means[1],
      mean_wl15 = suitability_means[2],
      mean_wl2 = suitability_means[3],
      mean_wl3 = suitability_means[4],
      mean_wl4 = suitability_means[5],
      prop_high_baseline = suitability_props[1],
      prop_high_wl15 = suitability_props[2],
      prop_high_wl2 = suitability_props[3],
      prop_high_wl3 = suitability_props[4],
      prop_high_wl4 = suitability_props[5]
    )
  }
)

# Combine commune data with zonal statistics
commune_table <- Haiti_adm2 %>%
  st_drop_geometry() %>%
  bind_cols(commune_zonal_stats) %>%
  mutate(
    # Calculate IDP density
    idp_density = ifelse(total_population_2020 > 0,
                        (total_idp_persons / total_population_2020) * 100, 0),
    # Round values for presentation
    across(starts_with("mean_"), ~round(.x, 3)),
    across(starts_with("prop_high_"), ~round(.x * 100, 1)),  # Convert to percentage
    idp_density = round(idp_density, 2)
  ) %>%
  arrange(ADM1_EN, ADM2_EN) %>%
  dplyr::select(
    Department = ADM1_EN,
    Commune = ADM2_EN,
    # Population metrics
    Population = total_population_2020,
    IDP_Persons = total_idp_persons,
    IDP_Households = total_idp_households,
    IDP_Sites = total_idp_sites,
    IDP_Density_Pct = idp_density,
    # Baseline metrics
    Baseline_Mean = mean_baseline,
    Baseline_High_Pct = prop_high_baseline,
    # 1.5C warming
    WL15_Mean = mean_wl15,
    WL15_High_Pct = prop_high_wl15,
    # 2C warming
    WL2_Mean = mean_wl2,
    WL2_High_Pct = prop_high_wl2,
    # 3C warming
    WL3_Mean = mean_wl3,
    WL3_High_Pct = prop_high_wl3,
    # 4C warming
    WL4_Mean = mean_wl4,
    WL4_High_Pct = prop_high_wl4
  )

# ============================================================================
# EXPORT NON-HTML VERSIONS FOR MANUSCRIPT
# ============================================================================

# Export department table
write.csv(
  dept_table,
  file.path(output_dir, "Table_Department_Suitability_Summary.csv"),
  row.names = FALSE
)

# Export commune table
write.csv(
  commune_table,
  file.path(output_dir, "Table_Commune_Suitability_Summary.csv"),
  row.names = FALSE
)

cat("\n=== SUPPLEMENTARY TABLES EXPORTED ===\n")
cat("Department table saved to:", file.path(output_dir, "Table_Department_Suitability_Summary.csv"), "\n")
cat("Commune table saved to:", file.path(output_dir, "Table_Commune_Suitability_Summary.csv"), "\n\n")

# ============================================================================
# CREATE HTML-FORMATTED TABLES FOR DOCUMENT
# ============================================================================

# Department table for HTML output
dept_table_html <- dept_table %>%
  kable(
    format = "html",
    caption = "Table S1. Department-level summary of Aedes aegypti suitability, population, and IDP metrics",
    col.names = c(
      "Department",
      "Population (2020)",
      "IDP Persons",
      "IDP HH",
      "IDP Sites",
      "IDP Density (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)"
    ),
    align = c("l", rep("r", 15)),
    format.args = list(big.mark = ",")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    font_size = 11
  ) %>%
  add_header_above(c(
    " " = 1,
    "Population & IDP Data" = 5,
    "Baseline" = 2,
    "1.5°C" = 2,
    "2°C" = 2,
    "3°C" = 2,
    "4°C" = 2
  )) %>%
  add_header_above(c(
    " " = 6,
    "Suitability Metrics" = 10
  )) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(c(2:6), background = "#f0f0f0") %>%
  footnote(
    general = "Mean = average suitability (0-1 scale); High (%) = percentage of pixels with suitability > 0.5; IDP = Internally Displaced Persons; HH = Households",
    general_title = "Note:",
    footnote_as_chunk = TRUE
  )

# Commune table for HTML output
commune_table_html <- commune_table %>%
  kable(
    format = "html",
    caption = "Table S2. Commune-level summary of Aedes aegypti suitability, population, and IDP metrics",
    col.names = c(
      "Department",
      "Commune",
      "Population (2020)",
      "IDP Persons",
      "IDP HH",
      "IDP Sites",
      "IDP Density (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)",
      "Mean",
      "High (%)"
    ),
    align = c("l", "l", rep("r", 14)),
    format.args = list(big.mark = ",")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    font_size = 10
  ) %>%
  add_header_above(c(
    " " = 2,
    "Population & IDP Data" = 5,
    "Baseline" = 2,
    "1.5°C" = 2,
    "2°C" = 2,
    "3°C" = 2,
    "4°C" = 2
  )) %>%
  add_header_above(c(
    " " = 7,
    "Suitability Metrics" = 10
  )) %>%
  column_spec(1:2, bold = TRUE) %>%
  column_spec(c(3:7), background = "#f0f0f0") %>%
  footnote(
    general = "Mean = average suitability (0-1 scale); High (%) = percentage of pixels with suitability > 0.5; IDP = Internally Displaced Persons; HH = Households",
    general_title = "Note:",
    footnote_as_chunk = TRUE
  ) %>%
  scroll_box(height = "600px")

# Store tables in global environment
assign("dept_summary_table", dept_table, envir = .GlobalEnv)
assign("commune_summary_table", commune_table, envir = .GlobalEnv)

# ============================================================================
# DISPLAY SUMMARY STATISTICS
# ============================================================================

cat("=== DEPARTMENT TABLE SUMMARY ===\n")
cat("Number of departments:", nrow(dept_table), "\n")
cat("Total population:", scales::comma(sum(dept_table$Population, na.rm = TRUE)), "\n")
cat("Total IDPs:", scales::comma(sum(dept_table$IDP_Persons, na.rm = TRUE)), "\n")
cat("Mean baseline suitability:", round(mean(dept_table$Baseline_Mean, na.rm = TRUE), 3), "\n")
cat("Mean % high suitability (baseline):", round(mean(dept_table$Baseline_High_Pct, na.rm = TRUE), 1), "%\n\n")

cat("=== COMMUNE TABLE SUMMARY ===\n")
cat("Number of communes:", nrow(commune_table), "\n")
cat("Total population:", scales::comma(sum(commune_table$Population, na.rm = TRUE)), "\n")
cat("Total IDPs:", scales::comma(sum(commune_table$IDP_Persons, na.rm = TRUE)), "\n")
cat("Mean baseline suitability:", round(mean(commune_table$Baseline_Mean, na.rm = TRUE), 3), "\n")
cat("Mean % high suitability (baseline):", round(mean(commune_table$Baseline_High_Pct, na.rm = TRUE), 1), "%\n\n")

# ============================================================================
# OUTPUT HTML TABLES
# ============================================================================

# Display department table
cat("\n")
dept_table_html

# Display commune table  
cat("\n")
commune_table_html

```

```{r}
#| label: Calculate-section-population-at-risk
#| code-summary: "Calculate population and IDP counts at different risk thresholds by section (ADM3)"
#| echo: false
#| warning: false

# Load required libraries
library(dplyr)
library(tidyr)
library(purrr)
library(gt)
library(ggplot2)
library(sf)

# Define output directory
output_dir <- "C:/Users/ianpsheasmith/OneDrive - University of Florida/Documents - Haiti Vector/General/Figures/Haiti_SDMs/Ensemble_BRT"

# ============================================================================
# CALCULATE POPULATION AT RISK ACROSS SUITABILITY THRESHOLDS (SECTION LEVEL)
# ============================================================================

# Define risk thresholds: ≥0.5 (baseline risk), ≥0.75 (high risk)
calculate_at_risk_populations <- function(section_data, suitability_col, scenario_name) {
	section_data %>%
		sf::st_drop_geometry() %>%
		summarise(
			scenario = scenario_name,
			# Baseline risk threshold (≥0.5)
			baseline_risk_sections = sum(.data[[suitability_col]] >= 0.5, na.rm = TRUE),
			baseline_risk_population = sum(
				ifelse(.data[[suitability_col]] >= 0.5, total_population_2020, 0), 
				na.rm = TRUE
			),
			baseline_risk_idps = sum(
				ifelse(.data[[suitability_col]] >= 0.5, total_idp_persons, 0), 
				na.rm = TRUE
			),
			# High risk threshold (≥0.75)
			high_risk_sections = sum(.data[[suitability_col]] >= 0.75, na.rm = TRUE),
			high_risk_population = sum(
				ifelse(.data[[suitability_col]] >= 0.75, total_population_2020, 0), 
				na.rm = TRUE
			),
			high_risk_idps = sum(
				ifelse(.data[[suitability_col]] >= 0.75, total_idp_persons, 0), 
				na.rm = TRUE
			)
		)
}

# Calculate for all scenarios using map
scenarios_list <- list(
	list(col = "mean_baseline_suitability", name = "Baseline"),
	list(col = "mean_suitability_1_5C", name = "1.5°C Warming"),
	list(col = "mean_suitability_2C", name = "2°C Warming"),
	list(col = "mean_suitability_3C", name = "3°C Warming"),
	list(col = "mean_suitability_4C", name = "4°C Warming")
)

section_population_at_risk <- map_dfr(scenarios_list, function(scenario) {
	calculate_at_risk_populations(
		Haiti_adm3_with_stats, 
		scenario$col, 
		scenario$name
	)
}) %>%
	mutate(
		scenario = factor(
			scenario, 
			levels = c("Baseline", "1.5°C Warming", "2°C Warming", 
			           "3°C Warming", "4°C Warming")
		)
	)

# Calculate total population and IDPs for percentage calculations
total_population <- sum(sf::st_drop_geometry(Haiti_adm3_with_stats)$total_population_2020, na.rm = TRUE)
total_idps <- sum(sf::st_drop_geometry(Haiti_adm3_with_stats)$total_idp_persons, na.rm = TRUE)
total_sections <- nrow(Haiti_adm3_with_stats)

# Add percentage columns
section_population_at_risk <- section_population_at_risk %>%
	mutate(
		baseline_risk_pop_pct = (baseline_risk_population / total_population) * 100,
		baseline_risk_idp_pct = (baseline_risk_idps / total_idps) * 100,
		baseline_risk_section_pct = (baseline_risk_sections / total_sections) * 100,
		high_risk_pop_pct = (high_risk_population / total_population) * 100,
		high_risk_idp_pct = (high_risk_idps / total_idps) * 100,
		high_risk_section_pct = (high_risk_sections / total_sections) * 100
	)

# ============================================================================
# EXPORT DATA FOR MANUSCRIPT
# ============================================================================

write.csv(
	section_population_at_risk,
	file.path(output_dir, "Table_Section_Population_At_Risk.csv"),
	row.names = FALSE
)

cat("\n=== SECTION-LEVEL POPULATION AT RISK DATA EXPORTED ===\n")
cat("File saved to:", file.path(output_dir, "Table_Section_Population_At_Risk.csv"), "\n\n")

# ============================================================================
# CREATE HTML TABLE FOR DOCUMENT
# ============================================================================

section_at_risk_table_html <- section_population_at_risk %>%
	dplyr::select(
		scenario,
		baseline_risk_sections,
		high_risk_sections,
		baseline_risk_population,
		high_risk_population,
		baseline_risk_idps,
		high_risk_idps,
		baseline_risk_pop_pct,
		high_risk_pop_pct,
		baseline_risk_idp_pct,
		high_risk_idp_pct
	) %>%
	gt() %>%
	tab_header(
		title = "Population and IDPs at Risk by Suitability Threshold and Warming Scenario",
		subtitle = "Section-level analysis"
	) %>%
	cols_label(
		scenario = "Scenario",
		baseline_risk_sections = "N (≥0.5)",
		high_risk_sections = "N (≥0.75)",
		baseline_risk_population = "Count (≥0.5)",
		high_risk_population = "Count (≥0.75)",
		baseline_risk_idps = "Count (≥0.5)",
		high_risk_idps = "Count (≥0.75)",
		baseline_risk_pop_pct = "% (≥0.5)",
		high_risk_pop_pct = "% (≥0.75)",
		baseline_risk_idp_pct = "% (≥0.5)",
		high_risk_idp_pct = "% (≥0.75)"
	) %>%
	tab_spanner(
		label = "Sections",
		columns = c(baseline_risk_sections, high_risk_sections)
	) %>%
	tab_spanner(
		label = "Population",
		columns = c(baseline_risk_population, high_risk_population)
	) %>%
	tab_spanner(
		label = "IDPs",
		columns = c(baseline_risk_idps, high_risk_idps)
	) %>%
	tab_spanner(
		label = "Population %",
		columns = c(baseline_risk_pop_pct, high_risk_pop_pct)
	) %>%
	tab_spanner(
		label = "IDPs %",
		columns = c(baseline_risk_idp_pct, high_risk_idp_pct)
	) %>%
	fmt_number(
		columns = c(baseline_risk_sections, high_risk_sections,
		            baseline_risk_population, high_risk_population,
		            baseline_risk_idps, high_risk_idps),
		decimals = 0,
		use_seps = TRUE
	) %>%
	fmt_number(
		columns = c(baseline_risk_pop_pct, high_risk_pop_pct,
		            baseline_risk_idp_pct, high_risk_idp_pct),
		decimals = 2
	) %>%
	tab_style(
		style = cell_fill(color = "#f0f0f0"),
		locations = cells_body(columns = c(baseline_risk_sections, high_risk_sections))
	) %>%
	tab_style(
		style = cell_fill(color = "#e8f4f8"),
		locations = cells_body(columns = c(baseline_risk_population, high_risk_population))
	) %>%
	tab_style(
		style = cell_fill(color = "#fff3cd"),
		locations = cells_body(columns = c(baseline_risk_idps, high_risk_idps))
	) %>%
	tab_style(
		style = cell_fill(color = "#e3f2fd"),
		locations = cells_body(columns = c(baseline_risk_pop_pct, high_risk_pop_pct))
	) %>%
	tab_style(
		style = cell_fill(color = "#fff9c4"),
		locations = cells_body(columns = c(baseline_risk_idp_pct, high_risk_idp_pct))
	) %>%
	tab_footnote(
		footnote = "At risk defined as residing in communal sections with mean suitability meeting or exceeding the specified threshold. Percentages calculated relative to total population/IDPs across all of Haiti."
	) %>%
	tab_options(
		table.font.size = px(11),
		heading.align = "left"
	)

# ============================================================================
# CREATE VISUALIZATIONS
# ============================================================================

# Prepare data for plotting
plot_data <- section_population_at_risk %>%
	dplyr::select(scenario, ends_with("_population"), ends_with("_idps")) %>%
	pivot_longer(
		cols = -scenario,
		names_to = "metric",
		values_to = "count"
	) %>%
	mutate(
		risk_level = case_when(
			grepl("baseline_risk", metric) ~ "Baseline Risk (≥0.5)",
			grepl("high_risk", metric) ~ "High Risk (≥0.75)"
		),
		population_type = ifelse(grepl("_population", metric), "Population", "IDPs"),
		risk_level = factor(
			risk_level, 
			levels = c("Baseline Risk (≥0.5)", "High Risk (≥0.75)")
		)
	)

# Combined plot: Population and IDPs at risk by threshold and scenario
combined_risk_plot <- plot_data %>%
	mutate(
		# Scale appropriately for display
		count_scaled = ifelse(
			population_type == "Population", 
			count / 1000000, 
			count / 1000
		),
		y_label = ifelse(
			population_type == "Population", 
			"Population (millions)", 
			"IDPs (thousands)"
		)
	) %>%
	ggplot(aes(x = scenario, y = count_scaled, fill = risk_level)) +
	geom_col(position = "dodge", alpha = 0.85, width = 0.7) +
	facet_wrap(~population_type, scales = "free_y", ncol = 1) +
	scale_fill_manual(
		values = c(
			"Baseline Risk (≥0.5)" = "#f39c12",
			"High Risk (≥0.75)" = "#e67e22"
		),
		name = "Risk Level"
	) +
	labs(
		title = "Population and IDPs at Risk Across Warming Scenarios",
		subtitle = "Section-level analysis by suitability threshold",
		x = "Warming Scenario",
		y = "Count",
		caption = "Risk defined as residing in sections meeting specified suitability thresholds"
	) +
	theme_minimal() +
	theme(
		plot.title = element_text(size = 14, face = "bold"),
		plot.subtitle = element_text(size = 11, color = "gray40"),
		axis.title = element_text(size = 11, face = "bold"),
		axis.text.x = element_text(angle = 45, hjust = 1),
		strip.text = element_text(size = 12, face = "bold"),
		strip.background = element_rect(fill = "gray90", color = NA),
		panel.grid.major.x = element_blank(),
		plot.caption = element_text(size = 9, color = "gray50", hjust = 0),
		legend.position = "bottom",
		legend.title = element_text(size = 10, face = "bold")
	) +
	scale_y_continuous(
		expand = expansion(mult = c(0, 0.1)),
		labels = scales::comma
	)

# Save plot as SVG
ggsave(
	file.path(output_dir, "Figure_Section_Population_IDP_At_Risk.svg"),
	combined_risk_plot,
	width = 10, 
	height = 8, 
	dpi = 300
)

# Store in global environment
assign("section_population_at_risk", section_population_at_risk, envir = .GlobalEnv)

# ============================================================================
# DISPLAY SUMMARY STATISTICS
# ============================================================================

cat("=== SECTION-LEVEL POPULATION AT RISK SUMMARY ===\n\n")
cat("Total population:", scales::comma(total_population), "\n")
cat("Total IDPs:", scales::comma(total_idps), "\n")
cat("Total sections:", total_sections, "\n\n")

# Baseline scenario summary
baseline_stats <- section_population_at_risk %>% 
	filter(scenario == "Baseline")

cat("BASELINE SCENARIO:\n")
cat("  ≥0.5 suitability:", scales::comma(baseline_stats$baseline_risk_population), 
    sprintf("people (%.2f%%) |", baseline_stats$baseline_risk_pop_pct),
    scales::comma(baseline_stats$baseline_risk_idps), 
    sprintf("IDPs (%.2f%%)\n", baseline_stats$baseline_risk_idp_pct))
cat("  ≥0.75 suitability:", scales::comma(baseline_stats$high_risk_population), 
    sprintf("people (%.2f%%) |", baseline_stats$high_risk_pop_pct),
    scales::comma(baseline_stats$high_risk_idps), 
    sprintf("IDPs (%.2f%%)\n\n", baseline_stats$high_risk_idp_pct))

# 4°C warming scenario summary
wl4_stats <- section_population_at_risk %>% 
	filter(scenario == "4°C Warming")

cat("4°C WARMING SCENARIO:\n")
cat("  ≥0.5 suitability:", scales::comma(wl4_stats$baseline_risk_population), 
    sprintf("people (%.2f%%) |", wl4_stats$baseline_risk_pop_pct),
    scales::comma(wl4_stats$baseline_risk_idps), 
    sprintf("IDPs (%.2f%%)\n", wl4_stats$baseline_risk_idp_pct))
cat("  ≥0.75 suitability:", scales::comma(wl4_stats$high_risk_population), 
    sprintf("people (%.2f%%) |", wl4_stats$high_risk_pop_pct),
    scales::comma(wl4_stats$high_risk_idps), 
    sprintf("IDPs (%.2f%%)\n\n", wl4_stats$high_risk_idp_pct))

# ============================================================================
# OUTPUT HTML TABLE AND PLOT
# ============================================================================

cat("\n")
section_at_risk_table_html

cat("\n")
print(combined_risk_plot)
```

## Save Document

Save this as `haiti-enm.qmd` and run:

``` bash
quarto render haiti-enm.qmd
```
